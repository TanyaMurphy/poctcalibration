---
title: "Calibration Curve-fitting"
output:
  html_document:
    toc: true

bibliography: "../../Literature/library.bib"
---

<!-- Section tabs -->
<ul class="nav nav-tabs" role="tablist">
  <li><a href="calib_overview.html">&laquo; Back to calibration overview</a></li>
  <li><a href="calib_tut4_curve_background.html">Background</a></li>
  <li><a href="calib_tut4_curve_ocon.html">O'Connell's ELISA</a></li>
  <li><a href="calib_tut4_curve_elisa.html">R's ELISA{gtools}</a></li>
</ul>
<br>

## Background

*in development*

<!--

TODO Still needs major editing, especially the later sections. Some details from the ELISA tutorial probably belong here

In this tutorial we finally get to the crux of calibration. We will:

- describe the relationship between analyte concentration and observed response in data set using classical linear, non-linear and weighted regression methods;

- use the resulting model to estimate analyte concentration in new samples;

- propose metrics by which to compare models and describe the uncertainty in estimated concentrations;

- use existing `R` packages and functions;


- use programming loops to repeat a function over a range of 'new' observations.


## Background

Let's review some background for the upcoming analysis steps:

- Curve-fitting 

- Point-estimation  

- Inference?

At last, we get to the actual curve fitting. The concentration-signal relationship may be descending or ascending, linear or curved (often sigmoidal) depending on test, to be approximated by the 'calibration curve' or 'standard curve' (even if it's linear). Generally, immunometric ELISA generate an ascending sigmoidal curve, often with some asymmetry. If the range of clinically relevant values falls within the nearly linear portion of the curve, a simpler linear function can be used. Before this and other simplifying assumptions are made, however, the relationship should be well understood. The estimated scalars and constants of the function are referred to as the parameters (specific values differ by test, and possibly by run). A linear function has 2 parameters: slope and y-intercept. A symmetrical sigmoidal curve (e.g. Hill equation, 4PL) has 4 parameters: upper and lower asymptotes, Hill slope (an exponent, not a directly interpretable linear slope) and an inflexion point. The upper asymptote represents the signal generated by concentrations that virtually saturate the [enzyme-antigen-enzyme] complex—the maximum possible signal generated by that test (given the conditions). The inflection point corresponds to IC/EC50 in a symmetrical curve and is directly interpretable and useful in drug studies, for example. If the concentration-signal relationship is significantly asymmetric, as is the case for many ELISAs, a 5th parameter is added to the 4PL model. In the popular 5PL model, the inflection point is still important for curve fitting, but it does not have the same directly interpretable meaning of IC50. Whether frequentist or Bayesian, the non-linear functions are estimated with an iterative algorithm and need some initial values. I don’t know how the typical frequentist curve-fitting software chooses initial values, but in a Bayesian analysis the investigators choose them (along with a range of plausible values) explicitly for each model based on prior knowledge of the test’s properties. In theory, much better estimates can be obtained if there is a lot of prior knowledge. Sometimes the data are manipulated to have the lower asymptote equal zero and the function appears to have 1 parameter less (as in Hill equation in Pollock/Whitesides'  paper).] Imagine all the possible ascending curves you could physically make on a table with a somewhat stiff cable or rope. There are many possibilities if allowed to vary size, shape and position on the table! Similarly, the statistical software has to contend with a huge [sample space], but will estimate a curve function that is closer to the 'true' concentration-signal relationship if the provided data restricts the possibilities, or 'anchors' the curve, especially at the extremes. In the hCG data we have played with, we have good anchoring near the bottom, but estimates for the size and shape of the rest of the curve are very vague. If we had data over a wider range plus concentrations at which the signal is flattening out (and if it hooks), our curve parameter estimates would be so much better. If exploratory or theoretical work can also generate a 'ballpark' figure for IC/EC50, study design and curve estimation can be further improved. A thorough investigation of low, mid and high concentrations across important measurement conditions will also help us develop an efficient calibration verification and adjustment protocol for field use. The major simplifications we hope to justify are that the only curve parameters to vary significantly over reagent or bulb age and hydrogel batch, etc. will be the placement of the curve relative to the y-axis—the height of the curve. If the overall difference between upper and lower asymptotes and the rate of change or shape of the curve between the asymptotes is constant enough, then a model based on difference from a zero standard could be sufficient to ensure reliability between analysers in the field and over time. In this case, a zero calibrator (on the chip?) would deal with a host of potential sources of bias in individual runs: reagent and bulb age, environmental conditions. Protocols for calibration verification, adjustment and software/firmware updates should be kept as simple as possible. Some cost-benefit choices could be made in light of the planned validity study since some of the data from the validity study could be used to enhance the accuracy and precision of the next version of the 'master' curve. 'Master curve 1.0' needs to be very good for the validity study since it will affect the comparison with a reference test, but perhaps the full range of conditions expected in the field need not be replicated in the development of master curve 1.0 if conditions will be fairly controlled for the validity study. The data from the validity study and ongoing work in the lab with expanded conditions (e.g. more) could then be added to the original data to refine 'master curve 2.0' for launch into field use.

-->

### Curve functions and their inverse

The most commonly recommended method for calibration is least squares (non-)linear regression using a data set consisting of calibrators of known concentration (x) and their signal readings (y). The unknown sample concentrations are then calculated from the *inverse* of the estimated function. This is sometimes called inverse regression, but note that the inverse function is not being directly estimated from the data.

**Common calibration curve models. See below for parameter definitions:**

| Name               |      Function                                           |       Inverse             |
|:-------------------|:--------------------------------------------------------|:--------------------------|
| *Linear*           | $y = int + b * x$               |   $x = \frac{y-int}{b}$     |
| *Michaelis-Menten* |   $y = \frac{Vm*x}{k+x}$           |    $x = \frac{k*y}{Vm-y}$    |
| *4P Logistic*      | $y = d + \frac{a-d}{1 + (\frac{x}{c})^b}$               | $x = c \Big(\frac{a-d}{y-d} -1 \Big)^{1/b}$ |
| *5P Logistic*      | $y = d + \frac{(a-d)}{\Big(1 + (\frac{x}{c})^b\Big)^g}$ | $x = c \Big((\frac{a-d}{y-d})^{1/g} -1 \Big)^{1/b}$ |


Note: In our R scripts, we will use more descriptive labels for the parameters of the logistic models than used by Dunn and Wild or Findlay. These will serve as a memory aid, but also single letter labels are at high risk of being confused with or replaced by another R object since very short names are popular for intermediate steps.

- **$b$ (`beta` or `hill`):** Rate of change, slope, "slope factor" or "Hill slope". In the case of linear regression, the units of $b$ are x-units per y-unit. In logistic functions, $b$ is unit-less.

- **$Vm$:** On the scale of $y$. Traditional notation for upper asymptote (as $x$ goes to infinity) in Michael-Menten models. Analogous to $a$ in the logistic models.

- **$k$:** on the scale of $x$. [No intuitive meaning], it affects the rate of approach to $Vm$.

- **$c$ (`inflec`):** On the scale of $x$; scales $x$. In many applications it has a meaningful value: 50% effective dose (ED50) or inhibitory concentration (IC50) is used as an important benchmark in whole-cell assays or inhibitory drug studies, respectively. Since ED/IC50 is not meaningful in our domain, we will focus on the graphical meaning of $c$ as inflection point in logistic models. 
    
- **$a$ (`inf.x.asymp`):** On the scale of y; horizontal asymptote as $x$ goes to infinity. 

- **$d$ (`small.x.asymp`):** On the scale of y; horizontal asymptote as $x$ goes, in theory, to negative infinity, but, in practice, to zero.

   Note that here and in texts such as Dunn and Wild [-@Dunn2013], $a$ is the asymptote as x goes to infinity regardless of curve direction (increasing or decreasing). The sign of $b$ controls curve direction. Occasionally authors reverse the position of $a$ and $d$ to describe an increasing versus decreasing curve and keep the sign of $b$ constant. In addition to any theoretical motivation, we find the former choice practical since $d$ approximates the y-intercept of other models or plotted data. 

- **$g$ (`g.5pl`):** no units. Similar properties to $b$, it further alters the rate of change, [specifically the section of the curve between $c$ and $a$.] 

Four (and fewer) parameter logistic models are symmetric around the inflection point, $c$, *on the log(x) scale*. The 5PL model is used to fit asymmetric sigmoidal curves (using log(x)) allowing a sharper transition toward one asymptote than the other. 

Figures x--x show how the parameter values change curve shape in the four major model types. Familiarity with the parameters will help with model checking and choosing starting, or initial, values in non-linear regression (explained in more detail later).

```{r intro-functions}
# require(knitr)
# Graphical preferences for this project
source("AMgraph.R")

# --- Non-linear and inverse functions used in tutorials ----
# Actually included in source file "../../../../source/R/AMfunctions.R"
# if changes are made here, change in AMfunctions.R, too
# Every model has an intercept (a), and parameters that relate non-zero  
# x-values to y; sometimes int is zero and drops out
# ex. linear model: int varies, y change by b*x
#-------------- Define models -----------------------------------
# Michaelis-Menten model
# int always 0, no slope param needed, shape controlled by form
# Use built-in SSmicmen (stats package): Vm*input/(K+input) or this
M.micmen <- function(x, offset, Vm, K){
    f <- offset + Vm * x / (K + x)
    return(f)
}

# 4PL model
M.4pl <- function(x, small.x.asymp, inf.x.asymp, inflec, hill){
    f <- small.x.asymp + ((inf.x.asymp - small.x.asymp)/
                              (1 + (x / inflec)^hill))
    return(f)
}

# 5PL model  
M.5pl <- function(x, small.x.asymp, inf.x.asymp, c.5pl, hill, g.5pl){
    f <- small.x.asymp + ((inf.x.asymp - small.x.asymp)/
                              (1 + (x / c.5pl)^hill)^g.5pl)
    return(f)
}

# ----- Inverse functions -------------------------
Inv.lr <- function(y, int, beta){
    f <- (y - int)/ beta
    names(f) <- "x.hat"
    return(f)
} 

Inv.micmen <- function(y, offset, Vm, K){
    f <- K * (y - offset) / (Vm - (y - offset))
    names(f) <- "x.hat"
    return(f)
} 

Inv.4pl <- function(y, small.x.asymp, inf.x.asymp, inflec, hill){
    f <- inflec * ((inf.x.asymp - small.x.asymp) / 
                       (y - small.x.asymp) - 1)^(1 / hill)
    names(f) <- "x.hat"
    return(f)
} 

Inv.5pl <- function(y, small.x.asymp, inf.x.asymp, c.5pl, hill, g.5pl){
    f <- c.5pl * (((inf.x.asymp - small.x.asymp) / 
                       (y - small.x.asymp))^(1/g.5pl) - 1)^(1 / hill)
    names(f) <- "x.hat"
    return(f)
} 
# Also defined in this source file which will be loaded for each tutorial
source("AMfunctions.R")
```


```{r intro-figs2, echo=FALSE, fig.width=12, fig.height=10}
frame <- ggplot(data.frame(x = c(0, 100), y = c(0, 120)), aes(y, x)) + 
    ylab("Response") + xlab("Concentration") +
    expand_limits(x = 0, y = 0) + 
    scale_x_continuous(limits=c(0, 105), expand = c(0, 0)) + 
    scale_y_continuous(limits=c(0, 110), expand = c(0, 0))

plot.lin <- frame + 
    geom_abline(intercept = 1, slope = 1.2, size = 1.1) +
    geom_text(aes( 15, 5, label = "intercept = 1"), size = 3) +
    geom_text(aes( 55, 51, label = "slope(b) = 1.2"), size = 3) + 
    ggtitle("Linear model")

# Parameters: Vm, K
plot.micmen <- frame + 
    stat_function(fun = SSmicmen, size = 1.1,
                  args = list(Vm = 100, K = 40)) + 
    stat_function(fun = SSmicmen, linetype="dashed", size = 1.1,  
                  args = list(Vm = 100, K = 10)) + 
    stat_function(fun = SSmicmen,  
                  args = list(Vm = 75, K = 40)) + 
    geom_text(aes( 85, 82, label = "k = 10, Vm = 100"), size = 3) +
    geom_text(aes( 85, 60, label = "k = 40, Vm = 100"), size = 3) +
    geom_text(aes( 85, 45, label = "k = 40, Vm = 75"), size = 3) +
    ggtitle("Michaelis-Menten")

# Parameters: small.x.asymp, inf.x.asymp, inflec, hill
plot.4pl <- frame + 
    stat_function(fun = M.4pl, size = 1.1,
                  args = list(small.x.asymp= 5, inf.x.asymp = 100, 
                              inflec = 50, hill = -3)) + 
    stat_function(fun = M.4pl, linetype="dashed",  
                  args = list(small.x.asymp= 5, inf.x.asymp = 100, 
                              inflec = 50, hill = -1.5)) + 
    stat_function(fun = M.4pl,  
                  args = list(small.x.asymp= 5, inf.x.asymp = 100, 
                              inflec = 50, hill = -2)) + 
    geom_hline(aes(yintercept=100), colour="#BB0000", linetype="dashed") +
    geom_text(aes( 2, 100, label = "a", vjust = -1), size = 3) +
    geom_hline(aes(yintercept=5), colour="#BB0000", linetype="dashed") +
    geom_text(aes( 1, 5, label = "d", vjust = 1.2), size = 3) +
    geom_vline(aes(xintercept=50), colour="#BB0000", linetype="dashed") +
    geom_text(aes( 50, 48, label = "c", hjust = -1), size = 3) +
    geom_text(aes( 82, 90, label = "b = -3"), size = 3) +
    geom_text(aes( 91, 81, label = "b = -2"), size = 3) +
    geom_text(aes( 98, 69, label = "b = -1.5"), size = 3) +
    ggtitle("4PL")

# Parameters: small.x.asymp, inf.x.asymp, c.5pl, hill, g.5pl
plot.5pl <- frame + 
    stat_function(fun = M.5pl, size = 1.1,
                  args = list(small.x.asymp = 5, inf.x.asymp = 100, 
                              c.5pl = 50, hill = -3, g.5pl = 0.5)) + 
    stat_function(fun = M.4pl, linetype="dashed",  
                  args = list(small.x.asymp = 5, inf.x.asymp = 100, 
                              inflec = 50, hill = -3)) +
    ggtitle("5PL vs 4PL") +
    geom_hline(aes(yintercept=100), colour="#BB0000", linetype="dashed") +
    geom_text(aes( 2, 100, label = "a", vjust = -1), size = 3) +
    geom_hline(aes(yintercept=5), colour="#BB0000", linetype="dashed") +
    geom_text(aes( 1, 5, label = "d", vjust = 1.2), size = 3) +
    geom_vline(aes(xintercept=50), colour="#BB0000", linetype="dashed") +
    geom_text(aes( 50, 48, label = "c", hjust = -1), size = 3) +
    geom_text(aes( 35, 75, label = "5PL: b = -3, g = 0.5"), size = 3) +
    geom_text(aes( 80, 65, label = "4PL: b = -3 (or 5PL where g = 1)"), 
              size = 3) 

grid.arrange(plot.lin, plot.micmen, plot.4pl, plot.5pl)
```

**Plotted as log(concentration):**

```{r intro-figs3, echo=FALSE, fig.width=12, fig.height=10, eval=TRUE}

x1 <- cbind(seq(2, 120, by =1))
ylm = 1 + 1.2 * x1

ymm1 <- data.frame(x1, y1 = SSmicmen(x1, Vm = 100, K = 40), 
                   Parameters = "Vm = 100, K = 40")
ymm2 <- data.frame(x1, y1 = SSmicmen(x1, Vm = 100, K = 10), 
                   Parameters = "Vm = 100, K = 10")
ymm3 <- data.frame(x1, y1 = SSmicmen(x1, Vm = 75, K = 40), 
                   Parameters = "Vm = 75, K = 40")
ymm <- rbind(ymm1, ymm2, ymm3)

y4pl1 <- data.frame(x1, y1 = M.4pl(x1, small.x.asymp= 5, inf.x.asymp = 100, 
                              inflec = 50, hill = -3), 
                    Parameters = "d = 5, a = 100, c = 50, b = -3")
y4pl2 <- data.frame(x1, y1 = M.4pl(x1, small.x.asymp= 5, inf.x.asymp = 100, 
                              inflec = 50, hill = -1.5), 
                    Parameters = "d = 5, a = 100, c = 50, b = -1.5")
y4pl3 <- data.frame(x1, y1 = M.4pl(x1, small.x.asymp= 5, inf.x.asymp = 100, 
                              inflec = 50, hill = -2), 
                    Parameters = "d = 5, a = 100, c = 50, b = -2")
y4pl <- rbind(y4pl1, y4pl2, y4pl3)

y5pl1 <- data.frame(x1, y1 = M.5pl(x1, small.x.asymp= 5, inf.x.asymp = 100, 
                              c.5pl = 50, hill = -3, g.5pl = 0.5), 
                    Parameters = "d = 5, a = 100, c = 50, b = -3, g = 0.5")
y5pl2 <- data.frame(x1, y1 = M.5pl(x1, small.x.asymp= 5, inf.x.asymp = 100, 
                              c.5pl = 50, hill = -3, g.5pl = 1), 
                    Parameters = "d = 5, a = 100, c = 50, b = -3, g = 1")
y5pl <- rbind(y5pl1, y5pl2)

# Parameters: int, b
plot.log.lin <- ggplot() + geom_line(aes(ylm, x1), size = 1.1) +
    ylab("Response") + xlab("log(Concentration)") +
    scale_x_continuous(limits=c(2, 150), expand = c(0, 0), 
                       trans = "log", breaks = c(2.5, 5, 10, 20, 40, 80)) + 
    scale_y_continuous(limits=c(-5, 120), expand = c(0, 0), 
                       breaks = c(0, 25, 50, 75, 100)) +
    geom_text(aes(3, 10, label = "intercept = 1"), size = 3) +
    geom_text(aes(30, 40, label = "slope(b) = 1.2"), size = 3) + 
    ggtitle("Linear model")

# Parameters: Vm, K
plot.micmen <- ggplot(ymm, aes(y= y1, x = x1, group = Parameters)) + 
    geom_line(aes(linetype = Parameters), size = 0.8) + 
    scale_x_continuous(limits=c(2, 150), expand = c(0, 0), 
                       trans = "log", breaks = c(2.5, 5, 10, 20, 40, 80)) + 
    scale_y_continuous(limits=c(-5, 120), expand = c(0, 0), breaks = c(0, 25, 50, 75, 100)) +
    ylab("Response") + xlab("log(Concentration)") + ggtitle("Michaelis-Menten") +
    theme(legend.justification=c(0, 1), legend.position=c(0, 1))
 
# Parameters: small.x.asymp, inf.x.asymp, inflec, hill
plot.4pl <- ggplot(y4pl, aes(y= y1, x = x1, group = Parameters)) + 
    geom_line(aes(linetype = Parameters), size = 0.8) + 
    scale_x_continuous(limits=c(2, 150), expand = c(0, 0), 
                       trans = "log", breaks = c(2.5, 5, 10, 20, 40, 80)) + 
    scale_y_continuous(limits=c(-5, 120), expand = c(0, 0), breaks = c(0, 25, 50, 75, 100)) +
    geom_vline(aes(xintercept=50), colour="#BB0000", linetype="dashed") +
    ylab("Response") + xlab("log(Concentration)") + ggtitle("4PL") +
    theme(legend.justification=c(0, 1), legend.position=c(0, 1))

# Parameters: small.x.asymp, inf.x.asymp, c.5pl, hill, g.5pl
plot.5pl <- ggplot(y5pl, aes(y= y1, x = x1, group = Parameters)) + 
    geom_line(aes(linetype = Parameters), size = 0.8) + 
    scale_x_continuous(limits=c(2, 150), expand = c(0, 0), 
                       trans = "log", breaks = c(2.5, 5, 10, 20, 40, 80)) + 
    scale_y_continuous(limits=c(-5, 120), expand = c(0, 0), breaks = c(0, 25, 50, 75, 100)) +
    geom_vline(aes(xintercept=50), colour="#BB0000", linetype="dashed") +
    ylab("Response") + xlab("log(Concentration)") + ggtitle("5PL (vs. 4PL equiv.)") +
    theme(legend.justification=c(0, 1), legend.position=c(0, 1))

grid.arrange(plot.log.lin, plot.micmen, plot.4pl, plot.5pl, 
             main = textGrob("Same data plotted with logged x-axis",
                      gp = gpar(fontsize = 16, fontface = "bold")))
```

#### Relationships among parameters in the 5PL model {#appx-curveParam-5PL}

See figure 2 on page x. The lower part of the curve, below *c* is a function of *b\*g*. In the upper portion, above the inflection point, the rate of approach to *a* is only a function of *b*.

| Model |    b |   g |  b\*g | Curve shape (lower portion) |
|:------|-----:|----:|------:|:----------------------------|
| I     | -4.3 | 0.1 |  -0.4 | Most convex                 |
| II    |   -2 | 0.3 |  -0.6 | More convex                 |
| III   |   -3 | 0.3 |  -0.9 | Nearly linear               |
| IV    | -4.3 | 0.3 |  -1.3 | Slight concave up           |
| V     |   -5 | 0.3 |  -1.5 | More concave                |
| VI    | -4.3 | 0.5 |  -2.2 | Most concave                |

  : Table y. Effect of 5PL power parameters on curve shape

### Heteroscedasticity and weighting

The reciprocal of the variance function is used to weight the data because...
<http://en.wikipedia.org/wiki/Variance_function#Application_.E2.80.93_weighted_least_squares> [look into original source]

See also: <http://www.ats.ucla.edu/stat/r/dae/rreg.htm>

<http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.154.5344&rep=rep1&type=pdf>

#### Calculating weights

The reciprocal of the variance function is used to weight the data; that is, if 

$$ Var(y) = a * Mean(y)^{theta} $$

then 

$$ weight(obs) = standardized[1/EstimatedMean(y.group)^{theta}] $$

#### Errors in weighted models

Weighted residuals...

Regression line confidence bands:  
From investr `plotFit` Rdocs: "Confidence/prediction bands for nonlinear regression (i.e., objects of class nls) are based on a linear approximation as described in Bates & Watts (2007). This [function was inspired] by the plotfit function in the nlstools package."

### Model evaluation

<!--

In this tutorial, models will be evaluated and compared based on the residual plots and, when extra data are available, cross-validation. For example, a cross-validation set made up of the 'QC' samples is used for the ELISA analysis. Raw and percent difference between the predicted and actual (as labelled) concentrations are the metrics used. In recommending these graphical and descriptive data summaries for model diagnostics, we try to relate the research question (i.e. a small sub-question) directly to the method, whereas some traditional model evaluation methods use null-hypothesis tests (ex. t-test/p-value for non-zero slope). A no-association null hypothesis is a ridiculous comparison in many of these applications. [We realise, however, that some uncertainty intervals that we use imply the null hypothesis (?).] Direct measures of fit plus the scientist's judgement will yield conclusions that are as good or better than null-hypothesis tests and lead to faster skill development. 

-->

See Anderson-Sprecher_1994.pdf about R-squ

<!--

The next best situation is parallelism where in a 4PL (or 5PL) relationship, all parameters except *c* are common to all runs. This assumes that, once zero-dose adjusted, the lower and upper asymptotes and the rate/shape (*b* and *g*) are constant (within a margin of error) between runs and the only difference is a shift the e the right of left described by *c*. This could be sufficient to explain the apparent varying curve heights since we are only observing data in the lower portion of the underlying curve. 

This assumption plus the ability to model all runs simultaneously, especially given a larger number of runs than the 4 we had in this tutorial, would make curve-fitting more efficient. It would be faster and effectively provide a larger sample for estimation (albeit not simply the sum total of all the data). This can be done with hierarchical, or mixed, models. The parallelism assumption is a big one and elements of hierarchical mixed effects methods written for R (mainly the nlme package) are somewhat controversial---notably the [parameter standard errors?]. We may not need to concern ourselves with those controversies, however, since parametric prediction intervals for the concentration calculated from an inverse function are so controversial anyway. As we show in the supplement for this chapter and the cross-validation section for the ELISA data, [bootstrap intervals or Bayesian methods perform best anyway.]

-->

### Estimating the concentration of unknown samples

See the next set of tutorials, [Concentration estimation and the precision profile](calib_tut5_precision_background.html).

### R functions introduced in this tutorial

Some of our favourite functions for calibration and non-linear regression are not part of the more familiar packages for linear regression (e.g. residual plots). Some of these `functions`{package} are:

- `nlsLM`{minipack.lm}: We have opted to use the `nlsLM` function for non-linear regression instead of the core `nls` function for comparability with GraphPad. As explained [here](http://www.graphpad.com/guides/prism/6/curve-fitting/index.htm?reg_who_developed_nonlinear_regres.htm) and [here](http://rmazing.wordpress.com/2012/07/05/a-better-nls/) there are some advantages to using the Levenberg-Marquardt non-linear least-squares algorithm rather than [the other one]. The resulting R 'objects' can be used the same as regular `nls` objects.

<!-- Move to DNase curve-fitting tutorial?

- `L.4` or `L.5{drc}`: Self-starting 4- and 5-PL functions. Parameterised as 

$$f(x) = c + \frac{d-c}{(1+\exp(b(x - e)))^f}$$

where, in the case of the 5-PL, $f=1$. This version takes $x$ not $log(x)$ as an input.

--->

- `plotFit`{investr}: Plots the estimated (non-)linear model with the original data, the fitted line and, if desired, the confidence and/or prediction bands.
- `hatvalues` or `cooks.distance`{drc}

The drc, quantchem and other calibration curve fitting packages contain a lot of convenience functions, but we prefer to build up a set of functions based on basic functions for instructional purposes. Some of the curve-fitting packages incorporate a lot into one function, including various formal (null-hypothesis) tests for fit (model, parameters, etc). As explained above, we prefer to evaluate models in a more direct way because the null hypothesis tests can be difficult to interpret---they rarely directly address the question of interest. Also, presentation of results is very subjective so we hope the small functions built in these tutorials will help the user build functions that are customised to the needs of their lab and domain and avoid a lot of manual cut-and-paste or formatting (error prone). 

###Summary




### References

