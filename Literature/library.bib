Automatically generated by Mendeley Desktop 1.11
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Options -> BibTeX in Mendeley Desktop

@book{Huet2004,
abstract = {(Springer Series in Statistics)},
address = {New York},
author = {Huet, S and Bouvier, A and Poursat, M-A and Jolivet, E},
edition = {2nd},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Huet\_2004.pdf:pdf},
isbn = {0387400818},
keywords = {huet},
publisher = {Springer-Verlag},
title = {{Statistical Tools for Nonlinear Regression: A Practical Guide with S-PLUS and R Examples}},
year = {2004}
}
@article{Bland1999,
abstract = {Agreement between two methods of clinical measurement can be quantified using the differences between observations made using the two methods on the same subjects. The 95\% limits of agreement, estimated by mean difference +/- 1.96 standard deviation of the differences, provide an interval within which 95\% of differences between measurements by the two methods are expected to lie. We describe how graphical methods can be used to investigate the assumptions of the method and we also give confidence intervals. We extend the basic approach to data where there is a relationship between difference and magnitude, both with a simple logarithmic transformation approach and a new, more general, regression approach. We discuss the importance of the repeatability of each method separately and compare an estimate of this to the limits of agreement. We extend the limits of agreement approach to data with repeated measurements, proposing new estimates for equal numbers of replicates by each method on each subject, for unequal numbers of replicates, and for replicated data collected in pairs, where the underlying value of the quantity being measured is changing. Finally, we describe a nonparametric approach to comparing methods.},
annote = {
        From Duplicate 1 ( 
        
        
          Measuring agreement in method comparison studies.
        
        
         - Bland, J M; Altman, Douglas G )

        
        

        From Duplicate 1 ( 
        
        
          Measuring agreement in method comparison studies.
        
        
         - Bland, J M; Altman, Douglas G )

        
        

        From Duplicate 1 ( 
        
        
          Measuring agreement in method comparison studies.
        
        
         - Bland, J M; Altman, Douglas G )

        
        
"The standard methos is sometimes known as the 'gold standard', but this does not--or should not--imply that it is measured without error."

        
"...we should define satisfactory agreement in advance."

        
Validity:
Straightforward plotting and description of agreement as difference with "limits of agreement".  Correction for bias and heteroscedasticity straighforward, too.

        
Reliability:
"The comparison of the repeatability of each method is relevant to methos comparison because the repeatabilities of two methods of measurement limit the amount of agreement which is possible."

        
Style: Very clear and instructive.  Great introductory sentences--really summarize what's to come in section while providing clear explanation of basic concept. Great discussion/conclusion!

        

        

        

        

        From Duplicate 2 ( 
        
        
          Measuring agreement in method comparison studies.
        
        
         - Bland, J M; Altman, Douglas G )

        
        
"The standard methos is sometimes known as the 'gold standard', but this does not--or should not--imply that it is measured without error."

        
"...we should define satisfactory agreement in advance."

        
Validity:
Straightforward plotting and description of agreement as difference with "limits of agreement". Correction for bias and heteroscedasticity straighforward, too.

        
Reliability:
"The comparison of the repeatability of each method is relevant to methos comparison because the repeatabilities of two methods of measurement limit the amount of agreement which is possible."

        
Style: Very clear and instructive. Great introductory sentences--really summarize what's to come in section while providing clear explanation of basic concept. Great discussion/conclusion!

        

        

        

        

        

        

        From Duplicate 2 ( 
        
        
          Measuring agreement in method comparison studies.
        
        
         - Bland, J M; Altman, Douglas G )

        
        
"The standard methos is sometimes known as the 'gold standard', but this does not--or should not--imply that it is measured without error."

        
"...we should define satisfactory agreement in advance."

        
Validity:
Straightforward plotting and description of agreement as difference with "limits of agreement". Correction for bias and heteroscedasticity straighforward, too.

        
Reliability:
"The comparison of the repeatability of each method is relevant to methos comparison because the repeatabilities of two methods of measurement limit the amount of agreement which is possible."

        
Style: Very clear and instructive. Great introductory sentences--really summarize what's to come in section while providing clear explanation of basic concept. Great discussion/conclusion!

        

        

        

        

        

        

        From Duplicate 2 ( 
        
        
          Measuring agreement in method comparison studies.
        
        
         - Bland, J M; Altman, Douglas G )

        
        
"The standard methos is sometimes known as the 'gold standard', but this does not--or should not--imply that it is measured without error."

        
"...we should define satisfactory agreement in advance."

        
Validity:
Straightforward plotting and description of agreement as difference with "limits of agreement".  Correction for bias and heteroscedasticity straighforward, too.

        
Reliability:
"The comparison of the repeatability of each method is relevant to methos comparison because the repeatabilities of two methods of measurement limit the amount of agreement which is possible."

        
Style: Very clear and instructive.  Great introductory sentences--really summarize what's to come in section while providing clear explanation of basic concept. Great discussion/conclusion!

        

        

        

        

      },
author = {Bland, J Martin and Altman, Douglas G},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Bland\_1999.pdf:pdf},
issn = {0962-2802},
journal = {Statistical methods in medical research},
keywords = {Clinical Laboratory Techniques,Clinical Laboratory Techniques: statistics \& numer,Confidence Intervals,Humans,Linear Models,Medical Laboratory Science,Medical Laboratory Science: statistics \& numerical,Nonparametric,Reproducibility of Results,Statistics,agreement studies},
mendeley-tags = {agreement studies},
month = jun,
number = {2},
pages = {135--160},
pmid = {10501650},
title = {{Measuring agreement in method comparison studies}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3360667\&tool=pmcentrez\&rendertype=abstract},
volume = {8},
year = {1999}
}
@article{Blockley2009,
annote = {
        From Duplicate 1 ( 
        
        
          Calibration commentary
        
        
         - Blockley, S P E; Housley, R A )

        
        

        From Duplicate 2 ( 
        
        
          Calibration commentary
        
        
         - Holloway, Royal )

        
        

        

        

        

        

        From Duplicate 2 ( 
        
        
          Calibration commentary
        
        
         - Blockley, S P E; Housley, R A )

        
        

        From Duplicate 1 ( 
        
        
          Calibration commentary
        
        
         - Blockley, S P E; Housley, R A )

        
        

        From Duplicate 2 ( 
        
        
          Calibration commentary
        
        
         - Holloway, Royal )

        
        

        

        

        

        

        

        

      },
author = {Blockley, S P E and Housley, R A},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blockley, Housley - 2009 - Calibration commentary.pdf:pdf},
journal = {Radiocarbon},
number = {1},
pages = {287--290},
title = {{Calibration commentary}},
volume = {51},
year = {2009}
}
@techreport{CLSI2014,
address = {Pennsylvania},
author = {{Clinical and Laboratory Standards Institute (CLSI)}},
booktitle = {CLSI documents EP05-A3},
institution = {Clinical and Laboratory Standards Institute},
title = {{Measurement Procedure Comparison and Bias Estimation Using Patient Samples; Approved Guideline—Third Edition (EP05-A3)}},
year = {2014}
}
@article{Rozet2011,
abstract = {Bioanalytical method validation is a mandatory step to evaluate the ability of developed methods to provide accurate results for their routine application in order to trust the critical decisions that will be made with them. Even if several guidelines exist to help perform bioanalytical method validations, there is still the need to clarify the meaning and interpretation of bioanalytical method validation criteria and methodology. Yet, different interpretations can be made of the validation guidelines as well as for the definitions of the validation criteria. This will lead to diverse experimental designs implemented to try fulfilling these criteria. Finally, different decision methodologies can also be interpreted from these guidelines. Therefore, the risk that a validated bioanalytical method may be unfit for its future purpose will depend on analysts personal interpretation of these guidelines. The objective of this review is thus to discuss and highlight several essential aspects of methods validation, not only restricted to chromatographic ones but also to ligand binding assays owing to their increasing role in biopharmaceutical industries. The points that will be reviewed are the common validation criteria, which are selectivity, standard curve, trueness, precision, accuracy, limits of quantification and range, dilutional integrity and analyte stability. Definitions, methodology, experimental design and decision criteria are reviewed. Two other points closely connected to method validation are also examined: incurred sample reproducibility testing and measurement uncertainty as they are highly linked to bioanalytical results reliability. Their additional implementation is foreseen to strongly reduce the risk of having validated a bioanalytical method unfit for its purpose.},
annote = {
        From Duplicate 1 ( 
        
        
          Advances in validation, risk and uncertainty assessment of bioanalytical methods.
        
        
         - Rozet, E; Marini, R D; Ziemons, E; Boulanger, B; Hubert, Ph )

        
        
Helpful (although poorly edited!): States some things more plainly than in the CLSI guidelines. And offers some opinions or trends more recent than the most recent guidelines (generally early 2000s).

        

        From Duplicate 2 ( 
        
        
          Advances in validation, risk and uncertainty assessment of bioanalytical methods.
        
        
         - Rozet, E; Marini, R D; Ziemons, E; Boulanger, B; Hubert, Ph )

        
        
Helpful (although poorly edited!): States some things more plainly than in the CLSI guidelines.  And offers some opinions or trends more recent than the most recent guidelines (generally early 2000s).

        

      },
author = {Rozet, E and Marini, R D and Ziemons, E and Boulanger, B and Hubert, Ph},
doi = {10.1016/j.jpba.2010.12.018},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Rozet\_2011.pdf:pdf},
issn = {1873-264X},
journal = {Journal of pharmaceutical and biomedical analysis},
keywords = {Analytical,Analytical: methods,Animals,Calibration,Chemistry,Chemistry Techniques,Chromatography,Chromatography: instrumentation,Chromatography: methods,Data Interpretation,Dose-Response Relationship,Drug,Equipment Design,Humans,Pharmaceutical,Pharmaceutical Preparations,Pharmaceutical Preparations: analysis,Pharmaceutical: methods,Reproducibility of Results,Risk,Statistical,United States,United States Food and Drug Administration},
month = jun,
number = {4},
pages = {848--58},
pmid = {21237607},
publisher = {Elsevier B.V.},
title = {{Advances in validation, risk and uncertainty assessment of bioanalytical methods}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21237607},
volume = {55},
year = {2011}
}
@article{Schwartz2009,
abstract = {BACKGROUND: Evaluating new technology in clinical practice is an important component of translating research into practice. We considered the feasibility of using a Clinical Laboratory Improvement Amendments (CLIA)-waived point of care (POC) glycohemoglobin (HbA1c) methodology in busy family medicine centers by comparing the results of POC HbA1c and laboratory analysis results.

METHODS: Recruited from 5 MetroNet practices, the participants were adult diabetic patients having blood samples drawn for laboratory analysis of HbA1c. Each agreed to provide a capillary blood sample for POC testing.

RESULTS: With data on 99 paired samples, the POC method yielded a mean HbA1c of 7.38\%, which was equivalent to the mean of 7.53\% produced with all combined standard laboratory analyses. The Pearson correlation between POC and the laboratory analysis test results was 0.884 (P < .001). POC test sensitivity was 81.8\% and specificity was 93.2\%. Eighteen percent of patients with an HbA1c > = 7\% by laboratory analysis were not identified as such by the POC test.

CONCLUSIONS: Before adopting a POC methodology, practices are encouraged to review its feasibility in the context of the office routine, and also to conduct periodic comparisons of the accuracy of POC test results compared with those from laboratory analysis.},
author = {Schwartz, Kendra L and Monsur, Joseph and Hammad, Adnan and Bartoces, Monina G and Neale, Anne Victoria},
doi = {10.3122/jabfm.2009.04.090057},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Schwartz\_2009.pdf:pdf},
issn = {1557-2625},
journal = {Journal of the American Board of Family Medicine : JABFM},
keywords = {Adolescent,Adult,Cross-Sectional Studies,Diabetes Mellitus,Diabetes Mellitus: blood,Diagnostic Tests, Routine,Diagnostic Tests, Routine: standards,Family Practice,Hemoglobin A, Glycosylated,Hemoglobin A, Glycosylated: analysis,Humans,Michigan,Point-of-Care Systems,Sensitivity and Specificity,Young Adult},
number = {4},
pages = {461--3},
pmid = {19587262},
title = {{Comparison of point of care and laboratory HbA1c analysis: a MetroNet study.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/19587262},
volume = {22},
year = {2009}
}
@article{Choi2007,
abstract = {The mathematical model for predicting the precision, limit of detection (LOD) and range of quantitation (ROQ) in a competitive enzyme-linked immunosorbent assay (ELISA) proposed by Hayashi et al. (Anal. Chem., 2004, 76, 1295) was validated. The model describes the relative standard deviation (RSD) of concentration estimates by the RSDs of pipetting volumes of analyte, enzyme-conjugated antigen, antibody and substrate solutions, and the standard deviation (SD) of inherent absorbances between the wells in an ELISA plate. For 6 kinds of direct competitive ELISA kits, the LOD and ROQ predicted by the model agreed well with those obtained by experiments with real samples. It was also confirmed that the model is applicable to the prediction of uncertainty that depends on the pipetting error of the viscous antiserum solution. The model was demonstrated to be useful for estimating the LOD and ROQ of competitive ELISA.},
author = {Choi, Dong Hwan and Katakura, Yoshio and Matsuda, Rieko and Hayashi, Yuzuru and Hirobe, Masato and Goda, Yasuhiro and Ninomiya, Kazuaki and Shioya, Suteaki},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi et al. - 2007 - Validation of a method for predicting the precision, limit of detection and range of quantitation in competitive EL.pdf:pdf},
issn = {0910-6340},
journal = {Analytical sciences : the international journal of the Japan Society for Analytical Chemistry},
keywords = {Benzophenones,Benzophenones: analysis,Calibration,Enzyme-Linked Immunosorbent Assay,Enzyme-Linked Immunosorbent Assay: methods,Enzyme-Linked Immunosorbent Assay: standards,Estradiol,Estradiol: analysis,Estrogens,Estrogens: analysis,Estrone,Estrone: analysis,Fenitrothion,Humans,Immune Sera,Immune Sera: analysis,Medical Laboratory Science,Medical Laboratory Science: methods,Medical Laboratory Science: standards,Reproducibility of Results,Sensitivity and Specificity,Time Factors,elisa},
mendeley-tags = {elisa},
month = feb,
number = {2},
pages = {215--8},
pmid = {17297235},
title = {{Validation of a method for predicting the precision, limit of detection and range of quantitation in competitive ELISA.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17297235},
volume = {23},
year = {2007}
}
@incollection{Wild2013,
annote = {
        From Duplicate 1 ( 
        
          Standardization and Calibration
        
         - Wild, David; Sheehan, Chris )

        
        

        First reading: 18Jul2013
          
Context: background for ELISA project with Nandini

        
This chapter helps understand, in more detail, the molecular and laboratory procedures that generate the data and potentially the uncertainty (sometimes directional bias) that will be observed in ELISA data analysis.  There is probably little in the first half that is actionable for data analysts, but may guide the clinical chemists' design decisions for better test properties and data analysis results.

        
Standardization: "process of ensuring that all methods for determining concentration of a particular analyte give the same results"

        
[Reliability, validity and consistency] are needed for clinical and publication purposes.  Changing reference values (especially if not well communicated), originally published to guide pratitioners, can lead to frustration, lack of confidence in the tests and, worse, misdiagnosis.

        
          
Summary of Standardizition sections (for our purposes):
        

        
The first 3 1/2 pages of this chapter discuss the importance of external validity and the elements that contribute it.  While I do not think it it plays a direct role in our early studies, which deal with data analysis methods for samples within Achira [and small related population studies], some forethought to it is needed:  (These are questions for the clinical chemists.) Any design considerations that are likely to affect external validity and that are best layed down/determined early in the development process?

        
Provides context for Noble et al. (2008)

        
Some points in 'Sample and Calibrator Matrices' may inform decisions about unknown concentration estimation (mainly from patient samples). [Still need to synthesise annotations]

        

        'Calibration' section (p. 6):
        

        
"Calibration is the process of assigning values to unknown samples using a standard"

        
"Most proteins of clinical importance exhibit some degree of heterogeneity in patient samples"

        
Fascinating! My head is swimming. Summary not complete.  Will re-read after reading 'Calibration curve-fitting' and 'Method evaluation' chapters.

        

        From Duplicate 2 ( 
        
          Standardization and Calibration
        
         - Wild, David; Sheehan, Chris )

        
        

        From Duplicate 1 ( 
        
        
          Standardization and Calibration
        
        
         - Wild, David; Sheehan, Chris )

        
        

        From Duplicate 1 ( 
        
        
          Standardization and Calibration
        
        
         - Wild, David; Sheehan, Chris )

        
        

        From Duplicate 1 ( 
        
        
          Standardization and Calibration
        
        
         - Wild, David; Sheehan, Chris )

        
        

        First reading: 18Jul2013
          
Context: background for ELISA project with Nandini

        
This chapter helps understand, in more detail, the molecular and laboratory procedures that generate the data and potentially the uncertainty (sometimes directional bias) that will be observed in ELISA data analysis.  There is probably little in the first half that is actionable for data analysts, but may guide the clinical chemists' design decisions for better test properties and data analysis results.

        
Standardization: "process of ensuring that all methods for determining concentration of a particular analyte give the same results"

        
[Reliability, validity and consistency] are needed for clinical and publication purposes.  Changing reference values (especially if not well communicated), originally published to guide pratitioners, can lead to frustration, lack of confidence in the tests and, worse, misdiagnosis.

        
          
Summary of Standardizition sections (for our purposes):
        

        
The first 3 1/2 pages of this chapter discuss the importance of external validity and the elements that contribute it.  While I do not think it it plays a direct role in our early studies, which deal with data analysis methods for samples within Achira [and small related population studies], some forethought to it is needed:  (These are questions for the clinical chemists.) Any design considerations that are likely to affect external validity and that are best layed down/determined early in the development process?

        
Provides context for Noble et al. (2008)

        
Some points in 'Sample and Calibrator Matrices' may inform decisions about unknown concentration estimation (mainly from patient samples). [Still need to synthesise annotations]

        

        'Calibration' section (p. 6):
        

        
"Calibration is the process of assigning values to unknown samples using a standard"

        
"Most proteins of clinical importance exhibit some degree of heterogeneity in patient samples"

        
Fascinating! My head is swimming. Summary not complete.  Will re-read after reading 'Calibration curve-fitting' and 'Method evaluation' chapters.

        

        From Duplicate 2 ( 
        
        
          Standardization and Calibration
        
        
         - Wild, David; Sheehan, Chris )

        
        
 First reading: 18Jul2013 
Context: background for ELISA project with Nandini

        
This chapter helps understand, in more detail, the molecular and laboratory procedures that generate the data and potentially the uncertainty (sometimes directional bias) that will be observed in ELISA data analysis. There is probably little in the first half that is actionable for data analysts, but may guide the clinical chemists' design decisions for better test properties and data analysis results.

        
Standardization: "process of ensuring that all methods for determining concentration of a particular analyte give the same results"

        
[Reliability, validity and consistency] are needed for clinical and publication purposes. Changing reference values (especially if not well communicated), originally published to guide pratitioners, can lead to frustration, lack of confidence in the tests and, worse, misdiagnosis.

        
Summary of Standardizition sections (for our purposes): 

        
The first 3 1/2 pages of this chapter discuss the importance of external validity and the elements that contribute it. While I do not think it it plays a direct role in our early studies, which deal with data analysis methods for samples within Achira [and small related population studies], some forethought to it is needed: (These are questions for the clinical chemists.) Any design considerations that are likely to affect external validity and that are best layed down/determined early in the development process?

        
Provides context for Noble et al. (2008)

        
Some points in 'Sample and Calibrator Matrices' may inform decisions about unknown concentration estimation (mainly from patient samples). [Still need to synthesise annotations]

        
'Calibration' section (p. 6): 

        
"Calibration is the process of assigning values to unknown samples using a standard"

        
"Most proteins of clinical importance exhibit some degree of heterogeneity in patient samples"

        
Fascinating! My head is swimming. Summary not complete. Will re-read after reading 'Calibration curve-fitting' and 'Method evaluation' chapters.

        

        
          
From Duplicate 3 ( 
        
        
          CHAPTER 3.5 - Standardization and Calibration
        
        
         - Wild, David; Sheehan, Chris )

        
        

        

        

        

        

        

        

        From Duplicate 2 ( 
        
        
          Standardization and Calibration
        
        
         - Wild, David; Sheehan, Chris )

        
        

        From Duplicate 1 ( 
        
        
          Standardization and Calibration
        
        
         - Wild, David; Sheehan, Chris )

        
        

        First reading: 18Jul2013
          
Context: background for ELISA project with Nandini

        
This chapter helps understand, in more detail, the molecular and laboratory procedures that generate the data and potentially the uncertainty (sometimes directional bias) that will be observed in ELISA data analysis.  There is probably little in the first half that is actionable for data analysts, but may guide the clinical chemists' design decisions for better test properties and data analysis results.

        
Standardization: "process of ensuring that all methods for determining concentration of a particular analyte give the same results"

        
[Reliability, validity and consistency] are needed for clinical and publication purposes.  Changing reference values (especially if not well communicated), originally published to guide pratitioners, can lead to frustration, lack of confidence in the tests and, worse, misdiagnosis.

        
          
Summary of Standardizition sections (for our purposes):
        

        
The first 3 1/2 pages of this chapter discuss the importance of external validity and the elements that contribute it.  While I do not think it it plays a direct role in our early studies, which deal with data analysis methods for samples within Achira [and small related population studies], some forethought to it is needed:  (These are questions for the clinical chemists.) Any design considerations that are likely to affect external validity and that are best layed down/determined early in the development process?

        
Provides context for Noble et al. (2008)

        
Some points in 'Sample and Calibrator Matrices' may inform decisions about unknown concentration estimation (mainly from patient samples). [Still need to synthesise annotations]

        

        'Calibration' section (p. 6):
        

        
"Calibration is the process of assigning values to unknown samples using a standard"

        
"Most proteins of clinical importance exhibit some degree of heterogeneity in patient samples"

        
Fascinating! My head is swimming. Summary not complete.  Will re-read after reading 'Calibration curve-fitting' and 'Method evaluation' chapters.

        

        From Duplicate 2 ( 
        
        
          Standardization and Calibration
        
        
         - Wild, David; Sheehan, Chris )

        
        
 First reading: 18Jul2013 
Context: background for ELISA project with Nandini

        
This chapter helps understand, in more detail, the molecular and laboratory procedures that generate the data and potentially the uncertainty (sometimes directional bias) that will be observed in ELISA data analysis. There is probably little in the first half that is actionable for data analysts, but may guide the clinical chemists' design decisions for better test properties and data analysis results.

        
Standardization: "process of ensuring that all methods for determining concentration of a particular analyte give the same results"

        
[Reliability, validity and consistency] are needed for clinical and publication purposes. Changing reference values (especially if not well communicated), originally published to guide pratitioners, can lead to frustration, lack of confidence in the tests and, worse, misdiagnosis.

        
Summary of Standardizition sections (for our purposes): 

        
The first 3 1/2 pages of this chapter discuss the importance of external validity and the elements that contribute it. While I do not think it it plays a direct role in our early studies, which deal with data analysis methods for samples within Achira [and small related population studies], some forethought to it is needed: (These are questions for the clinical chemists.) Any design considerations that are likely to affect external validity and that are best layed down/determined early in the development process?

        
Provides context for Noble et al. (2008)

        
Some points in 'Sample and Calibrator Matrices' may inform decisions about unknown concentration estimation (mainly from patient samples). [Still need to synthesise annotations]

        
'Calibration' section (p. 6): 

        
"Calibration is the process of assigning values to unknown samples using a standard"

        
"Most proteins of clinical importance exhibit some degree of heterogeneity in patient samples"

        
Fascinating! My head is swimming. Summary not complete. Will re-read after reading 'Calibration curve-fitting' and 'Method evaluation' chapters.

        

        
          
From Duplicate 3 ( 
        
        
          CHAPTER 3.5 - Standardization and Calibration
        
        
         - Wild, David; Sheehan, Chris )

        
        

        

        

        

        

        

        

      },
author = {Wild, David and Sheehan, Chris},
booktitle = {The Immunoassay Handbook},
chapter = {3.5},
doi = {10.1016/B978-0-08-097037-0.00009-9},
edition = {Fourth},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Wild\_2013a.pdf:pdf;:D$\backslash$:/Home/SkyDrive/Documents/Literature/Wild2013\_Chap3\_5-annotated.pdf:pdf},
isbn = {9780080970370},
pages = {315--322},
publisher = {Elsevier Ltd},
title = {{Standardization and Calibration}},
url = {http://dx.doi.org/10.1016/B978-0-08-097037-0.00009-9},
year = {2013}
}
@article{Dudley1985,
abstract = {These guidelines outline the minimum requirements for a data-processing package to be used in the immunoassay laboratory. They include recommendations on hardware, software, and program design. We outline the statistical analyses that should be performed to obtain the analyte concentrations of unknown specimens and to ensure adequate monitoring of within- and between-assay errors of measurement.},
author = {Dudley, R A and Edwards, P and Ekins, R P and Finney, D J and McKenzie, I G and Raab, G M and Rodbard, D and Rodgers, R P},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Dudley\_1985.pdf:pdf},
issn = {0009-9147},
journal = {Clinical chemistry},
keywords = {Computers,Humans,Immunologic Techniques,Mathematics,Quality Control,Reference Values,Software},
month = aug,
number = {8},
pages = {1264--71},
pmid = {3893796},
title = {{Guidelines for immunoassay data processing}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/3893796},
volume = {31},
year = {1985}
}
@techreport{Monobind2010b,
abstract = {Human Chorionic Gonadotropin (hCG), Human Prolactin (hPRL), Human Luteinizing Hormone (hLH) Follicle Stimulating Hormone (FSH)},
author = {(Monobind)},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Monobind\_2010.pdf:pdf},
number = {6},
pages = {1--4},
title = {{Monobind fertility test package insert}},
year = {2010}
}
@article{O'Malley2006,
abstract = {A Bayesian multivariate hierarchical transformation model (BMHTM) is developed for receiver operating characteristic (ROC) curve analysis based on clustered continuous diagnostic outcome data with covariates. Two special features of this model are that it incorporates non-linear monotone transformations of the outcomes and that multiple correlated outcomes may be analysed. The mean, variance, and transformation components are all modelled parametrically, enabling a wide range of inferences. The general framework is illustrated by focusing on two problems: (1) analysis of the diagnostic accuracy of a covariate-dependent univariate test outcome requiring a Box-Cox transformation within each cluster to map the test outcomes to a common family of distributions; (2) development of an optimal composite diagnostic test using multivariate clustered outcome data. In the second problem, the composite test is estimated using discriminant function analysis and compared to the test derived from logistic regression analysis where the gold standard is a binary outcome. The proposed methodology is illustrated on prostate cancer biopsy data from a multi-centre clinical trial.},
author = {O'Malley, A James and Zou, Kelly H},
doi = {10.1002/sim.2187},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/O'Malley, Zou - 2006 - Bayesian multivariate hierarchical transformation models for ROC analysis.pdf:pdf},
issn = {0277-6715},
journal = {Statistics in medicine},
keywords = {Bayes Theorem,Biopsy,Cluster Analysis,Humans,Male,Models,Multicenter Studies as Topic,Multivariate Analysis,Prospective Studies,Prostate-Specific Antigen,Prostate-Specific Antigen: blood,Prostatectomy,Prostatic Neoplasms,Prostatic Neoplasms: diagnosis,ROC Curve,Statistical},
month = feb,
number = {3},
pages = {459--79},
pmid = {16217836},
title = {{Bayesian multivariate hierarchical transformation models for ROC analysis.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1540405\&tool=pmcentrez\&rendertype=abstract},
volume = {25},
year = {2006}
}
@article{Statistics2009,
abstract = {This paper reviews the literature on Bayesian experimental design. A unified view of this topic is presented, based on a decision- theoretic approach. This framework casts criteria from the Bayesian lit- erature of design as part of a single coherent approach. The decision- theoretic structure incorporates both linear and nonlinear design problems and it suggests possible new directions to the experimental design problem, motivated by the use of new utility functions. We show that, in some special cases of linear design problems, Bayesian solutions change in a sensible way when the prior distribution and the utility func- tion are modified to allow for the specific structure of the experiment. The decision-theoretic approach also gives a mathematical justification for selecting the appropriate optimality criterion},
author = {Chaloner, Kathryn and Verdinelli, Isabella},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaloner - 1995 - Bayesian Experimental Design A Review.pdf:pdf;:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Chaloner, Verdinelli - 1995 - Bayesian Experimental Design A Review.pdf:pdf},
journal = {Statistical Science},
keywords = {bayesian,decision theory,hierarchical linear models,logistic regression,nonlinear design,nonlinear models,optimal design,optimality criteria,utility functions},
mendeley-tags = {bayesian},
number = {3},
pages = {273--304},
title = {{Bayesian Experimental Design: A Review}},
volume = {10},
year = {1995}
}
@article{Luo2011,
abstract = {Taguchi design, a statistics-based design of experiment method, is widely used for optimization of products and complex production processes in many different industries. However, its use for antibody microarray optimization has remained underappreciated. Here, we provide a brief explanation of Taguchi design and present its use for the optimization of antibody sandwich immunoassay microarray with five breast cancer biomarkers: CA15-3, CEA, HER2, MMP9, and uPA. Two successive optimization rounds with each 16 experimental trials were performed. We tested three factors (capture antibody, detection antibody, and analyte) at four different levels (concentrations) in the first round and seven factors (including buffer solution, streptavidin-Cy5 dye conjugate concentration, and incubation times for five assay steps) with two levels each in the second round; five two-factor interactions between selected pairs of factors were also tested. The optimal levels for each factor as measured by net assay signal increase were determined graphically, and the significance of each factor was analyzed statistically. The concentration of capture antibody, streptavidin-Cy5, and buffer composition were identified as the most significant factors for all assays; analyte incubation time and detection antibody concentration were significant only for MMP9 and CA15-3, respectively. Interactions between pairs of factors were identified, but were less influential compared with single factor effects. After Taguchi optimization, the assay sensitivity was improved between 7 and 68 times, depending on the analyte, reaching 640 fg/mL for uPA, and the maximal signal intensity increased between 1.8 and 3 times. These results suggest that Taguchi design is an efficient and useful approach for the rapid optimization of antibody microarrays.},
author = {Luo, Wen and Pla-Roca, Mateu and Juncker, David},
doi = {10.1021/ac103239f},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Luo, Pla-Roca, Juncker - 2011 - Taguchi design-based optimization of sandwich immunoassay microarrays for detecting breast cancer biomar.pdf:pdf},
issn = {1520-6882},
journal = {Analytical chemistry},
keywords = {Antibodies, Immobilized,Antibodies, Immobilized: immunology,Breast Neoplasms,Breast Neoplasms: diagnosis,Female,Humans,Immunoassay,Immunoassay: methods,Protein Array Analysis,Protein Array Analysis: methods,Sensitivity and Specificity,Tumor Markers, Biological,Tumor Markers, Biological: analysis,Tumor Markers, Biological: immunology},
month = jul,
number = {14},
pages = {5767--74},
pmid = {21667934},
title = {{Taguchi design-based optimization of sandwich immunoassay microarrays for detecting breast cancer biomarkers.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21667934},
volume = {83},
year = {2011}
}
@incollection{Chan1996,
author = {Chan, Daniel W},
booktitle = {Immunoassay},
chapter = {21},
edition = {1},
editor = {Christopoulos, Theodore K and Diamandis, Eleftherios P},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Christopoulos\_1996\_chap21.pdf:pdf},
pages = {483--504},
publisher = {Elsevier Inc.},
title = {{AUTOMATION OF IMMUNOASSAYS}},
year = {1996}
}
@article{Sackmann2014,
abstract = {Asthma is a chronic inflammatory disorder that affects more than 300 million people worldwide. Asthma management would benefit from additional tools that establish biomarkers to identify phenotypes of asthma. We present a microfluidic solution that discriminates asthma from allergic rhinitis based on a patient's neutrophil chemotactic function. The handheld diagnostic device sorts neutrophils from whole blood within 5 min, and generates a gradient of chemoattractant in the microchannels by placing a lid with chemoattractant onto the base of the device. This technology was used in a clinical setting to assay 34 asthmatic (n = 23) and nonasthmatic, allergic rhinitis (n = 11) patients to establish domains for asthma diagnosis based on neutrophil chemotaxis. We determined that neutrophils from asthmatic patients migrate significantly more slowly toward the chemoattractant compared with nonasthmatic patients (P = 0.002). Analysis of the receiver operator characteristics of the patient data revealed that using a chemotaxis velocity of 1.55 $\mu$m/min for asthma yields a diagnostic sensitivity and specificity of 96\% and 73\%, respectively. This study identifies neutrophil chemotaxis velocity as a potential biomarker for asthma, and we demonstrate a microfluidic technology that was used in a clinical setting to perform these measurements.},
author = {Sackmann, Eric Karl-Heinz and Berthier, Erwin and Schwantes, Elizabeth a and Fichtinger, Paul S and Evans, Michael D and Dziadzio, Laura L and Huttenlocher, Anna and Mathur, Sameer K and Beebe, David J},
doi = {10.1073/pnas.1324043111},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Sackmann\_2014.pdf:pdf},
issn = {1091-6490},
journal = {Proceedings of the National Academy of Sciences of the United States of America},
keywords = {1,2,and david j,anna huttenlocher e,b,beebe a,drop of blood using,dziadzio e,elizabeth a,eric karl-heinz sackmann a,erwin berthier b,evans d,fichtinger c,laura l,mathur c,michael d,neutrophil chemotaxis,paul s,racterizing asthma from a,sameer k,schwantes c},
month = apr,
number = {16},
pages = {5813--8},
pmid = {24711384},
title = {{Characterizing asthma from a drop of blood using neutrophil chemotaxis.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24711384},
volume = {111},
year = {2014}
}
@article{Klauenberg2011a,
abstract = {BACKGROUND: Immunoassays are biochemical tests applied to measure even very small amounts of substance using the highly specific binding between an antibody and its antigen. They have a wide range of applications. The measurement however, might be associated with substantial uncertainty; this can have significant consequences for any diagnosis, or clinical decision. An international comparability study was thus performed to assess the sources of uncertainty involved in the estimation of a protein cytokine concentration using a fluorescent ELISA. METHODS: In contrast to the original publication for this international comparability study, we reanalyse the data using Bayesian inference. This provides a statistically coherent approach to estimate ELISA concentrations and their associated uncertainties. RESULTS: The Bayesian uncertainties of individual ELISAs and laboratory estimates are considerably larger than previously reported uncertainties. The average concentrations estimated here differ from the ones estimated by each study participant. In general, this leads to different conclusions about the study. In particular, the inter- and intra-laboratory consistency is increased, and repeatability problems occur for fewer laboratories. CONCLUSIONS: Decisions which are based on plausible ranges of measurements (such as credible intervals), are generally superior to those solely based on point estimates (such as the mean). Reliable uncertainties are thus vital, and not only in metrology. In this paper, a general method is developed to derive concentration estimates and valid uncertainties for ELISAs. Guidance on applying this Bayesian method is provided and the importance of reliable uncertainties associated with ELISAs is underlined. The applicability and virtues of the presented method are demonstrated in the context of an international comparability study.},
author = {Klauenberg, Katy and Ebert, Bernd and Voigt, Jan and Walzel, Monika and Noble, James E and Knight, Alex E and Elster, Clemens},
doi = {10.1515/CCLM.2011.648},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Klauenberg et al. - 2011 - Bayesian analysis of an international ELISA comparability study.pdf:pdf},
issn = {1434-6621},
journal = {Clinical chemistry and laboratory medicine},
keywords = {Bayes Theorem,Calibration,Enzyme-Linked Immunosorbent Assay,Enzyme-Linked Immunosorbent Assay: standards,Internationality,Reference Standards,Uncertainty,bayesian,elisa},
mendeley-tags = {bayesian,elisa},
month = sep,
number = {9},
pages = {1459--1468},
pmid = {21726164},
title = {{Bayesian analysis of an international ELISA comparability study.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21726164},
volume = {49},
year = {2011}
}
@inproceedings{Dell2011,
abstract = {Many of the diagnostic tests administered in well-funded clinical laboratories are inappropriate for point-of-care test- ing in low-resource environments. As a result, inexpensive, portable immunoassay tests have been developed to facil- itate the rapid diagnosis of many diseases common to de- veloping countries. However, manually analyzing the test results at the point of care may be complex and error-prone for untrained users reading test results by eye, and providing methods for automatically processing these tests could sig- nificantly increase their utility. In this paper, we present a mobile application that automatically quantifies immunoas- say test data on a smart phone. The speed and accuracy demonstrated by the application suggest that cell-phone based analysis could aid disease diagnosis at the point of care.},
author = {Dell, Nicola and Stevens, Dean and Yager, Paul},
booktitle = {Networked Systems for Developing Regions},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Dell, Stevens, Yager - 2011 - Towards a Point-of-Care Diagnostic System Automated Analysis of Immunoassay Test Data on a Cell Phone.pdf:pdf},
isbn = {9781450307390},
keywords = {POC,computer vision,computing for development,immunoassay,mobile phone,point-of-care diagnostics,smartphone},
mendeley-tags = {POC},
number = {June},
pages = {3--8},
title = {{Towards a Point-of-Care Diagnostic System : Automated Analysis of Immunoassay Test Data on a Cell Phone}},
year = {2011}
}
@article{Gelman2005,
author = {Gelman, Andrew},
doi = {10.1214/009053604000001048},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman - 2005 - Analysis of variancewhy it is more important than ever.pdf:pdf},
issn = {0090-5364},
journal = {The Annals of Statistics},
month = feb,
number = {1},
pages = {1----53},
title = {{Analysis of variance? Why it is more important than ever}},
url = {http://projecteuclid.org/Dienst/getRecord?id=euclid.aos/1112967698/},
volume = {33},
year = {2005}
}
@incollection{Fox2010,
author = {Fox, J-P},
booktitle = {Bayesian Item Response Modeling: Theory and Applications},
chapter = {2},
doi = {10.1007/978-1-4419-0742-4},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Fox\_2010.pdf:pdf},
isbn = {9781441907424},
pages = {31--45},
title = {{Bayesian Hierarchical Response Modeling}},
year = {2010}
}
@article{Ellington2010,
abstract = {BACKGROUND: The measurement of multiple protein biomarkers may refine risk stratification in clinical settings. This concept has stimulated development of multiplexed immunoassay platforms that provide multiple, parallel protein measurements on the same specimen.

CONTENT: We provide an overview of antibody-based multiplexed immunoassay platforms and discuss technical and operational challenges. Multiplexed immunoassays use traditional immunoassay principles in which high-affinity capture ligands are immobilized in parallel arrays in either planar format or on microspheres in suspension. Development of multiplexed immunoassays requires rigorous validation of assay configuration and analytical performance to minimize assay imprecision and inaccuracy. Challenges associated with multiplex configuration include selection and immobilization of capture ligands, calibration, interference between antibodies and proteins and assay diluents, and compatibility of assay limits of quantification. We discuss potential solutions to these challenges. Criteria for assessing analytical multiplex assay performance include the range of linearity, analytical specificity, recovery, and comparison to a quality reference method. Quality control materials are not well developed for multiplexed protein immunoassays, and algorithms for interpreting multiplex quality control data are needed.

SUMMARY: Technical and operational challenges have hindered implementation of multiplexed assays in clinical settings. Formal procedures that guide multiplex assay configuration, analytical validation, and quality control are needed before broad application of multiplexed arrays can occur in the in vitro diagnostic market.},
author = {Ellington, Allison a and Kullo, Iftikhar J and Bailey, Kent R and Klee, George G},
doi = {10.1373/clinchem.2009.127514},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Ellington\_2010.pdf:pdf},
issn = {1530-8561},
journal = {Clinical chemistry},
keywords = {Antibodies,Antibodies: chemistry,Biological Markers,Biological Markers: blood,Immunoassay,Immunoassay: methods,Proteins,Proteins: analysis},
month = feb,
number = {2},
pages = {186--93},
pmid = {19959625},
title = {{Antibody-based protein multiplex platforms: technical and operational challenges.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2901849\&tool=pmcentrez\&rendertype=abstract},
volume = {56},
year = {2010}
}
@article{OMalley2008b,
author = {O'Malley, A James and Smith, Murray H and Sadler, William A},
doi = {10.1111/j.1467-842X.2008.00506.x},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/OMalley\_2008b.pdf:pdf},
issn = {13691473},
journal = {Australian \& New Zealand Journal of Statistics},
keywords = {heteroscedastic responses,maximum likelihood,non-linear regression,nuisance parameter,restricted,variance function estimation},
month = jun,
number = {2},
pages = {161--177},
title = {{A Restricted Maximum Likelihood Procedure for Estimating the Variance Function of an Immunoassay}},
url = {http://doi.wiley.com/10.1111/j.1467-842X.2008.00506.x},
volume = {50},
year = {2008}
}
@inproceedings{Ziliak2009,
abstract = {We want to persuade you of one claim: that William Sealy Gosset (1876-1937)—aka "Student" of "Student's" t-test—was right, and that his difficult friend, Ronald A. Fisher (1890-1962), though a genius, was wrong. Fit is not the same thing as importance. Statistical significance is not the same thing as scientific importance or economic sense. But the mistaken equation is made, we find, in 8 or 9 of every 10 articles appearing in the leading journals of science, economics to medicine. The history of this "standard error" of science involves varied characters and plot twists, but especially R. A. Fisher's canonical translation of "Student's" t. William S. Gosset aka “Student,” who was for most of his life Head Experimental Brewer at Guinness, took an economic approach to the logic of uncertainty. Against Gosset’s wishes his friend Fisher erased the consciously economic element, Gosset's "real error.” We want to bring it back.},
author = {Ziliak, Stephen T and McCloskey, Deidre N},
booktitle = {JSM, section on statistical education},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Ziliak\_2009.pdf:pdf},
pages = {2302--2316},
title = {{The Cult of Statistical Significance}},
year = {2009}
}
@article{Finney1983,
abstract = {In quantitative estimates from radioimmunoassay, one of four types of response curves is usually used: a freehand curve, a spline function, an equation based upon mass-action considerations, or a logistic equation. This paper comments briefly on the subjectivity and labor of the first and on the overparametrization of the second. It is chiefly concerned to compare the single binding-site equation with a simple or modified logistic. Whatever the theoretical merits of the binding-site approach (these are not under discussion), estimation of parameters is difficult. The paper shows that under many but not all circumstances a four- or five-parameter logistic will fit data at least as well over a wide range of doses. This is particularly so when both the binding-site concentration and the equilibrium constant are small.},
author = {Finney, D J},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Finney\_1983.pdf:pdf},
journal = {Clinical Chemistry},
number = {10},
pages = {1762--1766},
title = {{Response Curves for Radioimmunoassay}},
volume = {29},
year = {1983}
}
@article{Findlay2007,
abstract = {Calibration curves for ligand binding assays are generally characterized by a nonlinear relationship between the mean response and the analyte concentration. Typically, the response exhibits a sigmoidal relationship with concentra- tion. The currently accepted reference model for these cali- bration curves is the 4-parameter logistic (4-PL) model, which optimizes accuracy and precision over the maximum usable calibration range. Incorporation of weighting into the model requires additional effort but generally results in improved calibration curve performance. For calibration curves with some asymmetry, introduction of a fi fth param- eter (5-PL) may further improve the goodness of fi t of the experimental data to the algorithm. Alternative models should be used with caution and with knowledge of the accu- racy and precision performance of the model across the entire calibration range, but particularly at upper and lower analyte concentration areas, where the 4- and 5-PL algorithms gen- erally outperform alternative models. Several assay design parameters, such as placement of calibrator concentrations across the selected range and assay layout on multiwell plates, should be considered, to enable optimal application of the 4- or 5-PL model. The fi t of the experimental data to the model should be evaluated by assessment of agreement of nominal and model-predicted data for calibrators.},
author = {Findlay, John W A and Dillard, Robert F},
doi = {10.1208/aapsj0902029},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Findlay, Dillard - 2007 - Appropriate Calibration Curve Fitting in Ligand Binding Assays.pdf:pdf},
isbn = {1550-7416},
issn = {1550-7416},
journal = {AAPS Journal},
keywords = {4/5-parameter logistic models,Ligand-binding assay,assay design parameters,elisa,nonlinear calibration},
mendeley-tags = {elisa},
number = {2},
pages = {260--267},
pmid = {17907767},
title = {{Appropriate Calibration Curve Fitting in Ligand Binding Assays}},
volume = {9},
year = {2007}
}
@article{Weiss1997,
author = {Weiss, James N},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Weiss\_1997.pdf:pdf},
isbn = {1110987654321},
journal = {FASEB Journal},
keywords = {binding,calibration curve,enzymes,ion channels,ligand-receptor interaction,ligands,receptors},
mendeley-tags = {calibration curve},
pages = {835--841},
title = {{The Hill equation revisited: uses and misuses}},
volume = {11},
year = {1997}
}
@article{Fong2012a,
abstract = {Summary. In the context of a bioassay or an immunoassay, calibration means fitting a curve, usually nonlinear, through the observations collected on a set of samples containing known concentrations of a target substance, and then using the fitted curve and observations collected on samples of interest to predict the concentrations of the target substance in these samples. Recent technological advances have greatly improved our ability to quantify minute amounts of substance from a tiny volume of biological sample. This has in turn led to a need to improve statistical methods for calibration. In this article, we focus on developing calibration methods robust to dependent outliers. We introduce a novel normal mixture model with dependent error terms to model the experimental noise. In addition, we propose a reparameterization of the five parameter logistic nonlinear regression model that allows us to better incorporate prior information. We examine the performance of our methods with simulation studies and show that they lead to a substantial increase in performance measured in terms of mean squared error of estimation and a measure of the average prediction accuracy. A real data example from the HIV Vaccine Trials Network Laboratory is used to illustrate the methods.},
author = {Fong, Youyi and Wakefield, Jon and {De Rosa}, S and Frahm, N},
doi = {10.1111/j.1541-0420.2012.01762.x},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Fong\_2012.pdf:pdf},
journal = {Biometrics},
keywords = {5pl curve,AR(1) process,calibration,correlated outliers,dose response curve,multiplex bead array},
number = {December},
pages = {1103--1112},
title = {{A Robust Bayesian Random Effects Model for Nonlinear Calibration Problems}},
volume = {68},
year = {2012}
}
@article{Fedorov2001,
abstract = {We discuss optimal experimental design issues for nonlinear models arising in dose response studies. The optimization is performed with respect to various criteria which depend on the Fisher information matrix. Special attention is given to models with a variance component that depends on unknown parameters.},
author = {Fedorov, Valerii V and Leonov, Sergei L},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Fedorov, Leonov - 2001 - OPTIMAL DESIGN OF DOSE RESPONSE EXPERIMENTS A Model-Oriented Approach.pdf:pdf},
journal = {Drug Information Journal},
keywords = {dose response,locally optimal designs,nonlinear regression models,optimal design of experiments,variance depending on unknown parameters},
pages = {1373--1383},
title = {{OPTIMAL DESIGN OF DOSE RESPONSE EXPERIMENTS : A Model-Oriented Approach}},
volume = {35},
year = {2001}
}
@book{Meehl1954,
author = {Meehl, Paul E},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Meehl\_1954\_ClinstixBook.pdf:pdf},
publisher = {University of Minnesota},
title = {{Clinical versus Statistical Prediction: A Theoretical Analysis and a Review of the Evidence}},
year = {1954}
}
@techreport{Krouwer2002,
abstract = {NCCLS document EP9-A2—Method Comparison and Bias Estimation Using Patient Samples; Approved Guideline—Second Edition, is written for laboratorians as well as manufacturers. It describes procedures for determining the relative bias between two methods, and it identifies factors to be considered when designing and analyzing a method-comparison experiment using split patient samples. For carrying out method-comparison evaluations, an overview of the experiment, sample data recording and calculation sheets, and an overview flowchart and a detailed flowchart for preliminary data examination are included. As an additional aid, a sample scatter plot and bias plot are introduced for those who are unfamiliar with these procedures. The final section contains recommendations for manufacturers' evaluation of bias and statement format for bias claims.},
author = {Krouwer, Jan S and Tholen, Daniel W and Garber, Carl C and Goldschmidt, Henk M J and Kroll, Martin Harris and Linnet, Kristian and Meier, Kristen and Robinowitz, Max and Kennedy, John W},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/EP09-A2.pdf:pdf},
institution = {Clinical Laboratory Standards Institute},
isbn = {1562384724},
keywords = {clsi},
mendeley-tags = {clsi},
number = {19},
title = {{Method Comparison and Bias Estimation Using Patient Samples; Approved Guideline — Second Edition}},
volume = {22},
year = {2002}
}
@incollection{Sokoll2013,
author = {Sokoll, Lori J and Chan, Daniel W},
booktitle = {The Immunoassay Handbook},
chapter = {6.4},
doi = {10.1016/B978-0-08-097037-0.00020-8},
edition = {Fourth Ed},
editor = {Wild, David},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sokoll, Chan - 2013 - Choosing an Automated Immunoassay System.pdf:pdf},
isbn = {9780080970370},
pages = {465--468},
publisher = {Elsevier Ltd},
title = {{Choosing an Automated Immunoassay System}},
url = {http://dx.doi.org/10.1016/B978-0-08-097037-0.00020-8},
year = {2013}
}
@incollection{John2013,
annote = {
        From Duplicate 1 ( 
        
          Thyroid
        
         - John, Rhys; Christofides, Nic; Spencer, Carole A; Wild, David )

        
        

        From Duplicate 1 ( 
        
        
          Thyroid
        
        
         - John, Rhys; Christofides, Nic; Spencer, Carole A; Wild, David )
And  Duplicate 2 ( 
        
        
          Thyroid
        
        
         - John, Rhys; Christofides, Nic; Spencer, Carole A; Wild, David )
And  Duplicate 3 ( 
        
        
          Thyroid
        
        
         - John, Rhys; Christofides, Nic; Spencer, Carole A; Wild, David )
And  Duplicate 4 ( 
        
        
          Thyroid
        
        
         - John, Rhys; Christofides, Nic; Spencer, Carole A; Wild, David )
And  Duplicate 5 ( 
        
        
          Thyroid
        
        
         - John, Rhys; Christofides, Nic; Spencer, Carole A; Wild, David )
And  Duplicate 6 ( 
        
        
          Thyroid
        
        
         - John, Rhys; Christofides, Nic; Spencer, Carole A; Wild, David )
And  Duplicate 8 ( 
        
        
          Thyroid
        
        
         - John, Rhys; Christofides, Nic; Spencer, Carole A; Wild, David )

        
        

        

        

        

        

        From Duplicate 2 ( 
        
          Thyroid
        
         - John, Rhys; Christofides, Nic; Spencer, Carole A; Wild, David )
And  Duplicate 3 ( 
        
          Thyroid
        
         - John, Rhys; Christofides, Nic; Spencer, Carole A; Wild, David )

        
        

        

        

      },
author = {John, Rhys and Christofides, Nic and Spencer, Carole A and Wild, David},
booktitle = {The Immunoassay Handbook},
chapter = {9.2},
doi = {10.1016/B978-0-08-097037-0.00045-2},
edition = {Fourth Ed},
editor = {Wild, David},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/John et al. - 2013 - Thyroid.pdf:pdf},
isbn = {9780080970370},
pages = {673--693},
publisher = {Elsevier Ltd},
title = {{Thyroid}},
year = {2013}
}
@article{Gelman2003,
abstract = {Exploratory data analysis (EDA) and Bayesian inference (or, more generally, complex statistical modeling) --which are generally considered as unrelated statistical paradigms--can be particularly ef- fective in combination. In this paper, we present a Bayesian framework for EDA based on posterior predictive checks. We explain how posterior predictive simulations can be used to create reference dis- tributions for EDA graphs, and how this approach resolves some theoretical problems in Bayesian data analysis. We show how the generalization of Bayesian inference to include replicated data yr'e and repli- cated parameters Hrep follows a long tradition of generalizations in Bayesian theory. On the theoretical level, we present a predictive Bayesian formulation of goodness-of-fit testing, dis- tinguishing between p-values (posterior probabilities that specified antisymmetric discrepancy measures will exceed 0) and u-values (data summaries with uniform sampling distributions). We explain that p- values, unlike u-values, are Bayesian probability statements in that they condition on observed data. Having reviewed the general theoretical framework, we discuss the implications for statistical graphics and exploratory data analysis, with the goal being to unify exploratory data analysis with more formal statistical methods based on probability models. We interpret various graphical displays as posterior predictive checks and discuss how Bayesian inference can be used to determine reference distributions. The goal of this work is not to downgrade descriptive statistics, or to suggest they be replaced by Bayesian modeling, but rather to suggest how exploratory data analysis fits into the probability-modeling paradigm. We conclude with a discussion of the implications for practical Bayesian inference. In particular, we an- ticipate that Bayesian software can be generalized to draw simulations of replicated data and parameters from their posterior predictive distribution, and these can in turn be used to calibrate EDA graphs.},
author = {Gelman, Andrew},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Gelman\_2003.pdf:pdf},
journal = {International Statistical Review},
keywords = {Bootstrap,Fisher's exact test,Graphics,Mixture model,Model checking,Multiple imputation,Posterior predictive check,Prior predictive check,p-value,u-value},
number = {2},
pages = {369--382},
title = {{A Bayesian Formulation of Exploratory Data Analysis and Goodness-of-fit Testing *}},
volume = {71},
year = {2003}
}
@article{Kottner2011,
abstract = {Objective: Results of reliability and agreement studies are intended to provide information about the amount of error inherent in any diagnosis, score, or measurement. The level of reliability and agreement among users of scales, instruments, or classifications is widely unknown. Therefore, there is a need for rigorously conducted interrater and intrarater reliability and agreement studies. Information about sample selection, study design, and statistical analysis is often incomplete. Because of inadequate reporting, interpretation and synthesis of study results are often difficult. Widely accepted criteria, standards, or guidelines for reporting reliability and agreement in the health care and medical field are lacking. The objective was to develop guidelines for reporting reliability and agreement studies. Study Design and Setting: Eight experts in reliability and agreement investigation developed guidelines for reporting. Results: Fifteen issues that should be addressed when reliability and agreement are reported are proposed. The issues correspond to the headings usually used in publications. Conclusion: The proposed guidelines intend to improve the quality of reporting.},
author = {Kottner, Jan and Audige, Laurent and Brorson, Stig and Donner, Allan and Gajewski, Byron J and Hrobjartsson, Asbjorn and Roberts, Chris and Shoukri, Mohamed M and Streiner, David L},
doi = {10.1016/j.jclinepi.2010.03.002},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Kottner\_2011.pdf:pdf},
journal = {Journal of clinical epidemiology},
keywords = {agreement,agreement studies,guideline,interrater,intrarater,reliability,reproducibility},
mendeley-tags = {agreement studies},
pages = {96--106},
title = {{Guidelines for Reporting Reliability and Agreement Studies ( GRRAS ) were proposed}},
volume = {64},
year = {2011}
}
@article{Zeng1997,
abstract = {A common assumption in the analysis of immunoassay data is a similar pattern of within-run variation across runs of the assays. One makes this assumption without formal investigation of its validity, despite the widely acknowledged fact that accurate understanding of intra-run variation is critical to reliable calibration inference. We propose a simple procedure for a formal test of the assumption of the homogeneity of parameters that characterize intra-run variation based on representation of standard curve data from multiple assay runs by a non-linear mixed effects model. We examine the performance of the procedure and investigate the robustness of calibration inference to incorrect assumptions about the pattern of intra-run variation.},
author = {Zeng, Q and Davidian, M},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Zeng\_1997b.pdf:pdf},
issn = {0277-6715},
journal = {Statistics in medicine},
keywords = {Calibration,Confidence Intervals,Deoxyribonucleases,Deoxyribonucleases: analysis,Enzyme-Linked Immunosorbent Assay,Enzyme-Linked Immunosorbent Assay: standards,Immunoassay,Immunoassay: standards,Least-Squares Analysis,Models, Statistical,Monte Carlo Method,Nonlinear Dynamics,Random Allocation,Reproducibility of Results,Sample Size},
month = aug,
number = {15},
pages = {1765--76},
pmid = {9265699},
title = {{Testing homogeneity of intra-run variance parameters in immunoassay.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/9265699},
volume = {16},
year = {1997}
}
@article{Plikaytis1991,
abstract = {We examined several of the more commonly used models (log-log, two forms of the logit-log, and the four-parameter logistic-log transformations) for forming standard or calibration curves by using a standardized enzyme-linked immunosorbent assay (ELISA). Assay range, accuracy, and error for each function were measured and compared. Antibody levels to Neisseria meningitidis group A polysaccharide were estimated by calculating antibody concentrations of a serially diluted standard reference serum of known concentration. Each function achieved a high squared correlation coefficient (r2 greater than 0.97), indicating a high degree of accuracy in forming the standard curves. However, when predicted antibody concentrations were compared with the known values, the log-log function exhibited the least precision, with extreme percentages of error occurring at several dilutions. A partially specified logit-log transformation performed better than the log-log model over a reduced range of standard dilutions. This indicated that a high r2 alone was not a reliable measure of the accuracy of the standard curve. Of the methods surveyed, the logistic-log and fully specified logit-log functions were the most accurate models for forming standard curves and for interpolating antibody concentrations from the standard curve. The accuracy of the fully specified logit-log function is highly dependent on the precise specification of two unknown quantities, the optical densities at zero and infinite concentrations, prior to fitting the model to a typical set of calibration data. The four-parameter logistic-log function was the preferred choice for quantitating N. meningitidis group A total polysaccharide antibody by using a standardized ELISA. The function does not require prespecification of any parameters before estimating the standard curve, and the four parameters are readily interpretable in terms of identifiable physical quantities. This model also has the advantage that it is easiest to visualize since it does not incorporate complex transformations of the optical density scale.},
author = {Plikaytis, Brian D and Turner, Susan H and Gheesling, LLinda L and Carlone, George M},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Plikaytis et al. - 1991 - Comparisons of standard curve-fitting methods to quantitate Neisseria meningitidis group A polysaccharide anti.pdf:pdf},
issn = {0095-1137},
journal = {Journal of clinical microbiology},
keywords = {Antibodies,Bacterial,Bacterial: analysis,Bacterial: classification,Bacterial: immunology,Enzyme-Linked Immunosorbent Assay,Enzyme-Linked Immunosorbent Assay: standards,Enzyme-Linked Immunosorbent Assay: statistics \& nu,Humans,Logistic Models,Neisseria meningitidis,Neisseria meningitidis: immunology,Polysaccharides,Reference Standards,elisa},
mendeley-tags = {elisa},
month = jul,
number = {7},
pages = {1439--46},
pmid = {1909345},
title = {{Comparisons of standard curve-fitting methods to quantitate Neisseria meningitidis group A polysaccharide antibody levels by enzyme-linked immunosorbent assay.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=270131\&tool=pmcentrez\&rendertype=abstract},
volume = {29},
year = {1991}
}
@incollection{Sheehan2013,
author = {Sheehan, Chris and He, Jianwen and Smith, Mari},
booktitle = {The Immunoassay Handbook},
chapter = {5.2},
doi = {10.1016/B978-0-08-097037-0.00089-0},
edition = {Fourth Ed},
editor = {Wild, David},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sheehan, He, Smith - 2013 - Method Evaluation — A Practical Guide.pdf:pdf},
isbn = {9780080970370},
pages = {395--402},
publisher = {Elsevier Ltd},
title = {{Method Evaluation — A Practical Guide}},
url = {http://dx.doi.org/10.1016/B978-0-08-097037-0.00089-0},
volume = {1},
year = {2013}
}
@techreport{DasGupta1996,
author = {DasGupta, Anirban},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/DasGupta\_1996.pdf:pdf},
institution = {Purdue University},
title = {{Review of Optimal Bayes Designs}},
year = {1996}
}
@phdthesis{Forkman2008,
abstract = {This thesis examines and develops statistical methods for design and analysis with applications in immunoassay and other analytical techniques. In immunoassay, concentrations of components in clinical samples are measured using antibodies. The responses obtained are related to the concentrations in the samples. The relationship between response and concentration is established by fitting a calibration curve to responses of samples with known concentrations, called calibrators or standards. The concentrations in the clinical samples are estimated, through the calibration curve, by inverse prediction. The optimal choice of calibrator concentrations is dependent on the true relationship between response and concentration. A locally optimal design is conditioned on a given true relationship. This thesis presents a novel method that accounts for the variation in the true relationships by considering unconditional variances and expected values. For immunoassay, it is suggested that the average coefficient of variation in inverse predictions be minimised. In immunoassay, the coefficient of variation is the most common measure of variability. Several clinical samples or calibrators may share the same coefficient of variation, although they have different expected values. It is shown here that this phenomenon can be a consequence of a random variation in the dispensed volumes, and that inverse regression is appropriate when the random variation is in concentration rather than in response. An estimator of a common coefficient of variation that is shared by several clinical samples is proposed, and inferential methods are developed for common coefficients of variation in normally distributed data. These methods are based on McKay’s chi-square approximation for the coefficient of variation. This study proves that McKay’s approximation is noncentral beta distributed, and that it is asymptotically normal with mean n - 1 and variance slightly smaller than 2(n - 1).},
author = {Forkman, Johannes},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Forkman\_2008.pdf:pdf},
isbn = {9789186195137},
keywords = {McKay’s approximation,calibration,calibration curve,coefficient of variation,elisa,four-parameter logistic function,immunoassay,inverse prediction,inverse regression},
mendeley-tags = {calibration curve,elisa},
pages = {80},
school = {Swedish University of Agricultural Sciences; Uppsala},
title = {{Optimal Calibration in Immunoassay and Inference on the Coefficient of Variation}},
type = {Doctoral},
year = {2008}
}
@article{Epner2013,
author = {Epner, Paul L and Gans, Janet E and Graber, Mark L},
doi = {10.1136/bmjqs-2012-001621},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Epner\_2013.pdf:pdf},
issn = {2044-5423},
journal = {BMJ quality \& safety},
keywords = {2014 - published by,bmj,com,com on january 30,group,nloaded from qualitysafety},
month = oct,
pages = {ii6--ii10},
pmid = {23955467},
title = {{When diagnostic testing leads to harm: a new outcomes-based approach for laboratory medicine.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3786651\&tool=pmcentrez\&rendertype=abstract},
volume = {22 Suppl 2},
year = {2013}
}
@article{Donner1987,
abstract = {This paper provides exact power contours to guide the planning of reliability studies, where the parameter of interest is the coefficient of intraclass correlation rho derived from a one-way analysis of variance model. The contours display the required numbers of subjects k and number of repeated measurements n that provide 80 per cent power for testing Ho: rho less than or equal to rho 0 versus H1: rho greater than rho 0 at the 5 per cent level of significance for selected values of rho o. We discuss the design considerations of these results.},
author = {Donner, A and Eliasziw, M},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Donner, Eliasziw - 1987 - Sample size requirements for reliability studies(2).pdf:pdf},
issn = {0277-6715},
journal = {Statistics in medicine},
keywords = {Analysis of Variance,Sampling Studies},
month = jun,
number = {4},
pages = {441--8},
pmid = {3629046},
title = {{Sample size requirements for reliability studies.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/3629046},
volume = {6},
year = {1987}
}
@article{Ismail2011,
abstract = {Background: Immunoassays are susceptible to analytical interferences including from endogenous immunoglobulin antibodies at a rate of ?0.4\% to 4\%. Hundreds of millions of immunoassay tests (.10 millions in the UK alone) are performed yearly worldwide for measurements of an array of large and small moieties such as proteins, hormones, tumour markers, rheumatoid factor, troponin, small peptides, steroids and drugs. Methods: Interference in these tests can lead to false results which when suspected, or surmised, can be analytically confirmed in most cases. Suspecting false laboratory data in the first place is not difficult when results are gross and without clinical correlates. However, when false results are subtle and/or plausible, it can be difficult to suspect with adverse clinical sequelae. This problem can be ameliorated by using a probabilistic Bayesian reasoning to flag up potentially suspect results even when laboratory data appear “not-unreasonable”. Results: Essentially, in disorders with low prevalence, the majority of positive results caused by analytical interference are likely to be false positives. On the other hand, when the disease prevalence is high, false negative results increase and become more significant. To illustrate the scope and utility of this approach, six different examples covering wide range of analytes are given, each highlighting specific aspect/nature of interference and suggested options to reduce it. Conclusion: Bayesian reasoning would allow laboratorians and/or clinicians to extract information about potentially false results, thus seeking follow-up confirmatory tests prior to the initiation of more expensive/invasive procedures or concluding a potentially wrong diagnosis.},
author = {Ismail, Adel A A and Ismail, Abbas A and Ismail, Yasmin},
doi = {10.1258/acb.2010.010197},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Ismail, Ismail\_2011.pdf:pdf},
journal = {Annals of Clinical Biochemistry},
pages = {65},
title = {{Probabilistic Bayesian reasoning can help identifying potentially wrong immunoassays results in clinical practice: even when they appear 'not-unreasonable'}},
volume = {48},
year = {2011}
}
@article{Steingart2007,
abstract = {BACKGROUND: The global tuberculosis epidemic results in nearly 2 million deaths and 9 million new cases of the disease a year. The vast majority of tuberculosis patients live in developing countries, where the diagnosis of tuberculosis relies on the identification of acid-fast bacilli on unprocessed sputum smears using conventional light microscopy. Microscopy has high specificity in tuberculosis-endemic countries, but modest sensitivity which varies among laboratories (range 20\% to 80\%). Moreover, the sensitivity is poor for paucibacillary disease (e.g., pediatric and HIV-associated tuberculosis). Thus, the development of rapid and accurate new diagnostic tools is imperative. Immune-based tests are potentially suitable for use in low-income countries as some test formats can be performed at the point of care without laboratory equipment. Currently, dozens of distinct commercial antibody detection tests are sold in developing countries. The question is "do they work?" METHODS AND FINDINGS: We conducted a systematic review to assess the accuracy of commercial antibody detection tests for the diagnosis of pulmonary tuberculosis. Studies from all countries using culture and/or microscopy smear for confirmation of pulmonary tuberculosis were eligible. Studies with fewer than 50 participants (25 patients and 25 control participants) were excluded. In a comprehensive search, we identified 68 studies. The results demonstrate that (1) overall, commercial tests vary widely in performance; (2) sensitivity is higher in smear-positive than smear-negative samples; (3) in studies of smear-positive patients, Anda-TB IgG by enzyme-linked immunosorbent assay shows limited sensitivity (range 63\% to 85\%) and inconsistent specificity (range 73\% to 100\%); (4) specificity is higher in healthy volunteers than in patients in whom tuberculosis disease is initially suspected and subsequently ruled out; and (5) there are insufficient data to determine the accuracy of most commercial tests in smear microscopy-negative patients, as well as their performance in children or persons with HIV infection. CONCLUSIONS: None of the commercial tests evaluated perform well enough to replace sputum smear microscopy. Thus, these tests have little or no role in the diagnosis of pulmonary tuberculosis. Lack of methodological rigor in these studies was identified as a concern. It will be important to review the basic science literature evaluating serological tests for the diagnosis of pulmonary tuberculosis to determine whether useful antigens have been described but their potential has not been fully exploited. Activities leading to the discovery of new antigens with immunodiagnostic potential need to be intensified.},
annote = {Possibly a good citation for need for increased rigour in validation of commercial tests.},
author = {Steingart, Karen R and Henry, Megan and Laal, Suman and Hopewell, Philip C and Ramsay, Andrew and Menzies, Dick and Cunningham, Jane and Weldingh, Karin and Pai, Madhukar},
doi = {10.1371/journal.pmed.0040202},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Steingart et al. - 2007 - Commercial serological antibody detection tests for the diagnosis of pulmonary tuberculosis a systematic revie.pdf:pdf},
issn = {1549-1676},
journal = {PLoS medicine},
keywords = {Adult,Agglutination Tests,Antibodies,Bacterial,Bacterial: blood,Blood Preservation,Blotting,Child,Comorbidity,Developing Countries,Diagnostic,Enzyme-Linked Immunosorbent Assay,Enzyme-Linked Immunosorbent Assay: methods,Enzyme-Linked Immunosorbent Assay: standards,HIV Infections,HIV Infections: epidemiology,Humans,Immunoglobulin G,Immunoglobulin G: blood,Kaolin,Mycobacterium tuberculosis,Mycobacterium tuberculosis: growth \& development,Mycobacterium tuberculosis: immunology,Mycobacterium tuberculosis: isolation \& purificati,Predictive Value of Tests,Pulmonary,Pulmonary: blood,Pulmonary: diagnosis,Pulmonary: epidemiology,Pulmonary: immunology,Reagent Kits,Reproducibility of Results,Research Design,Sensitivity and Specificity,Sputum,Sputum: microbiology,Tuberculosis,Western},
month = jun,
number = {6},
pages = {e202},
pmid = {17564490},
title = {{Commercial serological antibody detection tests for the diagnosis of pulmonary tuberculosis: a systematic review.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=1891320\&tool=pmcentrez\&rendertype=abstract},
volume = {4},
year = {2007}
}
@inproceedings{Devanarayan2004,
author = {Devanarayan, Viswanath},
booktitle = {BASS-XI},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Devanarayan BASS-XI Talk v041022.pdf:pdf},
title = {{Statistical Considerations in Biomarker Method Development \& Validation}},
year = {2004}
}
@article{Sasso2013,
abstract = {This work demonstrates the use of a continuous online monitoring system for tracking systemic inflammation biomarkers during cardiopulmonary bypass (CPB) procedures. The ability to monitor inflammation biomarkers during CPB will allow surgical teams to actively treat inflammation and reduce harmful effects on postoperative morbidity and mortality, enabling improved patient outcomes. A microfluidic device has been designed which allows automation of the individual processing steps of a microbead immunoassay to allow continuous tracking of antigen concentrations. Preliminary experiments have demonstrated that the results produced by the microimmunoassay are comparable to results produced from a standard enzyme-linked immunosorbent assay (r = 0.98). Additionally, integration of the assay with a simulated CPB circuit has been demonstrated with temporal tracking of C3a concentrations within blood continuously sampled from the circuit. The presented work describes the motivation, design challenges, and preliminary experimental results of this project.},
author = {Sasso, Lawrence a and Aran, Kiana and Guan, Yulong and \"{U}ndar, Akif and Zahn, Jeffrey D},
doi = {10.1111/aor.12021},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Sasso\_2013.pdf:pdf},
issn = {1525-1594},
journal = {Artificial organs},
keywords = {Benchmarking,Biological Markers,Biological Markers: blood,Cardiopulmonary Bypass,Cardiovascular,Enzyme-Linked Immunosorbent Assay,Humans,Immunoassay,Immunoassay: methods,Inflammation,Inflammation: blood,Microfluidic Analytical Techniques,Models,Pilot Projects},
month = jan,
number = {1},
pages = {E9--E17},
pmid = {23305589},
title = {{Continuous monitoring of inflammation biomarkers during simulated cardiopulmonary bypass using a microfluidic immunoassay device - a pilot study.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23305589},
volume = {37},
year = {2013}
}
@article{Hoffman2007,
abstract = {PURPOSE: Typical acceptance criteria for analytical methods are not chosen with regard to the concept of method suitability and are commonly based on ad-hoc rules. Such approaches yield unknown and uncontrolled risks of accepting unsuitable analytical methods and rejecting suitable analytical methods. This paper proposes a formal statistical framework for the validation of analytical methods, which incorporates the use of total error and controls the risks of incorrect decision-making.

MATERIALS AND METHODS: A total error approach for method validation based on the use of two-sided beta-content tolerance intervals is proposed. The performance of the proposed approach is compared to the performance of current ad-hoc approaches via simulation techniques.

RESULTS: The current ad-hoc approaches for method validation fail to control the risk of incorrectly accepting unsuitable analytical methods. The proposed total error approach controls the risk of incorrectly accepting unsuitable analytical methods and provides adequate power to accept truly suitable methods.

CONCLUSION: Current ad-hoc approaches to method validation are inconsistent with ensuring method suitability. A total error approach based on the use of two-sided beta-content tolerance intervals was developed. The total error approach offers a formal statistical framework for assessing analytical method performance. The approach is consistent with the concept of method suitability and controls the risk of incorrectly accepting unsuitable analytical methods.},
annote = {
        From Duplicate 1 ( 
        
          A total error approach for the validation of quantitative analytical methods.
        
         - Hoffman, David; Kringle, Robert )

        
        

        From Duplicate 1 ( 
        
        
          A total error approach for the validation of quantitative analytical methods.
        
        
         - Hoffman, David; Kringle, Robert )

        
        

        

        

        

        

        From Duplicate 2 ( 
        
          A total error approach for the validation of quantitative analytical methods.
        
         - Hoffman, David; Kringle, Robert )

        
        

        

        

      },
author = {Hoffman, David and Kringle, Robert},
doi = {10.1007/s11095-007-9242-3},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hoffman, Kringle - 2007 - A total error approach for the validation of quantitative analytical methods(2).pdf:pdf},
issn = {0724-8741},
journal = {Pharmaceutical research},
keywords = {Analytical,Analytical: standards,Chemistry Techniques,Models,Theoretical},
month = jun,
number = {6},
pages = {1157--1164},
pmid = {17373576},
title = {{A total error approach for the validation of quantitative analytical methods.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17373576},
volume = {24},
year = {2007}
}
@article{Sadler1992,
abstract = {A trend to market-driven health care in many parts of the world is focusing increasing attention on getting maximum value from available resources. Laboratories are not exempted. Well-informed clinical input has a potentially valuable role in any laboratory rationalization process. However, a communication difficulty exists in the sense that, although laboratory workers, commercial developers, regulatory bodies, etc., are thoroughly conditioned to using assay coefficient of variation as a general performance measure (for excellent reasons), this is not necessarily the most intuitive or informative scale from a clinician's perspective. Here we use routine clinical data from an immunoradiometric assay of thyrotropin to illustrate, first, a general approach to estimation and prediction of reproducibility, and second, an alternative summary that expresses the discriminatory power of an assay. This latter measure, our experience suggests, is more suited to the way clinicians perceive assays and assay results. The overall aim is improved clinician/laboratory communication.},
author = {Sadler, William A and Murray, L M and Turner, J G},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Sadler\_1992.pdf:pdf},
issn = {0009-9147},
journal = {Clinical chemistry},
keywords = {Biological Assay,Humans,Models,Radioimmunoassay,Reproducibility of Results,Statistical,Thyrotropin,Thyrotropin: analysis},
month = sep,
number = {9},
pages = {1773--8},
pmid = {1526013},
title = {{Minimum distinguishable difference in concentration: a clinically oriented translation of assay precision summaries.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/1526013},
volume = {38},
year = {1992}
}
@article{Steyerberg2010,
abstract = {The performance of prediction models can be assessed using a variety of different methods and metrics. Traditional measures for binary and survival outcomes include the Brier score to indicate overall model performance, the concordance (or c ) statistic for discriminative ability (or area under the receiver operating characteristic (ROC) curve), and goodness-of-fit statistics for calibration. Several new measures have recently been proposed that can be seen as refinements of discrimination measures, including variants of the c statistic for survival, reclassification tables, net reclassification improvement (NRI), and integrated discrimination improvement (IDI). Moreover, decision–analytic measures have been proposed, including decision curves to plot the net benefit achieved by making decisions based on model predictions. We aimed to define the role of these relatively novel approaches in the evaluation of the performance of prediction models. For illustration we present a case study of predicting the presence of residual tumor versus benign tissue in patients with testicular cancer (n=544 for model development, n=273 for external validation). We suggest that reporting discrimination and calibration will always be important for a prediction model. Decision-analytic measures should be reported if the predictive model is to be used for making clinical decisions. Other measures of performance may be warranted in specific applications, such as reclassification metrics to gain insight into the value of adding a novel predictor to an established model.},
author = {Steyerberg, Ewout W and Vickers, Andrew J and Cook, Nancy R and Gerds, Thomas and Obuchowski, Nancy and Pencina, Michael J and Kattan, Michael W},
doi = {10.1097/EDE.0b013e3181c30fb2.Assessing},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Steyerberg\_2010.pdf:pdf},
journal = {Epidemiology},
number = {1},
pages = {128--138},
title = {{Assessing the performance of prediction models: a framework for some traditional and novel measures}},
volume = {21},
year = {2010}
}
@article{Sadler1990,
abstract = {Published estimates of the imprecision characteristics of immunoassays are often based on quantities of data that seem to be inadequate. The increasing use of imprecision profiles has not necessarily improved the situation. We describe and illustrate a method of computing 95\% confidence intervals for profiles estimated directly from replicated assay results. The data used were chosen to mimic the sort of data that are typically available in practice, either within laboratories or from external quality-assessment programs. Use of confidence intervals is an effective pictorial way of incorporating information about the quantity and the distribution of the data used for estimation. This is in keeping with an important property of imprecision profile plots: potentially complex information is summarized in a readily comprehensible way. A computer program is available for estimating and plotting profiles and their 95\% confidence intervals.},
author = {Sadler, William A and Smith, Murray H},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sadler, Smith - 1990 - Use and abuse of imprecision profiles some pitfalls illustrated by computing and plotting confidence intervals.pdf:pdf},
issn = {0009-9147},
journal = {Clinical chemistry},
keywords = {Computer Simulation,Confidence Intervals,Radioimmunoassay,Radioimmunoassay: standards,Software,Statistics as Topic},
month = jul,
number = {7},
pages = {1346--1350},
pmid = {2372950},
title = {{Use and abuse of imprecision profiles: some pitfalls illustrated by computing and plotting confidence intervals.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/2372950},
volume = {36},
year = {1990}
}
@article{Hasker2010,
abstract = {Control of human African trypanosomiasis (HAT) in the Democratic Republic of Congo is based on mass population screening by mobile teams; a costly and labor-intensive approach. We hypothesized that blood samples collected on filter paper by village health workers and processed in a central laboratory might be a cost-effective alternative. We estimated sensitivity and specificity of micro-card agglutination test for trypanosomiasis (micro-CATT) and enzyme-linked immunosorbent assay (ELISA)/T.b. gambiense on filter paper samples compared with parasitology-based case classification and used the results in a Monte Carlo simulation of a lot quality assurance sampling (LQAS) approach. Micro-CATT and ELISA/T.b. gambiense showed acceptable sensitivity (92.7\% [95\% CI 87.4-98.0\%] and 82.2\% [95\% CI 75.3-90.4\%]) and very high specificity (99.4\% [95\% CI 99.0-99.9\%] and 99.8\% [95\% CI 99.5-100\%]), respectively. Conditional on high sample size per lot (> or = 60\%), both tests could reliably distinguish a 2\% from a zero prevalence at village level. Alternatively, these tests could be used to identify individual HAT suspects for subsequent confirmation.},
author = {Hasker, Epco and Lutumba, Pascal and Mumba, Dieudonn\'{e} and Lejon, Veerle and B\"{u}scher, Phillipe and Kande, Victor and Muyembe, Jean Jacques and Menten, Joris and Robays, Jo and Boelaert, Marleen},
doi = {10.4269/ajtmh.2010.09-0735},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hasker et al. - 2010 - Diagnostic accuracy and feasibility of serological tests on filter paper samples for outbreak detection of T.b. g.pdf:pdf},
issn = {1476-1645},
journal = {The American journal of tropical medicine and hygiene},
keywords = {African,African: diagnosis,African: epidemiology,Agglutination Tests,Agglutination Tests: standards,Antibodies,Blood Specimen Collection,Blood Specimen Collection: instrumentation,Blood Specimen Collection: methods,Democratic Republic of the Congo,Democratic Republic of the Congo: epidemiology,Disease Outbreaks,Enzyme-Linked Immunosorbent Assay,Enzyme-Linked Immunosorbent Assay: standards,Filtration,Filtration: instrumentation,Humans,Latex Fixation Tests,Latex Fixation Tests: standards,Monte Carlo Method,Paper,Protozoan,Protozoan: blood,ROC Curve,Sensitivity and Specificity,Specimen Handling,Trypanosoma brucei gambiense,Trypanosoma brucei gambiense: immunology,Trypanosomiasis,elisa},
mendeley-tags = {elisa},
month = aug,
number = {2},
pages = {374----379},
pmid = {20682885},
title = {{Diagnostic accuracy and feasibility of serological tests on filter paper samples for outbreak detection of T.b. gambiense human African trypanosomiasis.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2911188\&tool=pmcentrez\&rendertype=abstract},
volume = {83},
year = {2010}
}
@article{Sadler2008,
abstract = {Background: For nearly 20 years, we and others have used a three-parameter power function as a direct estimation error model for immunoassays. The main application is imprecision profile plots (after translating from variance to coefficient of variation) but other uses includeweighting functions for regression analysis and variance stabilizing transformations. Although generally successful, the intrinsic monotonicity of the function means that it fails to describe small but distinct increases in variance that occasionally occur near the assay detection limit. Methods: A systematic comparison of five variance functions was undertaken, using randomly drawn samples from a large body of real immunoassay data. Results: Variance function accuracy (hence imprecision profile accuracy) can be markedly improved, particularly near the assay detection limit, by employing a pair of complementary three-parameter power functions, together with a constrained four-parameter function, which provides for a variance turning point. Conclusions: A set of rules, based on an objective goodness-of-fit statistic, can be used to automate presentation of the most appropriate function for any particular data-set. Flexibility is easily incorporated into the selection rules and is actually highly desirable to encourage ongoing evaluation with a wider variety of data. AWin32 computer program that performs the variance function estimation and plotting is freely available.},
author = {Sadler, William A},
doi = {10.1258/acb.2008.007230},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Sadler\_2008.pdf:pdf},
journal = {Annals of Clinical Biochemistry},
number = {5},
pages = {481----485},
title = {{Error models for immunoassays}},
volume = {45},
year = {2008}
}
@article{Bland1995,
abstract = {Summary When comparing a new method of measurement with a standard method, one of the things we want to know is whether the difference between the measurements by the two methods is related to the magnitude of the measurement. A plot of the difference against the standard measurement is sometimes suggested, but this will always appear to show a relation between difference and magnitude when there is none. A plot of the difference against the average of the standard and new measurements is unlikely to mislead in this way. We show this theoretically and by a practical example.},
author = {Bland, J Martin and Altman, Douglas G},
doi = {10.1016/S0140-6736(95)91748-9},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bland, Altman - 1995 - Comparing methods of measurement why plotting difference against standard method is misleading.pdf:pdf},
issn = {01406736},
journal = {The Lancet},
month = oct,
number = {8982},
pages = {1085--1087},
title = {{Comparing methods of measurement: why plotting difference against standard method is misleading}},
url = {http://www.sciencedirect.com/science/article/pii/S0140673695917489},
volume = {346},
year = {1995}
}
@article{Becerra2012,
abstract = {Measuring instruments are usually calibrated at discrete values but, it is very useful for users to have formulae to describe the errors of indications (and their uncertainties) as a function of the readings in the whole range of the calibrated instrument. In this paper, different approaches were analyzed for weighing instruments calibration; however these methods may apply for different kind of measuring instruments, where formulae could be more convenient for their normal usage.},
author = {Becerra, Luis O and Pe\~{n}a, Luis M and Ram\'{\i}rez, Lautaro J},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Becerra\_2012.pdf:pdf},
journal = {Ingeniera},
keywords = {Curve fitting,calibration,errors,indication,uncertainty},
number = {1},
pages = {13----23},
title = {{FITTING CURVES TO DESCRIBE ERRORS OF INDICATIONS IN USE OF MEASURING INSTRUMENTS}},
volume = {22},
year = {2012}
}
@inproceedings{Roche2012,
abstract = {Roche Diagnostics},
author = {Roche},
booktitle = {PSI Biomarkers SIG},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Roche\_2012.pdf:pdf},
title = {{Developing Companion Invitro Diagnostic Tests}},
year = {2012}
}
@article{Lewandrowski2011,
abstract = {CONTEXT: Managing the quality of point-of-care testing (POCT) is a continuing challenge. Advances in testing technologies and the development of specialized informatics for POCT have greatly improved the ability of hospitals to manage their POCT program.

OBJECTIVES: To present the evolving role of technology improvement, informatics, and program management as the key developments to ensure the quality of POCT.

DATA SOURCES: This presentation is based on a review of the literature and on our experiences with POCT at the Massachusetts General Hospital (Boston).

CONCLUSIONS: Federal and state regulations, along with accreditation standards developed by the College of American Pathologists and The Joint Commission, have established guidelines for the performance of POCT and have provided a strong incentive to improve the quality of testing. Many instruments for POCT have incorporated advanced design features to prevent analytic and operator errors. This, along with the development of connectivity standards and specialized data management software, has enabled remote review of test data and electronic flow of information to hospital information systems. However, documentation of manually performed, visually read tests remains problematic and some POCT devices do not have adequate safeguards to prevent significant errors. In the past 2 decades the structure of a successful POCT management program has been defined, emphasizing the role of POCT managers working in conjunction with a pathology-based medical director. The critical skill set of POCT managers has also been identified. The POCT manager is now recognized as a true specialist in laboratory medicine.},
author = {Lewandrowski, Kent and Gregory, Kimberly and Macmillan, Donna},
doi = {10.5858/arpa.2011-0157-RA},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lewandrowski, Gregory, Macmillan - 2011 - Assuring quality in point-of-care testing evolution of technologies, informatics, and program.pdf:pdf},
issn = {1543-2165},
journal = {Archives of pathology \& laboratory medicine},
keywords = {Clinical Laboratory Information Systems,Clinical Laboratory Information Systems: organizat,Clinical Laboratory Information Systems: standards,Humans,Pathology, Clinical,Pathology, Clinical: organization \& administration,Pathology, Clinical: standards,Point-of-Care Systems,Point-of-Care Systems: organization \& administrati,Point-of-Care Systems: standards,Quality Assurance, Health Care,Quality Assurance, Health Care: standards,Quality Control},
month = nov,
number = {11},
pages = {1405--14},
pmid = {22032566},
title = {{Assuring quality in point-of-care testing: evolution of technologies, informatics, and program management.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22032566},
volume = {135},
year = {2011}
}
@article{Araujo2007,
abstract = {It is suggested that power analysis should be formally incorporated into quantification experiment reports in order to substantiate the conclusions derived from experimental data more effectively. The article addressed the issues of power analysis calculation, sample size estimation and appropriate data reporting in quantitative analytical comparisons. Illustrative examples from the literature are used to show how the described power analysis theory could be applied in practice.},
author = {Araujo, Pedro and Fr\o yland, Livar},
doi = {10.1016/j.jchromb.2006.10.002},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Araujo\_2007.pdf:pdf},
issn = {1570-0232},
journal = {Journal of chromatography. B, Analytical technologies in the biomedical and life sciences},
keywords = {Analytical,Analytical: methods,Analytical: statistics \& num,Chemistry Techniques,Chromatography,Chromatography: methods,Chromatography: statistics \& numerical data,Confidence Intervals,Models,Statistical,Statistics as Topic,Statistics as Topic: methods},
month = mar,
number = {2},
pages = {305--8},
pmid = {17070737},
title = {{Statistical power and analytical quantification.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17070737},
volume = {847},
year = {2007}
}
@article{Gelman2013,
author = {Gelman, Andrew},
doi = {10.1097/EDE.0b013e31827886f7},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Gelman\_2013.pdf:pdf},
issn = {1531-5487},
journal = {Epidemiology (Cambridge, Mass.)},
keywords = {Bayes Theorem,Data Interpretation, Statistical},
month = jan,
number = {1},
pages = {69--72},
pmid = {23232612},
title = {{P values and statistical practice.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23232612},
volume = {24},
year = {2013}
}
@article{Feinstein2002,
author = {Feinstein, A R},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Feinstein\_2002.pdf:pdf},
journal = {Journal of epidemiology and community health},
pages = {330--332},
title = {{Misguided efforts and future challenges for research on “diagnostic tests”}},
volume = {56},
year = {2002}
}
@article{Straume1998,
abstract = {We describe a data reduction procedure to assign statis- tically accurate estimates of unknown hormone concentrations, with associated uncertainties, based on experimental uncertainties in sample replicates and the fitted calibration curve. Three mathematical calibration curve functions are considered. The one providing optimal statistical characterization of reference calibrators is chosen for unknown evaluation. Experimental error is addressed by assigning and propagating uncertainty estimates for each measured response (including zero-dose responses) by an empirically determined discrete uncertainty profile and by propagating calibration curve uncertainty. Discrete uncertainty profiles account for both response precision (replicability) and accuracy (deviation from predicted calibration curves) without relying on assumed theoretical response variance–assay response relations. The validity of assigning variable response weighting by this procedure was assessed by Monte Carlo simulations based on chemiluminescence growth hormone calibration curves. Much-improved accuracy and estimated precision are achieved for unknown hormone concentrations, particularly extremely low concentrations, by using this variable response weighting procedure.},
author = {Straume, Martin and Johnson, Michael L and Veldhuis, Johannes D},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Straume\_1998.pdf:pdf},
journal = {Clinical chemistry},
number = {1},
pages = {116--123},
title = {{Statistically accurate estimation of hormone concentrations and associated uncertainties: methodology, validation, and applications}},
volume = {44},
year = {1998}
}
@incollection{Spencer2013,
annote = {
        From Duplicate 1 ( 
        
          Pregnancy
        
         - Spencer, Kevin; Chard, Tim )
And  Duplicate 2 ( 
        
          Pregnancy
        
         - Spencer, Kevin; Chard, Tim )

        
        

        

        

        From Duplicate 3 ( 
        
          Pregnancy
        
         - Spencer, Kevin; Chard, Tim )

        
        

        From Duplicate 1 ( 
        
        
          Pregnancy
        
        
         - Spencer, Kevin; Chard, Tim )
And  Duplicate 2 ( 
        
        
          Pregnancy
        
        
         - Spencer, Kevin; Chard, Tim )
And  Duplicate 3 ( 
        
        
          Pregnancy
        
        
         - Spencer, Kevin; Chard, Tim )
And  Duplicate 5 ( 
        
        
          Pregnancy
        
        
         - Spencer, Kevin; Chard, Tim )
And  Duplicate 6 ( 
        
        
          Pregnancy
        
        
         - Spencer, Kevin; Chard, Tim )
And  Duplicate 7 ( 
        
        
          Pregnancy
        
        
         - Spencer, Kevin; Chard, Tim )
And  Duplicate 8 ( 
        
        
          Pregnancy
        
        
         - Spencer, Kevin; Chard, Tim )

        
        

        

        

        

        

      },
author = {Spencer, Kevin and Chard, Tim},
booktitle = {The Immunoassay Handbook},
chapter = {9.8},
doi = {10.1016/B978-0-08-097037-0.00051-8},
edition = {Fourth Ed},
editor = {Wild, David},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Spencer, Chard - 2013 - Pregnancy.pdf:pdf},
isbn = {9780080970370},
pages = {757--776},
publisher = {Elsevier Ltd},
title = {{Pregnancy}},
volume = {1},
year = {2013}
}
@incollection{Davies2013b,
author = {Davies, Chris},
booktitle = {The Immunoassay Handbook},
chapter = {2.1},
doi = {10.1016/B978-0-08-097037-0.00001-4},
edition = {Fourth Ed.},
editor = {Wild, David},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Wild2013\_Chap2\_1.pdf:pdf},
isbn = {9780080970370},
pages = {29--59},
publisher = {Elsevier Ltd},
title = {{Principles of Competitive and Immunometric Assays (Including ELISA)}},
url = {http://dx.doi.org/10.1016/B978-0-08-097037-0.00001-4},
year = {2013}
}
@article{Gelman2004,
abstract = {In a serial dilution assay, the concentration of a compound is estimated by combining measurements of several different dilutions of an unknown sample. The relation between concentration and measurement is nonlinear and heteroscedastic, and so it is not appropriate to weight these measurements equally. In the standard existing approach for analysis of these data, a large proportion of the measurements are discarded as being above or below detection limits. We present a Bayesian method for jointly estimating the calibration curve and the unknown concentrations using all the data. Compared to the existing method, our estimates have much lower standard errors and give estimates even when all the measurements are outside the "detection limits." We evaluate our method empirically using laboratory data on cockroach allergens measured in house dust samples. Our estimates are much more accurate than those obtained using the usual approach. In addition, we develop a method for determining the "effective weight" attached to each measurement, based on a local linearization of the estimated model. The effective weight can give insight into the information conveyed by each data point and suggests potential improvements in design of serial dilution experiments.},
author = {Gelman, Andrew and Chew, Ginger L and Shnaidman, Michael},
doi = {10.1111/j.0006-341X.2004.00185.x},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman, Chew, Shnaidman - 2004 - Bayesian analysis of serial dilution assays.pdf:pdf},
issn = {0006-341X},
journal = {Biometrics},
keywords = {Allergens,Allergens: analysis,Animals,Bayes Theorem,Biometry,Cockroaches,Cockroaches: immunology,Enzyme-Linked Immunosorbent Assay,Enzyme-Linked Immunosorbent Assay: statistics \& nu,Models,Statistical,bayesian,elisa},
mendeley-tags = {bayesian,elisa},
month = jun,
number = {2},
pages = {407--17},
pmid = {15180666},
title = {{Bayesian analysis of serial dilution assays.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15180666},
volume = {60},
year = {2004}
}
@article{Anderson-Sprecher1994,
author = {Anderson-Sprecher, Richard},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Anderson-Sprecher\_1994.pdf:pdf},
journal = {The American Statistician},
keywords = {113-117,1994,2,48,american statistical association,author,el comparisons and r2,may,no,pp,published by,richard anderson-sprecher,s,source,the american statistician,vol},
number = {2 (May)},
pages = {113--7},
title = {{Model Comparisons and R2}},
volume = {48},
year = {1994}
}
@article{Giraudeau2001,
abstract = {Reproducibility of a quantitative outcome is usually assessed by means of the intraclass correlation coe?cient (ICC). When we are interested in assessing reproducibility from only one sample, we suggest that the study be planned with regards to the expected width of the 95 per cent con?dence interval of the ICC. An approximation of this latter width is derived, which allows appraisal of the in?uence of n the number of subjects and p the number of replicates. Through simulation studies, the approximation is shown to be of good accuracy and can therefore be used reliably. Optimal designs are also discussed such as the optimal distribution between the number of subjects and the number of replicates per subject for a ?xed total number of measures.},
author = {Giraudeau, B and Mary, J Y},
doi = {10.1002/sim.935},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Giraudeau, Mary - 2001 - Planning a reproducibility study how many subjects and how many replicates per subject for an expected width o.pdf:pdf},
journal = {Statistics in medicine},
number = {January},
pages = {3205--3214},
title = {{Planning a reproducibility study: how many subjects and how many replicates per subject for an expected width of the 95 per cent confidence interval of the intraclass correlation coefficient}},
volume = {20},
year = {2001}
}
@article{Chaloner1989,
abstract = {Abstract: A traditional way to design a binary response experiment is to design the experiment to be most efficient for a best guess of the parameter values. A design which is optimal for a best guess however may not be efficient for parameter values close to that best guess. We propose designs which formally account for the prior uncertainty in the parameter values. A design for a situation where the best guess has substantial uncertainty attached to it is very different from a design for a situation where approximate values of the parameters are known. We derive a general theory for concave design critria for non-linear models and then apply the theory to logistic regression. Designs found by numerical optimization are examined for a range of prior distributions and a range of criteria. The theoretical results are used to verify that the designs are indeed optimal.},
author = {Chaloner, Kathryn and Larntz, Kinley},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Chaloner\_1989.pdf:pdf},
journal = {Journal of Statistical Planning and Inference},
keywords = {A-optimality,Algorithm,Bayesian design,D-optimality,Nelder-Mead algorithm,binary data,equivalence theorem,logistic regression,non-linear models,optimal design.,optimization,reliability experiments,simplex method,stress testing},
pages = {191--208},
title = {{OPTIMAL LOGISTIC BAYESIAN DESIGN APPLIED to Logistic Regression Experiments}},
volume = {21},
year = {1989}
}
@article{May2005,
abstract = {Non-linear dose response relationships pose statistical challenges for their discovery. Even when an initial linear approximation is followed by other approaches, the results may be misleading and, possibly, preclude altogether the discovery of the nonlinear relationship under investigation. We review a variety of straightforward statistical approaches for detecting nonlinear relationships and discuss several factors that hinder their detection. Our specific context is that of epidemiologic studies of exposure-outcome associations and we focus on threshold and J-effect dose response relationships. The examples presented reveal that no single approach is universally appropriate; rather, these (and possibly other) nonlinearities require for their discovery a variety of both graphical and numeric techniques.},
author = {May, Susanne and Bigelow, Carol},
doi = {10.2203/dose-response.003.04.004},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/May\_2005.pdf:pdf},
issn = {1559-3258},
journal = {Dose-response : a publication of International Hormesis Society},
keywords = {2005,2006 university of massachusetts,3,474,490,and medicine,copyright,e-response,formerly nonlinearity in biology,toxicology},
month = jan,
number = {4},
pages = {474--90},
pmid = {18648629},
title = {{Modeling nonlinear dose-response relationships in epidemiologic studies: statistical approaches and practical challenges.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2477199\&tool=pmcentrez\&rendertype=abstract},
volume = {3},
year = {2005}
}
@incollection{Davies2013a,
author = {Davies, Chris},
booktitle = {The Immunoassay Handbook},
chapter = {9.1},
doi = {10.1016/B978-0-08-097037-0.00083-X},
edition = {Fourth Ed},
editor = {Wild, David},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Davies\_2013b.pdf:pdf},
isbn = {9780080970370},
pages = {661--672},
publisher = {Elsevier Ltd},
title = {{Clinical Concepts}},
year = {2013}
}
@article{Bellera2008,
abstract = {It is common to define a change in health status or in a disease state on the basis of a sustained rise (or decline) in a biomarker over time. However, such observations are often subject to important variability unrelated to the underlying biologic process. The authors propose a method to evaluate rules that define an event on the basis of consecutive increases (or decreases) in the observations, given the presence of random variation. They examine how well these rules correctly identify a truly rising biomarker trajectory and, conversely, how often they can recognize a truly stable series or a slowly rising series. The method relies on simulation of realistic, sophisticated data sets that accurately reflect the systematic and random variations observed in marker series. These flexible, empirically based simulations enable estimation of the sensitivity and specificity of rules of consecutive rises as a function of the underlying trend, amount of random variation, and schedule of measurements (frequency and duration of follow-up). The authors illustrate the approach with postradiotherapy series of prostate-specific antigen, where three consecutive rises in prostate-specific antigen indicate treatment failure; the data are described by using a Bayesian hierarchical changepoint model. The method is particularly flexible and could be applied to evaluate other rules that purport to accurately detect upturns (downturns) in other noisy data series, including other medical data or other application areas.},
author = {Bellera, Carine A and Hanley, James A and Joseph, Lawrence and Albertsen, Peter C},
doi = {10.1093/aje/kwn003},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bellera et al. - 2008 - Detecting trends in noisy data series application to biomarker series.pdf:pdf},
issn = {1476-6256},
journal = {American journal of epidemiology},
keywords = {Aged,Bayes Theorem,Biological,Biological: blood,Connecticut,Decision Support Techniques,Humans,Linear Models,Male,Markov Chains,Prostate-Specific Antigen,Prostate-Specific Antigen: blood,Prostatic Neoplasms,Prostatic Neoplasms: blood,Prostatic Neoplasms: radiotherapy,Registries,Retrospective Studies,Sensitivity and Specificity,Tumor Markers,bayesian,biomarker,mcgill},
mendeley-tags = {bayesian,biomarker,mcgill},
month = may,
number = {9},
pages = {1130--9},
pmid = {18303004},
title = {{Detecting trends in noisy data series: application to biomarker series.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18303004},
volume = {167},
year = {2008}
}
@article{Sivaganesan2008,
abstract = {BACKGROUND: In real-time quantitative PCR studies using absolute plasmid DNA standards, a calibration curve is developed to estimate an unknown DNA concentration. However, potential differences in the amplification performance of plasmid DNA compared to genomic DNA standards are often ignored in calibration calculations and in some cases impossible to characterize. A flexible statistical method that can account for uncertainty between plasmid and genomic DNA targets, replicate testing, and experiment-to-experiment variability is needed to estimate calibration curve parameters such as intercept and slope. Here we report the use of a Bayesian approach to generate calibration curves for the enumeration of target DNA from genomic DNA samples using absolute plasmid DNA standards.

RESULTS: Instead of the two traditional methods (classical and inverse), a Monte Carlo Markov Chain (MCMC) estimation was used to generate single, master, and modified calibration curves. The mean and the percentiles of the posterior distribution were used as point and interval estimates of unknown parameters such as intercepts, slopes and DNA concentrations. The software WinBUGS was used to perform all simulations and to generate the posterior distributions of all the unknown parameters of interest.

CONCLUSION: The Bayesian approach defined in this study allowed for the estimation of DNA concentrations from environmental samples using absolute standard curves generated by real-time qPCR. The approach accounted for uncertainty from multiple sources such as experiment-to-experiment variation, variability between replicate measurements, as well as uncertainty introduced when employing calibration curves generated from absolute plasmid DNA standards.},
author = {Sivaganesan, Mano and Seifring, Shawn and Varma, Manju and Haugland, Richard a and Shanks, Orin C},
doi = {10.1186/1471-2105-9-120},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Sivaganesan\_2008.pdf:pdf},
issn = {1471-2105},
journal = {BMC bioinformatics},
keywords = {Algorithms,Automated,Automated: methods,Base Sequence,Bayes Theorem,Calibration,DNA,DNA: methods,DNA: standards,Molecular Sequence Data,Pattern Recognition,Plasmids,Plasmids: genetics,Plasmids: standards,Reverse Transcriptase Polymerase Chain Reaction,Reverse Transcriptase Polymerase Chain Reaction: i,Reverse Transcriptase Polymerase Chain Reaction: s,Sequence Analysis,United States},
month = jan,
pages = {120},
pmid = {18298858},
title = {{A Bayesian method for calculating real-time quantitative PCR calibration curves using absolute plasmid DNA standards.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2292693\&tool=pmcentrez\&rendertype=abstract},
volume = {9},
year = {2008}
}
@article{Fong2010,
abstract = {Generalized linear mixed models (GLMMs) continue to grow in popularity due to their ability to directly acknowledge multiple levels of dependency and model different data types. For small sample sizes especially, likelihood-based inference can be unreliable with variance components being particularly difficult to estimate. A Bayesian approach is appealing but has been hampered by the lack of a fast implementation, and the difficulty in specifying prior distributions with variance components again being particularly problematic. Here, we briefly review previous approaches to computation in Bayesian implementations of GLMMs and illustrate in detail, the use of integrated nested Laplace approximations in this context. We consider a number of examples, carefully specifying prior distributions on meaningful quantities in each case. The examples cover a wide range of data types including those requiring smoothing over time and a relatively complicated spline model for which we examine our prior specification in terms of the implied degrees of freedom. We conclude that Bayesian inference is now practically feasible for GLMMs and provides an attractive alternative to likelihood-based approaches such as penalized quasi-likelihood. As with likelihood-based approaches, great care is required in the analysis of clustered binary data since approximation strategies may be less accurate for such data.},
author = {Fong, Youyi and Rue, H\aa vard and Wakefield, Jon},
doi = {10.1093/biostatistics/kxp053},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Fong\_2010.pdf:pdf},
issn = {1468-4357},
journal = {Biostatistics (Oxford, England)},
keywords = {Bayes Theorem,Breast Neoplasms,Breast Neoplasms: epidemiology,Cohort Studies,Computer Simulation,Epilepsy,Epilepsy: drug therapy,Female,Humans,Linear Models,Longitudinal Studies,Seizures,Seizures: drug therapy,Stochastic Processes,bayesian},
mendeley-tags = {bayesian},
month = jul,
number = {3},
pages = {397--412},
pmid = {19966070},
title = {{Bayesian inference for generalized linear mixed models.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2883299\&tool=pmcentrez\&rendertype=abstract},
volume = {11},
year = {2010}
}
@article{Sim2005,
abstract = {PURPOSE: This article examines and illustrates the use and interpretation of the kappa statistic in musculoskeletal research.

SUMMARY OF KEY POINTS: The reliability of clinicians' ratings is an important consideration in areas such as diagnosis and the interpretation of examination findings. Often, these ratings lie on a nominal or an ordinal scale. For such data, the kappa coefficient is an appropriate measure of reliability. Kappa is defined, in both weighted and unweighted forms, and its use is illustrated with examples from musculoskeletal research. Factors that can influence the magnitude of kappa (prevalence, bias, and non-independent ratings) are discussed, and ways of evaluating the magnitude of an obtained kappa are considered. The issue of statistical testing of kappa is considered, including the use of confidence intervals, and appropriate sample sizes for reliability studies using kappa are tabulated.

CONCLUSIONS: The article concludes with recommendations for the use and interpretation of kappa.},
author = {Sim, Julius and Wright, Chris C},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Sim, Wright\_2005.pdf:pdf},
issn = {0031-9023},
journal = {Physical therapy},
keywords = {Data Interpretation,Humans,Models,Musculoskeletal Diseases,Musculoskeletal Diseases: diagnosis,Musculoskeletal Diseases: epidemiology,Musculoskeletal Diseases: physiopathology,Observer Variation,Physical Therapy Specialty,Physical Therapy Specialty: standards,Prevalence,Reproducibility of Results,Research Design,Sample Size,Statistical},
month = mar,
number = {3},
pages = {257--68},
pmid = {15733050},
title = {{The kappa statistic in reliability studies: use, interpretation, and sample size requirements.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15733050},
volume = {85},
year = {2005}
}
@article{Walter1998,
abstract = {A method is developed to calculate the required number of subjects k in a reliability study, where reliability is measured using the intraclass correlation ?. The method is based on a functional approximation to earlier exact results. The approximation is shown to have excellent agreement with the exact results and one can use it easily without intensive numerical computation. Optimal design configurations are also discussed; for reliability values of about 40 per cent or higher, use of two or three observations per subject will minimize the total number of observations required.},
author = {Walter, S D and Eliasziw, M and Donner, Allan},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Walter, Eliasziw, Donner - 1998 - SAMPLE SIZE AND OPTIMAL DESIGNS FOR RELIABILITY STUDIES.pdf:pdf},
journal = {Statistics in Medicine},
number = {April},
pages = {101--110},
title = {{SAMPLE SIZE AND OPTIMAL DESIGNS FOR RELIABILITY STUDIES}},
volume = {17},
year = {1998}
}
@incollection{Wild2013,
annote = {
        From Duplicate 1 ( 
        
          Immunoassay for Beginners
        
         - Wild, David )

        
        

        From Duplicate 1 ( 
        
        
          Immunoassay for Beginners
        
        
         - Wild, David )

        
        

        First reading: 17Jul2013
          
Context: background for ELISA project with Nandini

        
Immunoassays are very specific for the molecule with which they pair; therefore, as long as that molecule is a specific marker for the health condition of interest, immunoassays are a [reliable]test, even in a complex mixture such as blood. The molecules may be antibodies, antigens, enzymes, etc.

        
Since [antigens] occur in minute concentrations, they are not observable with the naked eye, usually not even with a microscope; therefore, additional procedures, which include introducing a 'tracer',  are needed to produce a detectable 'signal' from the captured antigen--the 'signal generation system'.  The level of the signal will be proportional to the concentration of the antigen in the original sample. ELISA (enzyme-linked immunosorbant assay) is a signal generation system that uses an enzyme that, when bound, produces colour, fluorescence or luminesccence.

        
'Immunometric', or 'sandwich', immunoassays are straightforward as described.  Even in these, however, some error may occur depending on how well the excess tracer rinses away.

        
Since there are a finite number of binding spots on the solid phase, if the concentration [of the molecule of interest] in the sample is higher than anticipated, the assay will be saturated and no longer sensitive to higher concentrations (beyond the limit of the test). 

        
'Competitive' immunoassays use similar mechanisms, but are designed for the detection of smaller molecules.  The solid state-molecule-tracer complex is a little different and the measured signal will be *inversely* proportional to the concentration in the sample. 'Homogeneous' immunoassays, which don't require a separation step, will also produce a signal that is inversely proportional to concentration.

        
Calibration of the signal generation system is performed by estimating a 'standard curve'.  A function is fit to data: pairs of observations of solutions of known concentration and their measured signal.  The standard concentrations may be bought as part of a kit, 'calibrators', or prepared in the lab.  'Quantitative' assays use this calibration, or standard, curve method.  'Qualitative' assays have been simplified to indicate categoral signals such as positive/negative.

        

        

        From Duplicate 2 ( 
        
        
          Immunoassay for Beginners
        
        
         - Wild, David )

        
        
 First reading: 17Jul2013 
Context: background for ELISA project with Nandini

        
Immunoassays are very specific for the molecule with which they pair; therefore, as long as that molecule is a specific marker for the health condition of interest, immunoassays are a [reliable]test, even in a complex mixture such as blood. The molecules may be antibodies, antigens, enzymes, etc.

        
Since [antigens] occur in minute concentrations, they are not observable with the naked eye, usually not even with a microscope; therefore, additional procedures, which include introducing a 'tracer', are needed to produce a detectable 'signal' from the captured antigen--the 'signal generation system'. The level of the signal will be proportional to the concentration of the antigen in the original sample. ELISA (enzyme-linked immunosorbant assay) is a signal generation system that uses an enzyme that, when bound, produces colour, fluorescence or luminesccence.

        
'Immunometric', or 'sandwich', immunoassays are straightforward as described. Even in these, however, some error may occur depending on how well the excess tracer rinses away.

        
Since there are a finite number of binding spots on the solid phase, if the concentration [of the molecule of interest] in the sample is higher than anticipated, the assay will be saturated and no longer sensitive to higher concentrations (beyond the limit of the test). 

        
'Competitive' immunoassays use similar mechanisms, but are designed for the detection of smaller molecules. The solid state-molecule-tracer complex is a little different and the measured signal will be *inversely* proportional to the concentration in the sample. 'Homogeneous' immunoassays, which don't require a separation step, will also produce a signal that is inversely proportional to concentration.

        
Calibration of the signal generation system is performed by estimating a 'standard curve'. A function is fit to data: pairs of observations of solutions of known concentration and their measured signal. The standard concentrations may be bought as part of a kit, 'calibrators', or prepared in the lab. 'Quantitative' assays use this calibration, or standard, curve method. 'Qualitative' assays have been simplified to indicate categoral signals such as positive/negative.

        

        

        

        

        From Duplicate 2 ( 
        
          Immunoassay for Beginners
        
         - Wild, David )

        
        
 First reading: 17Jul2013 
Context: background for ELISA project with Nandini

        
Immunoassays are very specific for the molecule with which they pair; therefore, as long as that molecule is a specific marker for the health condition of interest, immunoassays are a [reliable]test, even in a complex mixture such as blood. The molecules may be antibodies, antigens, enzymes, etc.

        
Since [antigens] occur in minute concentrations, they are not observable with the naked eye, usually not even with a microscope; therefore, additional procedures, which include introducing a 'tracer', are needed to produce a detectable 'signal' from the captured antigen--the 'signal generation system'. The level of the signal will be proportional to the concentration of the antigen in the original sample. ELISA (enzyme-linked immunosorbant assay) is a signal generation system that uses an enzyme that, when bound, produces colour, fluorescence or luminesccence.

        
'Immunometric', or 'sandwich', immunoassays are straightforward as described. Even in these, however, some error may occur depending on how well the excess tracer rinses away.

        
Since there are a finite number of binding spots on the solid phase, if the concentration [of the molecule of interest] in the sample is higher than anticipated, the assay will be saturated and no longer sensitive to higher concentrations (beyond the limit of the test). 

        
'Competitive' immunoassays use similar mechanisms, but are designed for the detection of smaller molecules. The solid state-molecule-tracer complex is a little different and the measured signal will be *inversely* proportional to the concentration in the sample. 'Homogeneous' immunoassays, which don't require a separation step, will also produce a signal that is inversely proportional to concentration.

        
Calibration of the signal generation system is performed by estimating a 'standard curve'. A function is fit to data: pairs of observations of solutions of known concentration and their measured signal. The standard concentrations may be bought as part of a kit, 'calibrators', or prepared in the lab. 'Quantitative' assays use this calibration, or standard, curve method. 'Qualitative' assays have been simplified to indicate categoral signals such as positive/negative.

        

        

      },
author = {Wild, David},
booktitle = {The Immunoassay Handbook},
chapter = {1.2},
doi = {10.1016/B978-0-08-097037-0.00002-6},
edition = {Fourth Ed},
editor = {Wild, David G},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wild - 2013 - Immunoassay for Beginners.pdf:pdf},
isbn = {9780080970370},
pages = {7--10},
publisher = {Elsevier Ltd},
title = {{Immunoassay for Beginners}},
year = {2013}
}
@article{Carroll2003,
abstract = {In classical problems, e.g., comparing two populations, fitting a regression surface, etc., variability is a nuisance parameter. The term "nuisance parameter" is meant here in both the technical and the practical sense. However, there are many instances where understanding the structure of variability is just as central as understanding the mean structure. The purpose of this article is to review a few of these problems. I focus in particular on two issues: (a) the determination of the validity of an assay; and (b) the issue of the power for detecting health effects from nutrient intakes when the latter are measured by food frequency questionnaires. I will also briefly mention the problems of variance structure in generalized linear mixed models, robust parameter design in quality technology, and the signal in microarrays. In these and other problems, treating variance structure as a nuisance instead of a central part of the modeling effort not only leads to inefficient estimation of means, but also to misleading conclusions.},
author = {Carroll, Raymond J},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Carroll\_2003.pdf:pdf},
issn = {0006-341X},
journal = {Biometrics},
keywords = {Analysis of Variance,Evaluation Studies as Topic,Humans,Immunoassay,Immunoassay: standards,Models,Nutrition Assessment,Nutrition Surveys,Oligonucleotide Array Sequence Analysis,Oligonucleotide Array Sequence Analysis: methods,Oligonucleotide Array Sequence Analysis: standards,Statistical},
month = jun,
number = {2},
pages = {211--20},
pmid = {12926705},
title = {{Variances are not always nuisance parameters}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/12926705},
volume = {59},
year = {2003}
}
@misc{Plikaytis2005,
author = {Plikaytis, Brian D and Carlone, George M},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Plikaytis, Carlone - 2005 - ELISA for WINDOWS User's Manual.pdf:pdf},
keywords = {computing,elisa},
mendeley-tags = {computing,elisa},
pages = {57},
publisher = {Centers for Disease Control and Prevention},
title = {{ELISA for WINDOWS: User's Manual}},
url = {http://www.cdc.gov/ncidod/dbmd/bimb/elisa.htm},
year = {2005}
}
@article{Gelman2008,
author = {Gelman, Andrew and Jakulin, Aleks and Pittau, Maria Grazia and Su, Yu-Sung},
doi = {10.1214/08-AOAS191},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gelman et al. - 2008 - A weakly informative default prior distribution for logistic and other regression models.pdf:pdf},
issn = {1932-6157},
journal = {The Annals of Applied Statistics},
month = dec,
number = {4},
pages = {1360--1383},
title = {{A weakly informative default prior distribution for logistic and other regression models}},
url = {http://projecteuclid.org/euclid.aoas/1231424214},
volume = {2},
year = {2008}
}
@incollection{Burtis2013,
author = {Burtis, Carl A and Ashwood, Edward R and Bruns, David E},
booktitle = {Tietz textbook of clinical chemistry and molecular diagnostics},
chapter = {20},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Tietz\_Chap20\_POCT.pdf:pdf},
title = {{Point-of-Care Testing}},
year = {2013}
}
@article{Gerlach1993,
author = {Gerlach, Robert W and White, Richard W and Deming, Stanley N and Palasota, John A and {Van Emon}, Jeanette M},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gerlach et al. - 1993 - An Evaluation of Five Commercial Immunoassay Data Analysis Software Systems.pdf:pdf},
journal = {Analytical Biochemistry},
pages = {185----193},
title = {{An Evaluation of Five Commercial Immunoassay Data Analysis Software Systems}},
volume = {212},
year = {1993}
}
@article{Kelley2007,
abstract = {The accuracy in parameter estimation approach to sample size planning is developed for the coefficient of variation, where the goal of the method is to obtain an accurate parameter estimate by achieving a sufficiently narrow confidence interval. The first method allows researchers to plan sample size so that the expected width of the confidence interval for the population coefficient of variation is sufficiently narrow. A modification allows a desired degree of assurance to be incorporated into the method, so that the obtained confidence interval will be sufficiently narrow with some specified probability (e.g., 85\% assurance that the 95\% confidence interval width will be no wider than w units). Tables of necessary sample size are provided for a variety of scenarios that may help researchers planning a study where the coefficient of variation is of interest plan an appropriate sample size in order to have a sufficiently narrow confidence interval, optionally with some specified assurance of the confidence interval being sufficiently narrow. Freely available computer routines have been developed that allow researchers to easily implement all of the methods discussed in the article.},
author = {Kelley, Ken},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Kelley\_2007.pdf:pdf},
journal = {Behavioural Research Methods},
keywords = {R,coefficient of variation,sample size},
mendeley-tags = {R,coefficient of variation,sample size},
number = {4},
pages = {755----766},
title = {{Sample size planning for the coefficient of variation from the accuracy in parameter estimation approach}},
volume = {39},
year = {2007}
}
@article{Bland1998,
author = {Bland, J Martin and Altman, Douglas G},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Bland\_1998.pdf:pdf},
journal = {British Medical Journal},
number = {October},
pages = {1998},
title = {{Statistics notes: Bayesians and frequentists}},
volume = {317},
year = {1998}
}
@techreport{Tholen2004a,
abstract = {NCCLS document EP5-A2, Evaluation of Precision Performance of Quantitative Measurement Methods; Approved Guideline— Second Edition provides guidance and procedures for evaluating the precision of in vitro diagnostic devices and includes recommendations for manufacturers in evaluating their devices and methods when establishing performance claims. Included are guidelines for the duration, procedures, materials, data summaries, and interpretation techniques that are adaptable for the widest possible range of analytes and device complexity. The procedures are designed for manufacturers or developers of clinical laboratory measurement methods, and for users of those methods who wish to determine their own performance capabilities or to verify claims from a manufacturer. A balance is created in the document between complexity of design and formulae, and simplicity of operation.},
author = {Tholen, Daniel W and Kallner, Anders and Kennedy, John W and Krouwer, Jan S and Meier, Kristen},
isbn = {1562385429},
number = {25},
title = {{Evaluation of Precision Performance of Quantitative Measurement Methods; Approved Guideline}},
volume = {24},
year = {2004}
}
@article{Vanbelle2012,
abstract = {Kappa-like agreement indexes are often used to assess the agreement among examiners on a categorical scale. They have the particularity of correcting the level of agreement for the effect of chance. In the present paper, we first define two agreement indexes belonging to this family in a hierarchical context. In particular, we consider the cases of a random and fixed set of examiners. Then, we develop amethod to evaluate the influence of factors on these indexes. Agreement indexes are directly related to a set of covariates through a hierarchical model. We obtain the posterior distribution of the model parameters in a Bayesian framework.We apply the proposed approach on dental data and compare it with the generalized estimating equations approach.},
author = {Vanbelle, Sophie and Mutsvari, Timothy and Lesaffre, Emmanuel},
doi = {10.1002/sim.5424},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Vanbelle\_2012.pdf:pdf},
journal = {Statistics in medicine},
keywords = {agreement studies,cohen's kappa,intraclass,markov chain monte carlo,multilevel,nested,rater,reliability},
mendeley-tags = {agreement studies},
number = {March},
pages = {3667----3680},
title = {{Hierarchical modeling of agreement}},
volume = {31},
year = {2012}
}
@article{Choi2013,
abstract = {We introduce an automated digital microfluidic (DMF) platform capable of performing immunoassays from sample to analysis with minimal manual intervention. This platform features (a) a 90 Pogo pin interface for digital microfluidic control, (b) an integrated (and motorized) photomultiplier tube for chemiluminescent detection, and (c) a magnetic lens assembly which focuses magnetic fields into a narrow region on the surface of the DMF device, facilitating up to eight simultaneous digital microfluidic magnetic separations. The new platform was used to implement a three-level full factorial design of experiments (DOE) optimization for thyroid-stimulating hormone immunoassays, varying (1) the analyte concentration, (2) the sample incubation time, and (3) the sample volume, resulting in an optimized protocol that reduced the detection limit and sample incubation time by up to 5-fold and 2-fold, respectively, relative to those from previous work. To our knowledge, this is the first report of a DOE optimization for immunoassays in a microfluidic system of any format. We propose that this new platform paves the way for a benchtop tool that is useful for implementing immunoassays in near-patient settings, including community hospitals, physicians’ offices, and small clinical laboratories.},
author = {Choi, Kihwan and Ng, Alphonsus H C and Fobel, Ryan and Chang-yen, David A and Yarnell, Lyle E and Pearson, Elroy L and Oleksak, Carl M and Fischer, Andrew T and Luoma, Robert P and Robinson, John M and Audet, Julie and Wheeler, Aaron R},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Choi et al. - 2013 - Automated Digital Microfluidic Platform for Magnetic-Particle-Based Immunoassays with Optimization by Design of Exp.pdf:pdf},
journal = {Analytical chemistry},
pages = {9638--9646},
title = {{Automated Digital Microfluidic Platform for Magnetic-Particle-Based Immunoassays with Optimization by Design of Experiments}},
volume = {85},
year = {2013}
}
@incollection{Brandt2013,
author = {Brandt, Doug and Figard, Steve},
booktitle = {The Immunoassay Handbook},
chapter = {5.4},
doi = {10.1016/B978-0-08-097037-0.00007-5},
edition = {Fourth Ed},
editor = {Wild, David},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Wild2013\_Chap5\_4.pdf:pdf},
isbn = {9780080970370},
pages = {417--423},
publisher = {Elsevier Ltd},
title = {{Immunoassay Development in the In Vitro Diagnostic Industry}},
url = {http://dx.doi.org/10.1016/B978-0-08-097037-0.00007-5},
year = {2013}
}
@incollection{Davies2013,
annote = {
        From Duplicate 1 ( 
        
          Immunoassay Performance Measures
        
         - Davies, Chris )

        
        

        From Duplicate 1 ( 
        
        
          Immunoassay Performance Measures
        
        
         - Davies, Chris )

        
        

        From Duplicate 1 ( 
        
        
          Immunoassay Performance Measures
        
        
         - Davies, Chris )

        
        

        First reading: 17Jul2013
          
Context: background for ELISA project with Nandini

        
Come back to this for diagnostic properties research.  Relates to Noble2008.

        
Immunoassay people like their 'coefficient of variation'!  Do you think it is a good and robust estimate of expected variability at a given concentration?

        
Analytic vs functional sensitivity

        
Precision (or imprecision) is the variability in the signal of replicates (of the same concentration).  Precision generally changes with concentration and is usually described with the coefficient of variation (\%CV) or less frequently, standard deviation.  

        
Precision profile: precision over the assay range

        
The variability is caused by [cumulative] variability in binding (properties of the molecules, time and temperature conditions during incubation, etc), and errors in separation (not enough), detection (random?) and manipulation.  In portable POC assays, maybe ambient conditions (at least temperature) should be recorded.  Or maybe the machine controls well the temperature around the sample.

        

        From Duplicate 2 ( 
        
        
          Immunoassay Performance Measures
        
        
         - Davies, Chris )

        
        
 First reading: 17Jul2013 
Context: background for ELISA project with Nandini

        
Come back to this for diagnostic properties research. Relates to Noble2008.

        
Immunoassay people like their 'coefficient of variation'! Do you think it is a good and robust estimate of expected variability at a given concentration?

        
Analytic vs functional sensitivity

        
Precision (or imprecision) is the variability in the signal of replicates (of the same concentration). Precision generally changes with concentration and is usually described with the coefficient of variation (\%CV) or less frequently, standard deviation. 

        
Precision profile: precision over the assay range

        
The variability is caused by [cumulative] variability in binding (properties of the molecules, time and temperature conditions during incubation, etc), and errors in separation (not enough), detection (random?) and manipulation. In portable POC assays, maybe ambient conditions (at least temperature) should be recorded. Or maybe the machine controls well the temperature around the sample.

        

        

        

        From Duplicate 2 ( 
        
        
          Immunoassay Performance Measures
        
        
         - Davies, Chris )

        
        
 First reading: 17Jul2013 
Context: background for ELISA project with Nandini

        
Come back to this for diagnostic properties research. Relates to Noble2008.

        
Immunoassay people like their 'coefficient of variation'! Do you think it is a good and robust estimate of expected variability at a given concentration?

        
Analytic vs functional sensitivity

        
Precision (or imprecision) is the variability in the signal of replicates (of the same concentration). Precision generally changes with concentration and is usually described with the coefficient of variation (\%CV) or less frequently, standard deviation. 

        
Precision profile: precision over the assay range

        
The variability is caused by [cumulative] variability in binding (properties of the molecules, time and temperature conditions during incubation, etc), and errors in separation (not enough), detection (random?) and manipulation. In portable POC assays, maybe ambient conditions (at least temperature) should be recorded. Or maybe the machine controls well the temperature around the sample.

        

        

        

        From Duplicate 2 ( 
        
          Immunoassay Performance Measures
        
         - Davies, Chris )

        
        

        From Duplicate 1 ( 
        
        
          Immunoassay Performance Measures
        
        
         - Davies, Chris )

        
        

        From Duplicate 1 ( 
        
        
          Immunoassay Performance Measures
        
        
         - Davies, Chris )

        
        

        From Duplicate 1 ( 
        
        
          Immunoassay Performance Measures
        
        
         - Davies, Chris )

        
        

        First reading: 17Jul2013
          
Context: background for ELISA project with Nandini

        
Come back to this for diagnostic properties research.  Relates to Noble2008.

        
Immunoassay people like their 'coefficient of variation'!  Do you think it is a good and robust estimate of expected variability at a given concentration?

        
Analytic vs functional sensitivity

        
Precision (or imprecision) is the variability in the signal of replicates (of the same concentration).  Precision generally changes with concentration and is usually described with the coefficient of variation (\%CV) or less frequently, standard deviation.  

        
Precision profile: precision over the assay range

        
The variability is caused by [cumulative] variability in binding (properties of the molecules, time and temperature conditions during incubation, etc), and errors in separation (not enough), detection (random?) and manipulation.  In portable POC assays, maybe ambient conditions (at least temperature) should be recorded.  Or maybe the machine controls well the temperature around the sample.

        

        From Duplicate 2 ( 
        
        
          Immunoassay Performance Measures
        
        
         - Davies, Chris )

        
        
 First reading: 17Jul2013 
Context: background for ELISA project with Nandini

        
Come back to this for diagnostic properties research. Relates to Noble2008.

        
Immunoassay people like their 'coefficient of variation'! Do you think it is a good and robust estimate of expected variability at a given concentration?

        
Analytic vs functional sensitivity

        
Precision (or imprecision) is the variability in the signal of replicates (of the same concentration). Precision generally changes with concentration and is usually described with the coefficient of variation (\%CV) or less frequently, standard deviation. 

        
Precision profile: precision over the assay range

        
The variability is caused by [cumulative] variability in binding (properties of the molecules, time and temperature conditions during incubation, etc), and errors in separation (not enough), detection (random?) and manipulation. In portable POC assays, maybe ambient conditions (at least temperature) should be recorded. Or maybe the machine controls well the temperature around the sample.

        

        

        

        From Duplicate 2 ( 
        
        
          Immunoassay Performance Measures
        
        
         - Davies, Chris )

        
        
 First reading: 17Jul2013 
Context: background for ELISA project with Nandini

        
Come back to this for diagnostic properties research. Relates to Noble2008.

        
Immunoassay people like their 'coefficient of variation'! Do you think it is a good and robust estimate of expected variability at a given concentration?

        
Analytic vs functional sensitivity

        
Precision (or imprecision) is the variability in the signal of replicates (of the same concentration). Precision generally changes with concentration and is usually described with the coefficient of variation (\%CV) or less frequently, standard deviation. 

        
Precision profile: precision over the assay range

        
The variability is caused by [cumulative] variability in binding (properties of the molecules, time and temperature conditions during incubation, etc), and errors in separation (not enough), detection (random?) and manipulation. In portable POC assays, maybe ambient conditions (at least temperature) should be recorded. Or maybe the machine controls well the temperature around the sample.

        

        

        

        From Duplicate 2 ( 
        
        
          Immunoassay Performance Measures
        
        
         - Davies, Chris )

        
        

        First reading: 17Jul2013
          
Context: background for ELISA project with Nandini

        
Come back to this for diagnostic properties research.  Relates to Noble2008.

        
Immunoassay people like their 'coefficient of variation'!  Do you think it is a good and robust estimate of expected variability at a given concentration?

        
Analytic vs functional sensitivity

        
Precision (or imprecision) is the variability in the signal of replicates (of the same concentration).  Precision generally changes with concentration and is usually described with the coefficient of variation (\%CV) or less frequently, standard deviation.  

        
Precision profile: precision over the assay range

        
The variability is caused by [cumulative] variability in binding (properties of the molecules, time and temperature conditions during incubation, etc), and errors in separation (not enough), detection (random?) and manipulation.  In portable POC assays, maybe ambient conditions (at least temperature) should be recorded.  Or maybe the machine controls well the temperature around the sample.

        

        

        

      },
author = {Davies, Chris},
booktitle = {The Immunoassay Handbook},
chapter = {1.3},
doi = {10.1016/B978-0-08-097037-0.00082-8},
edition = {Fourth Ed},
editor = {Wild, David G},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Davies\_2013.pdf:pdf},
isbn = {9780080970370},
pages = {11--26},
publisher = {Elsevier Ltd},
title = {{Immunoassay Performance Measures}},
url = {http://dx.doi.org/10.1016/B978-0-08-097037-0.00082-8},
year = {2013}
}
@incollection{Christopoulos1996,
author = {Christopoulos, Theodore K and Diamandis, Eleftherios P},
booktitle = {Immunoassay},
chapter = {1},
edition = {1},
editor = {Christopoulos, Theodore K and Diamandis, Eleftherios P},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Christoploulos\_1996\_chap01.pdf:pdf},
keywords = {and future,past,present},
pages = {1--3},
publisher = {Elsevier Inc.},
title = {{PAST, PRESENT, AND FUTURE OF IMMUNOASSAYS}},
year = {1996}
}
@article{Komsta2012,
abstract = {The calibration of an analytical method is a very important part of its development, and only the proper statistical and chemometric evaluation of the results, together with understanding this process, allows good results. The purpose of this minireview is to call the reader’s attention to the major problems in calibration: curvilinearity, heteroscedasticity, presence of outliers, transformation of results, and distribution and autocorrelation of residuals. The common misunderstandings and mistakes are emphasized to inform the reader. Additionally, the computational package “quantchem” for GNU R environment, allowing full and automatic calibration evaluation, is presented.},
author = {Komsta, Lukasz},
doi = {10.5740/jaoacint.SGE},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Komsta\_2012.pdf:pdf},
journal = {Journal of AOAC International},
number = {3},
pages = {669--672},
title = {{Chemometric and Statistical Evaluation of Calibration Curves in Pharmaceutical Analysis—A Short Review on Trends and Recommendations}},
volume = {95},
year = {2012}
}
@article{Rodbard1976,
abstract = {We have developed practical methods for evaluating the magnitude of the random errors in radioimmunoassay dose-response variables, and the relationship between this error and position on the dose-response curve. This is important: to obtain appropriate weights for each point on the dose-response curve when utilizing least-squares curve-fitting methods; to evaluate whether the standards and the unknowns are subject to error of the same magnitude; for quality-control purposes; and to study the sources of errors in radioimmunoassay. Both standards and unknowns in radioimmunoassays for cAMP and cGMP were analyzed in triplicate. The sample mean (Y), sample standard deviation, S\_y, and variance ((s\_y)\^{}2) of the response variable were cal- culated for each dose level. The relationship between (s\_y)\^{}2 and Y-bar was calculated utilizing several models. Re- suits for standards and unknowns from several assays were pooled, and a curve smoothing procedure was used to minimize random sampling errors. This pooling increased the reliability of the analysis, and confirmed the presence of the theoretically predicted nonuniformity of variance. Thus, the calculation of results from these radioimmunoassays should utilize a *weighted* least-squares curve-fitting program. These analyses have been computerized, and can be used as a “pre- processor” for programs for routine analysis of results of radioimmunoassay.},
author = {Rodbard, David and Lenox, Robert H and Wray, H Linton and Ramseth, Douglas},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Rodbard\_1976.pdf:pdf},
journal = {Clinical chemistry},
keywords = {computer analysis,heteroscedasticity,precision,quality control,weighted least-squares regression},
number = {3},
pages = {350--358},
title = {{Statistical Characterization of the Random Errors in the Radioimmunoassay Dose-Response Variable}},
url = {http://www.clinchem.org/content/22/3/350.full.pdf+html},
volume = {22},
year = {1976}
}
@article{Darwish2006,
abstract = {Immunoassays are bioanalytical methods in which the quantitation of the analyte depends on the reac- tion of an antigen (analyte) and an antibody. Immunoassays have been widely used in many important areas of pharmaceutical analysis such as diagnosis of diseases, therapeutic drug monitoring, clinical pharmaco- kinetic and bioequivalence studies in drug discovery and pharmaceutical industries. The importance and widespread of immunoassay methods in pharmaceutical analysis are attributed to their inherent specific- ity, high-throughput, and high sensitivity for the analysis of wide range of analytes in biological samples. Recently, marked improvements were achieved in the field of immunoassay development for the purposes of pharmaceutical analysis. These improvements involved the preparation of the unique immunoanalytical reagents, analysis of new categories of compounds, methodology, and instrumentation. The basic methodolo- gies and recent advances in immunoassay methods applied in different fields of pharmaceutical analysis have been reviewed.},
author = {Darwish, Ibrahim A},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Darwish\_2006.pdf:pdf},
journal = {International Journal of Biomedical Sciense},
keywords = {antibodies,drug discovery,elisa,immunoassay,pharmaceutical analysis,pharmaceutical industry},
mendeley-tags = {elisa},
number = {2},
pages = {217--235},
title = {{Immunoassay Methods and their Applications in Pharmaceutical Analysis : Basic Methodology and Recent Advances}},
year = {2006}
}
@article{Hayashi2004,
abstract = {This paper puts forward a method to describe an equation of the within-plate uncertainty (relative standard deviation (R.S.D.) of measurements) as a function of analyte concentration in sandwich enzyme-linked immunosorbent assay (ELISA). A kit for thyroid stimulating hormone is taken as an example. The pipetting procedures of analyte solution and chromogen-substrate solution and absorbance inherent to the wells of a microplate are identified as major error sources and their variability is included as parameters in the uncertainty equation. These parameters can be determined by the experiments with distilled water. The theoretical R.S.D. is shown to be in good agreement with the results of the repeated experiments using the real samples. Since the theory gives a continuous plot of R.S.D. against concentration, the uncertainty structure of the ELISA kit can be recognized over a wide concentration range and the detection limit and quantitation range can easily be determined on the plot.},
author = {Hayashi, Yuzuru and Matsuda, Rieko and Maitani, Tamio and Ito, Katsutoshi and Nishimura, Waka and Imai, Kazuhiro and Maeda, Masako},
doi = {10.1016/j.jpba.2004.05.017},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hayashi et al. - 2004 - An expression of within-plate uncertainty in sandwich ELISA.pdf:pdf},
issn = {0731-7085},
journal = {Journal of pharmaceutical and biomedical analysis},
keywords = {Enzyme-Linked Immunosorbent Assay,Enzyme-Linked Immunosorbent Assay: methods,Enzyme-Linked Immunosorbent Assay: standards,Enzyme-Linked Immunosorbent Assay: statistics \& nu,Models,Reproducibility of Results,Research Design,Selection Bias,Theoretical,Thyrotropin,Thyrotropin: analysis,elisa},
mendeley-tags = {elisa},
month = sep,
number = {1},
pages = {225--9},
pmid = {15351070},
title = {{An expression of within-plate uncertainty in sandwich ELISA.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15351070},
volume = {36},
year = {2004}
}
@book{Godfrey-Smith2003,
author = {Godfrey-Smith, Peter},
keywords = {bayesian,philosophy of science},
mendeley-tags = {bayesian,philosophy of science},
publisher = {The University of Chicago Press},
title = {{Theory and Reality: An Introduction to the Philosophy of Science}},
year = {2003}
}
@article{Streiner2003,
author = {Streiner, David L},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Streiner\_2003.pdf:pdf},
journal = {Journal of Personality Assessment},
number = {1},
pages = {99--103},
title = {{Starting at the Beginning: An Introduction to Coefficient Alpha and Internal Consistency}},
volume = {80},
year = {2003}
}
@inproceedings{Killeen2011b,
author = {Killeen, Anthony and Styer, Patricia and Castellani, William},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/cvl\_webinar\_presentation.pdf:pdf},
isbn = {1562384988},
organization = {College of American Pathologists},
title = {{The Calibration Verification / Linearity Program: Meeting Regulatory Requirements and Improving Laboratory Quality}},
year = {2011}
}
@techreport{Tholen2004,
abstract = {NCCLS document EP17-A—Protocols for Determination of Limits of Detection and Limits of Quantitation; Approved Guideline provides protocols for determining the lower limit of detection of clinical laboratory methods, for verifying claimed limits, and for the proper use and interpretation of these limits. This document also provides guidance for determining lower limits of quantitation based on a laboratory’s goals for performance at low-levels. This applies to all quantitative procedures, even if the reported result is qualitative. EP17-A is intended for use by clinical laboratories and by manufacturers of in vitro diagnostic tests.},
author = {Tholen, Daniel W and Linnet, Kristian and Kondratovich, Marina and Armbuster, David A and Garrett, Patricia E and Jones, Robert L and Kroll, Martin H and Lequin, Rudolf M and Pankratz, Thomas J and Scassellati, G A and Schimmel, Heinz and Tsai, Jane},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/EP17-A.pdf:pdf},
institution = {National Committee for Clinical Laboratory Standards (NCCLS)},
isbn = {1562385518},
keywords = {calibration curve,clsi,elisa},
mendeley-tags = {calibration curve,clsi,elisa},
number = {34},
title = {{Protocols for Determination of Limits of Detection and Limits of Quantitation ; Approved Guideline EP17-A}},
volume = {24},
year = {2004}
}
@unpublished{Team2013,
author = {development Team), (STAN},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/stan-reference-2.1.0.pdf:pdf},
title = {{Stan Modeling Language: User's Guide and Reference Manual}},
year = {2013}
}
@techreport{Ivor2002,
abstract = {This document addresses the need for the clinical evaluation of new immunoassays and new applications of existing assays. Existing NCCLS documents provide guidance for assessing analytical performance, methods comparison, and clinical accuracy of laboratory tests. However, none of the documents define the elements that are integral to generating clinical data. As a guide to designing, executing, and analyzing a clinical evaluation, this document will aid clinical and regulatory personnel responsible for commercializing products, developers of “in-house” assays for institutional use, and developers of assays used for monitoring pharmacologic effects of new drugs or biologics. The elements of this guideline include (1) a brief review of the analytical performance measures that must be in place prior to testing clinical specimens; (2) a thorough discussion of the planning and design considerations that are necessary for a successful evaluation; (3) a description of requirements for conducting the evaluation through monitoring and database management; and (4) a development plan for an effective analysis and evaluation.},
author = {Ivor, Linda and Maxim, Peter and Parker, Donald R and Payne, Gregory P and Ridderhof, John and Ryerson, Carol and Schaefer, Lawrence E and Zweig, Mark H},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/ILA21-A.pdf:pdf},
isbn = {1562384554},
keywords = {clsi},
mendeley-tags = {clsi},
number = {9},
title = {{Clinical Evaluation of Immunoassays ; Approved Guideline I/LA21-A}},
volume = {22},
year = {2002}
}
@techreport{Tholen2003,
abstract = {NCCLS document EP6-A— Evaluation of the Linearity of Quantitative Measurement Procedures: A Statistical Approach; Approved Guideline is intended to provide both manufacturers and users of quantitative analytical methods with an economical and user-friendly method of establishing or verifying the linear range. This guideline also can be used to demonstrate the extent to which a quantitative analytical method meets clinical requirements or manufacturer's linear range claims.},
author = {Tholen, Daniel W and Kroll, Martin H and Astles, J Rex and Happe, Thomas M and Krouwer, Jan and Lasky, Fred},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/EP6-A.pdf:pdf},
isbn = {1562384988},
number = {16},
title = {{Evaluation of the Linearity of Quantitative Measurement Procedures: A Statistical Approach; Approved Guideline EP6-A}},
volume = {23},
year = {2003}
}
@article{Huang2012,
abstract = {In biomedical research such as the development of vaccines for infectious diseases or cancer, study outcomes measured by an assay or device are often collected from multiple sources or laboratories. Measurement error that may vary between laboratories needs to be adjusted for when combining samples across data sources.We incorporate such adjustment in the main study by comparing and combining independent samples from differ- ent laboratories via integration of external data, collected on paired samples from the same two laboratories.We propose the following: (i) normalization of individual-level data from two laboratories to the same scale via the expectation of true measurements conditioning on the observed; (ii) comparison of mean assay values between two independent samples in themain study accounting for inter-source measurement error; and (iii) sample size calculations of the paired-sample study so that hypothesis testing error rates are appropriately controlled in the main study comparison. Because the goal is not to estimate the true underlying measurements but to combine data on the same scale, our proposed methods do not require that the true values for the error-prone measure- ments are known in the external data. Simulation results under a variety of scenarios demonstrate satisfactory finite sample performance of our proposed methods when measurement errors vary.We illustrate our methods using real enzyme-linked immunosorbent spot assay data generated by two HIV vaccine laboratories.},
author = {Huang, Yunda Ying and Moodie, Zoe and Self, Steve},
doi = {10.1002/sim.5446},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Huang et al. - 2012 - Comparing and combining data across multiple sources via integration of paired-sample data to correct for measurem.pdf:pdf},
journal = {Statistics in medicine},
keywords = {assay comparison,inter-laboratory measurement error,multiple data sources,regression calibration},
number = {January},
pages = {3748----3759},
title = {{Comparing and combining data across multiple sources via integration of paired-sample data to correct for measurement error}},
volume = {31},
year = {2012}
}
@article{OConnell1993,
author = {O'Connell, M.a. and Belanger, B.a. and Haaland, P.D.},
doi = {10.1016/0169-7439(93)80008-6},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/OConnell\_1993.pdf:pdf},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
month = sep,
number = {2},
pages = {97--114},
title = {{Calibration and assay development using the four-parameter logistic model}},
url = {http://linkinghub.elsevier.com/retrieve/pii/0169743993800086},
volume = {20},
year = {1993}
}
@misc{Fong2013,
abstract = {nCal is a package that performs nonlinear calibration and curve fitting for data from Lu- minex, RT-PCR, ELISA, RPPA etc. Additional documentation can be found at http://research.fhcrc.org/youyifong/en/resources/ncal.html.},
author = {Fong, Youyi and Sebestyen, Krisztian},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/nCal.pdf:pdf},
publisher = {R},
title = {{Package ‘ nCal ’}},
url = {http://research.fhcrc.org/youyifong/en/resources/ncal.html},
year = {2013}
}
@incollection{Sturgeon2013,
author = {Sturgeon, Catharine M},
booktitle = {The Immunoassay Handbook},
chapter = {6.2},
doi = {10.1016/B978-0-08-097037-0.00042-7},
edition = {Fourth Ed},
editor = {Wild, David},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sturgeon - 2013 - Quality Assurance.pdf:pdf},
isbn = {9780080970370},
pages = {441--454},
publisher = {Elsevier Ltd},
title = {{Quality Assurance}},
url = {http://dx.doi.org/10.1016/B978-0-08-097037-0.00042-7},
volume = {1},
year = {2013}
}
@techreport{Fossceco1996,
abstract = {ELISA methods are fundamental tools in the pharmaceutical industry with applications in drug discovery, animal studies, and clinical trials. ELISAs are readily automated and thus capable of high throughput quantification of analyte concentrations. Like other categories of bioassay, ELISAs are often performed on a 96-well microplate in a standard 8-row by 12-column format. Response measures from microplates often exhibit reproducible row and column patterns and several levels of variability. The SAS Analyst Application provides a graphical user interface for basic statistical techniques that can be used by scientists to explore and assess the magnitude of patterns and variability sources in assay execution. The Analyst Application provides results in a project format that is useful as a template for scientists to communicate with statisticians about basic statistical analyses.},
author = {Fossceco, Stewart L and Curtis, Nathan A},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/analyst\_elisa\_SAS.pdf:pdf},
keywords = {96-well microplate,biological assay optimization,data visualization,elisa,variance component estimation},
title = {{Exploring Enzyme-Linked Immunosorbent Assay (ELISA) Data with the SAS® Analyst Application Introduction}},
year = {1996}
}
@article{Rocke2013,
abstract = {In enzyme-linked immunosorbent assay (ELISA), as well as in many other kinds of immunoassay, a log-logistic or similar-shaped calibration curve is fit using standards at a series of known levels and then used to transform the measured values for the unknowns into estimated concentrations. The choice of the number of standards, the concentration of the standards, and the number of replicates of the standards and of the unknowns all affect the precision of the measurement. This article develops an optimal design paradigm for this type of problem and shows how optimal choices can be calculated so that the system achieves the maximum precision of which it is capable. Although exact calculation of optimal designs requires use of a computer program, close approximations to the optimum can be derived from simple rules for hand calculation},
author = {Rocke, David M and Jones, Geoffrey},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Rocke\_2013.pdf:pdf},
journal = {Technometrics},
keywords = {Log-logistic curve,Precision,calibration,elisa},
mendeley-tags = {elisa},
number = {2},
pages = {162--170},
title = {{Optimal Design for ELISA and Other Forms of Immunoassay}},
volume = {39},
year = {2013}
}
@misc{Baker2008,
author = {Baker, Samuel L},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Baker\_2006.pdf:pdf},
number = {X},
title = {{Non-Linear Regression}},
year = {2008}
}
@unpublished{Haaland2011,
abstract = {Robust and unbiased methods for curve fitting and calibration in the analysis of biological and chemical assays are of great importance for ensuring reliable and interpretable results. Two issues which repeatedly arise in the analysis of assay data are heteroscadisticity and failure to capture the entire range of values in the independent variable. The issue of heteroscedasticity is addressed through a variant on the weighted least squares model termed the variance function estimating (VFE) model (Davidian and Haaland 1990). The second issue is dealt with by implementing the latter model fitting technique to the family of four parameter logistic (FPL) and linear regression models. In this vignette we present a detailed overview of the calibFit package and its application to the latter type of problem.},
author = {Haaland, Perry and Samarov, Daniel and Mcvey, Elaine},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Haaland, Samarov, Mcvey - 2011 - Using Fit.pdf:pdf},
keywords = {R package,calibration curve},
mendeley-tags = {R package,calibration curve},
title = {{Using Fit}},
year = {2011}
}
@article{Rodbard1974,
author = {Rodbard, David},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Rodbard\_1974.pdf:pdf},
issn = {0009-9147},
journal = {Clinical chemistry},
keywords = {Analysis of Variance,Analysis of Variance: methods,Angiotensin II,Angiotensin II: blood,Automatic Data Processing,Bradykinin,Bradykinin: analysis,Chorionic Gonadotropin,Chorionic Gonadotropin: analysis,Dose-Response Relationship,Drug,Estradiol,Estradiol: analysis,Evaluation Studies as Topic,Female,Follicle Stimulating Hormone,Follicle Stimulating Hormone: analysis,Humans,Luteinizing Hormone,Luteinizing Hormone: analysis,Morphine,Morphine: analysis,Progesterone,Progesterone: analysis,Quality Control,Radioimmunoassay,Radioimmunoassay: standards,Radioligand Assay,Renin,Renin: blood,Statistics as Topic,Testosterone,Testosterone: analysis},
month = oct,
number = {10},
pages = {1255--70},
pmid = {4370388},
title = {{Statistical quality control and routine data processing for radioimmunoassays and immunoradiometric assays.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/4370388},
volume = {20},
year = {1974}
}
@article{Agirbasli2013,
abstract = {Until recently, risk scoring systems for adult patients consisted of only clinical criteria. Currently, we are experiencing an abundant surge of literature integrating a wide range of biomarker arrays to clinical criteria in assessing the risk in an adult. In fact, novel risk scoring systems such as Reynolds criteria have emerged by combining the validated biomarkers to the traditional risk factors. Novel biomarkers potentially improve clinical management of cardiovascular disease, but there are gaps in understanding their role during childhood. The reason might be related to relatively lower prevalence of cardiovascular disease in children compared to the adult population. One exceptional group is the children with congenital heart disease. Recent studies indicate that novel biomarkers can alert the clinician in a timely manner about neurological and myocardial injury and their inflammatory consequences. Current technologies enable us to measure several biomarkers using only a few microliters of plasma. The preliminary studies show that novel biomarkers in addition to the traditionally studied biomarkers can help the clinician to identify children at high risk following pediatric heart surgery. Future studies are needed to confirm the role of biomarkers in monitoring children after cardiopulmonary bypass.},
author = {Aĝirbaşli, Mehmet and \"{U}ndar, Akif},
doi = {10.1111/j.1525-1594.2012.01573.x},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Agirbasli\_2013.pdf:pdf},
issn = {1525-1594},
journal = {Artificial organs},
keywords = {Biological Markers,Biological Markers: metabolism,Cardiac Surgical Procedures,Cardiopulmonary Bypass,Congenital,Congenital: metabolism,Congenital: surgery,Heart Defects,Humans,Pediatrics,Postoperative Complications,Postoperative Complications: metabolism,Risk Assessment},
month = jan,
number = {1},
pages = {10--15},
pmid = {23305569},
title = {{Monitoring biomarkers after pediatric heart surgery: a new paradigm on the horizon.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23305569},
volume = {37},
year = {2013}
}
@article{Euser2007,
abstract = {OBJECTIVES: Reliability studies are frequently organized within the context of a large (multicenter) study, with only a small sample of subjects measured by the observers of the large study. To estimate interobserver reliability, data from the large study are not frequently used. In this article, the advantages of combining data from the reliability study and the large study to improve the estimation of intra-class correlation coefficients (ICCs) are highlighted.

STUDY DESIGN AND SETTING: This was done within the scope of estimating fat percentages in the Project On Preterm and Small-for-gestational-age infants-19 (POPS-19) study and with simulations. To calculate ICCs, three approaches were used: (1) the classical approach using data from a reliability study only, (2) the combined variances approach using inter-subject variances from the POPS-19 study, and (3) the maximum likelihood approach using all data.

RESULTS: The ICCs (95\% confidence interval [CI]) for fat percentage calculated by the three approaches were 0.84 (0.57, 0.99), 0.94 (0.90, 0.97), and 0.94 (0.88, 0.97), respectively.

CONCLUSION: The efficient use of data by combining data from a small reliability study with the data from the large study itself for the calculation of ICCs will lead to more precise ICCs.},
author = {Euser, Anne M and le Cessie, Saskia and Finken, Martijn J J and Wit, Jan M and Dekker, Friedo W},
doi = {10.1016/j.jclinepi.2006.09.014},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Euser et al. - 2007 - Reliability studies can be designed more efficiently by using variance components estimates from different sources.pdf:pdf},
issn = {0895-4356},
journal = {Journal of clinical epidemiology},
keywords = {Adult,Data Interpretation,Female,Humans,Infant,Male,Multicenter Studies as Topic,Multicenter Studies as Topic: methods,Newborn,Observer Variation,Premature,Reproducibility of Results,Research Design,Skinfold Thickness,Small for Gestational Age,Statistical},
month = oct,
number = {10},
pages = {1010----1014},
pmid = {17884594},
title = {{Reliability studies can be designed more efficiently by using variance components estimates from different sources}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17884594},
volume = {60},
year = {2007}
}
@article{MiraiBio,
author = {(MiraiBio)},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/MiraiBio Terms-and-Conditions.pdf:pdf},
pages = {1--7},
title = {{Terms and Conditions}}
}
@article{Tomonaga2011,
abstract = {BACKGROUND: Evidence of the clinical benefit of 3-in-1 point-of-care testing (POCT) for cardiac troponin T (cTnT), N-terminal pro-brain natriuretic peptide (NT-proBNP) and D-dimer in cardiovascular risk stratification at primary care level for diagnosing acute coronary syndromes (ACS), heart failure (HF) and thromboembolic events (TE) is very limited. The aim of this study is to analyse the diagnostic accuracy of POCT in primary care.

METHODS: Prospective multicentre controlled trial cluster-randomised to POCT-assisted diagnosis and conventional diagnosis (controls). Men and women presenting in 68 primary care practices in Zurich County (Switzerland) with chest pain or symptoms of dyspnoea or TE were consecutively included after baseline consultation and working diagnosis. A follow-up visit including confirmed diagnosis was performed to determine the accuracy of the working diagnosis, and comparison of working diagnosis accuracy between the two groups.

RESULTS: The 218 POCT patients and 151 conventional diagnosis controls were mostly similar in characteristics, symptoms and pre-existing diagnoses, but differed in working diagnosis frequencies. However, the follow-up visit showed no statistical intergroup difference in confirmed diagnosis frequencies. Working diagnoses overall were significantly more correct in the POCT group (75.7\% vs 59.6\%, p = 0.002), as were the working diagnoses of ACS/HF/TE (69.8\% vs 45.2\%, p = 0.002). All three biomarker tests showed good sensitivity and specificity.

CONCLUSION: POCT confers substantial benefit in primary care by correctly diagnosing significantly more patients.

TRIAL REGISTRATION: DRKS: DRKS00000709.},
author = {Tomonaga, Yuki and Gutzwiller, Felix and L\"{u}scher, Thomas F and Riesen, Walter F and Hug, Markus and Diemand, Albert and Schwenkglenks, Matthias and Szucs, Thomas D},
doi = {10.1186/1471-2296-12-12},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Tomonaga\_2011.pdf:pdf},
issn = {1471-2296},
journal = {BMC family practice},
keywords = {Acute Coronary Syndrome,Acute Coronary Syndrome: blood,Acute Coronary Syndrome: diagnosis,Aged,Aged, 80 and over,Biological Markers,Biological Markers: blood,Cluster Analysis,Female,Fibrin Fibrinogen Degradation Products,Fibrin Fibrinogen Degradation Products: metabolism,Heart Failure,Heart Failure: blood,Heart Failure: diagnosis,Humans,Male,Middle Aged,Natriuretic Peptide, Brain,Natriuretic Peptide, Brain: blood,Peptide Fragments,Peptide Fragments: blood,Point-of-Care Systems,Point-of-Care Systems: standards,Primary Health Care,Prospective Studies,Reproducibility of Results,Risk Factors,Thromboembolism,Thromboembolism: blood,Thromboembolism: diagnosis,Troponin T,Troponin T: blood},
month = jan,
number = {1},
pages = {12},
pmid = {21435203},
publisher = {BioMed Central Ltd},
title = {{Diagnostic accuracy of point-of-care testing for acute coronary syndromes, heart failure and thromboembolic events in primary care: a cluster-randomised controlled trial.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3071323\&tool=pmcentrez\&rendertype=abstract},
volume = {12},
year = {2011}
}
@article{Yager2008,
abstract = {Biomedical engineers have traditionally developed technologies in response to the needs of the developed world's medical community. As a result, the diagnostic systems on which they have worked have met the requirements of well-funded laboratories in highly regulated and quality-assessed environments. However, such approaches do not address the needs of the majority of the world's people afflicted with infectious diseases, who have, at best, access to poorly resourced health care facilities with almost no supporting clinical laboratory infrastructure. A major challenge for the biomedical engineering community is to develop diagnostic tests to meet the needs of these people, the majority of whom are in the developing world. We here review the context in which the diagnostics must operate, some of the appropriate diagnostic technologies already in distribution, and some emerging technologies that promise to address this challenge. However, there is much room for innovation, adaptation, and cost reduction before these technologies can impact health care in the developing world.},
author = {Yager, Paul and Domingo, Gonzalo J and Gerdes, John},
doi = {10.1146/annurev.bioeng.10.061807.160524},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Yager\_2008.pdf:pdf},
issn = {1523-9829},
journal = {Annual review of biomedical engineering},
keywords = {Biomedical,Diagnostic Techniques and Procedures,Diagnostic Techniques and Procedures: instrumentat,Diagnostic Techniques and Procedures: trends,Monitoring,POC,Physiologic,Physiologic: instrumentation,Physiologic: trends,Point-of-Care Systems,Point-of-Care Systems: trends,Technology Assessment,World Health},
mendeley-tags = {POC},
month = jan,
pages = {107----144},
pmid = {18358075},
title = {{Point-of-care diagnostics for global health.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18358075},
volume = {10},
year = {2008}
}
@article{Araujo2009,
abstract = {Method validation may be regarded as one of the most well-known areas in analytical chemistry as is reflected in the substantial number of articles submitted and published in peer review journals every year. However, some of the relevant parameters recommended by regulatory bodies are often used interchangeably and incorrectly or are miscalculated, due to few references to evaluate some of the terms as well as wrong application of the mathematical and statistical approaches used in their estimation. These mistakes have led to misinterpretation and ambiguity in the terminology and in some instances to wrong scientific conclusions. In this article, the definitions of various relevant performance indicators such as selectivity, specificity, accuracy, precision, linearity, range, limit of detection, limit of quantitation, ruggedness, and robustness are critically discussed with a view to prevent their erroneous usage and ensure scientific correctness and consistency among publications.},
author = {Araujo, Pedro},
doi = {10.1016/j.jchromb.2008.09.030},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Araujo\_2009.pdf:pdf},
issn = {1873-376X},
journal = {Journal of chromatography. B, Analytical technologies in the biomedical and life sciences},
keywords = {Analytical,Analytical: methods,Analytical: standards,Analytical: statistics \& num,Chemistry Techniques,Guidelines as Topic,Linear Models,Terminology as Topic,Validation Studies as Topic},
month = aug,
number = {23},
pages = {2224--2234},
pmid = {18929516},
title = {{Key aspects of analytical method validation and linearity evaluation.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18929516},
volume = {877},
year = {2009}
}
@article{Nagler1995,
author = {Nagler, Jonathan},
journal = {The Political Methodologist},
keywords = {computing},
mendeley-tags = {computing},
number = {2},
title = {{Coding Style and Good Computing Practices}},
url = {http://www.nyu.edu/classes/nagler/quant2/coding\_style.html},
volume = {6},
year = {1995}
}
@article{Demidenko2013,
abstract = {This paper develops a new metric, the standard error of inverse prediction (SEIP), for a dose– response relationship (calibration curve) when dose is estimated from response via inverse regression. SEIP can be viewed as a generalization of the coefficient of variation to regression problem when x is predicted using y -value. We employ nonstandard statistical methods to treat the inverse prediction, which has an infinite mean and variance due to the presence of a normally distributed variable in the denominator. We develop confidence intervals and hypothesis testing for SEIP on the basis of the normal approximation and using the exact statistical inference based on the noncentral t -distribution. We derive the power functions for both approaches and test them via statistical simulations. The theoretical SEIP, as the ratio of the regression standard error to the slope, is viewed as reciprocal of the signal-to-noise ratio, a popular measure of signal processing. The SEIP, as a figure of merit for inverse prediction, can be used for comparison of calibration curves with different dependent variables and slopes. We illustrate our theory with electron paramagnetic resonance tooth dosimetry for a rapid estimation of the radiation dose received in the event of nuclear terrorism.},
author = {Demidenko, Eugene and Williams, Benjamin B and Flood, Ann Barry and Swartz, Harold M},
doi = {10.1002/sim.5668.Standard},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Demidenko et al. - 2013 - Standard error of inverse prediction for dose-response relationship approximate and exact statistical inferenc.pdf:pdf},
journal = {Statistics in medicine},
keywords = {calibration,calibration curve,coefficient of variation,confidence interval,coverage probability,dose,nuclear terrorism,power function,response},
mendeley-tags = {calibration curve},
number = {12},
pages = {2048--2061},
title = {{Standard error of inverse prediction for dose-response relationship: approximate and exact statistical inference}},
volume = {32},
year = {2013}
}
@book{Reed2010,
author = {Reed, Roberta},
editor = {Armbuster, Dave and Cooper, Kelley},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/CC\_09\_10552\_ClinChemLearning\_singlepageFinal\_072710.pdf:pdf},
publisher = {Abbott Laboratories},
title = {{Learning Guide: Clinical Chemistry}},
year = {2010}
}
@article{FerrantediRuffano2012,
abstract = {The value of a diagnostic test is not simply measured by its accuracy, but depends on how it affects patient health. This article presents a framework for the design and interpretation of studies that evaluate the health consequences of new diagnostic tests},
author = {{Ferrante di Ruffano}, Lavinia and Hyde, Christopher J and Mccaffery, Kirsten J and Bossuyt, Patrick M M and Deeks, Jonathan J},
doi = {10.1136/bmj.e686},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Ferrante di Ruffano et al. - 2012 - Assessing the value of diagnostic tests a framework for designing and evaluating trials.pdf:pdf},
journal = {British Medical Journal},
number = {February},
pages = {1--9},
title = {{Assessing the value of diagnostic tests: a framework for designing and evaluating trials}},
volume = {344},
year = {2012}
}
@book{Law2005,
author = {Law, Brian and Copley, Clive G and Biddlecombe, Robert A and Warwick, Michael J and Malone, Michael D and Jenner, William J},
edition = {e-book},
editor = {Law, Brian},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Law et al. - 2005 - Immunoassay a Practical Guide.pdf:pdf},
isbn = {0203483499},
keywords = {calibration curve,elisa},
mendeley-tags = {calibration curve,elisa},
publisher = {Taylor \& Francis},
title = {{Immunoassay: A Practical Guide}},
year = {2005}
}
@article{Lachos2013,
author = {Lachos, Victor H. and Castro, Luis M. and Dey, Dipak K.},
doi = {10.1016/j.csda.2013.02.011},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Lachos\_2013.pdf:pdf},
issn = {01679473},
journal = {Computational Statistics \& Data Analysis},
month = aug,
pages = {237--252},
publisher = {Elsevier B.V.},
title = {{Bayesian inference in nonlinear mixed-effects models using normal independent distributions}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0167947313000558},
volume = {64},
year = {2013}
}
@incollection{Wild2013b,
abstract = {No table of contents available},
annote = {
        From Duplicate 1 ( 
        
          How to Use This Book
        
         - Wild, David )

        
        

        From Duplicate 1 ( 
        
        
          How to Use This Book
        
        
         - Wild, David )
And  Duplicate 2 ( 
        
        
          How to Use This Book
        
        
         - Wild, David )
And  Duplicate 4 ( 
        
        
          How to Use This Book
        
        
         - Wild, David )
And  Duplicate 5 ( 
        
        
          How to Use This Book
        
        
         - Wild, David )
And  Duplicate 6 ( 
        
        
          How to Use This Book
        
        
         - Wild, David )
And  Duplicate 7 ( 
        
        
          How to Use This Book
        
        
         - Wild, David )
And  Duplicate 8 ( 
        
        
          How to Use This Book
        
        
         - Wild, David )

        
        

        

        

        

        

        From Duplicate 2 ( 
        
          How to Use This Book
        
         - Wild, David )

        
        

        

        

      },
author = {Wild, David},
booktitle = {The Immunoassay Handbook},
chapter = {1.1},
doi = {10.1016/B978-0-08-097037-0.00091-9},
edition = {Fourth Ed},
editor = {Wild, David},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wild - 2013 - How to Use This Book.pdf:pdf},
isbn = {9780080970370},
pages = {3--5},
publisher = {Elsevier Ltd},
title = {{How to Use This Book}},
url = {http://dx.doi.org/10.1016/B978-0-08-097037-0.00091-9},
year = {2013}
}
@article{Tatsioni2005,
abstract = {Diagnostic tests are critical components of effective health care. They help determine treatments that are most beneficial for a given patient. Their assessment is a complex process that includes such challenges as a dearth of studies that evaluate clinical out- comes and lack of data on use of the test in realistic clinical settings. The methodologic quality of studies of diagnostic tests also lags behind the quality of studies of therapeutic interven- tions. Statistical methods to combine diagnostic accuracy data are more complex and not as well developed, leading to difficulties in the interpretation of results. The Agency for Healthcare Research and Quality Technology Assessment Program has adopted a 6-level framework for evaluating diagnostic technologies. The model emphasizes the need for systematic reviews of diagnostic test studies to go beyond the assessment of technical feasibility and accuracy to examine the impact of the test on health out- comes. In this paper, we use examples from 3 Evidence-based Practice Center reports to illustrate 3 challenges reviewers may face when reviewing diagnostic test literature: finding relevant studies, assessing methodologic quality of diagnostic accuracy studies, and synthesizing studies that evaluate tests in different patient populations or use different outcomes.},
author = {Tatsioni, Athina and Zarin, Deborah A and Aronson, Naomi and Samson, David J and Flamm, Carole R and Schmid, Christopher and Lau, Joseph},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Tatsioni\_2005.pdf:pdf},
journal = {Annals of Internal Medicine},
number = {June},
pages = {1048----1055},
title = {{Challenges in Systematic Reviews of Diagnostic Technologies}},
volume = {12},
year = {2005}
}
@incollection{Fox2011,
abstract = {The nonlinear regression model generalizes the linear regression model by allowing for mean functions like \$E(y∣x) = \backslash theta\_1/ \{1+exp[−(\backslash theta\_2 +\backslash theta\_3x)]\}, in which the parameters, the \backslash theta s in this example, enter the mean function nonlinearly. If we assume additive errors, then the parameters in models like this one are often estimated via least squares. In this appendix to Fox and Weisberg (2011) we describe how the nls function in R can be used to obtain estimates, and briefly discuss some of the major issues with nonlinear least squares estimation.},
author = {Fox, John and Weisberg, Sanford},
booktitle = {An R Companion to Applied Regression},
chapter = {appendix},
edition = {2nd},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Fox2010\_appendix-nonlinear-regression-update.pdf:pdf},
number = {December 2010},
pages = {1--20},
title = {{Nonlinear Regression and Nonlinear Least Squares in R}},
year = {2011}
}
@misc{Fong2013a,
author = {Fong, Youyi},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Fong\_nCal-vignette.pdf:pdf},
number = {3},
pages = {1--8},
title = {{nCal - Some Examples}},
year = {2013}
}
@article{Greenland2013,
abstract = {In response to the widespread abuse and misinterpretation of significance tests of null hypotheses, some editors and authors have strongly discouraged P values. However, null P values still thrive in most journals and are routinely misinterpreted as probabilities of a "chance finding" or of the null, when they are no such thing. This misuse may be lessened by recognizing correct Bayesian interpretations. For example, under weak priors, 95\% confidence intervals approximate 95\% posterior probability intervals, one-sided P values approximate directional posterior probabilities, and point estimates approximate posterior medians. Furthermore, under certain conditions, a one-sided P value for a prior median provides an approximate lower bound on the posterior probability that the point estimate is on the wrong side of that median. More generally, P values can be incorporated into a modern analysis framework that emphasizes measurement of fit, distance, and posterior probability in place of "statistical significance" and accept/reject decisions.},
author = {Greenland, Sander and Poole, Charles},
doi = {10.1097/EDE.0b013e3182785741},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Greenland\_2013.pdf:pdf},
issn = {1531-5487},
journal = {Epidemiology (Cambridge, Mass.)},
keywords = {Bayes Theorem,Confidence Intervals,Data Interpretation, Statistical,Likelihood Functions,Models, Statistical},
month = jan,
number = {1},
pages = {62--8},
pmid = {23232611},
title = {{Living with p values: resurrecting a Bayesian perspective on frequentist statistics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/23232611},
volume = {24},
year = {2013}
}
@article{Lei2012,
abstract = {Because of intensive developments in recent years, the microfluidic system has become a powerful tool for biological analysis. Entire analytic protocols including sample pretreatment, sample/reagent manipulation, separation, reaction, and detection can be integrated into a single chip platform. A lot of demonstrations on the diagnostic applications related to genes, proteins, and cells have been reported because of their advantages associated with miniaturization, automation, sensitivity, and specificity. The aim of this article is to review recent developments in microfluidic systems for diagnostic applications. Based on the categories of various fluid-manipulating mechanisms and biological detection approaches, in-depth discussion of the microfluidic-based diagnostic systems is provided. Moreover, a brief discussion on materials and manufacturing techniques will be included. The current excellent integration of microfluidic systems and diagnostic applications suggests a solid foundation for the development of practical point-of-care devices.},
author = {Lei, Kin Fong},
doi = {10.1177/2211068212454853},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Lei\_2012.pdf:pdf},
issn = {2211-0682},
journal = {Journal of laboratory automation},
keywords = {Clinical Laboratory Techniques,Clinical Laboratory Techniques: methods,Diagnostic Tests,Humans,Microfluidics,Microfluidics: methods,Point-of-Care Systems,Routine,Routine: methods,Specimen Handling,Specimen Handling: methods,elisa},
mendeley-tags = {elisa},
month = oct,
number = {5},
pages = {330--47},
pmid = {22893635},
title = {{Microfluidic systems for diagnostic applications: a review.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22893635},
volume = {17},
year = {2012}
}
@article{Schunemann2008,
abstract = {The GRADE system can be used to grade the quality of evidence and strength of recommendations for diagnostic tests or strategies. This article explains how patient-important outcomes are taken into account in this process.},
author = {Sch\"{u}nemann, A Holger J and Oxman, Andrew D and Brozek, Jan and Glasziou, Paul and Jaeschke, Roman and Vist, Gunn E and Williams, John W (Jr) and Kunz, Regina and Craig, Jonathan and Montori, Victor M and Bossuyt, Patrick and Guyatt, Gordon H},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Schunemann\_2008.pdf:pdf},
journal = {British Medical Journal},
number = {May},
pages = {1106----1110},
title = {{GRADE : grading quality of evidence and strength of recommendations for diagnostic tests and strategies}},
volume = {336},
year = {2008}
}
@incollection{Nichols2013,
author = {Nichols, James H},
booktitle = {The Immunoassay Handbook},
chapter = {6.3},
doi = {10.1016/B978-0-08-097037-0.00043-9},
edition = {Fourth Ed},
editor = {Wild, David},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nichols - 2013 - Point-of-Care Testing.pdf:pdf},
isbn = {9780080970370},
pages = {455--463},
publisher = {Elsevier Ltd},
title = {{Point-of-Care Testing}},
year = {2013}
}
@article{Greenwood1950,
abstract = {One frequently encounters the need of a rational selection of sample size when it is desired to estimate the standard devia- tion. Changing the permissible error of the estimate from an absolute to a relative one is acceptable in many cases and per- mits an exact, a priori solution to the problem of sample size without involving any previous estimate.},
author = {Greenwood, Joseph A and Sandomire, Marion M},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Greenwood\_1950.pdf:pdf},
institution = {U.S. Navy Reserach},
journal = {Journal of the American Statistical Association},
number = {250},
pages = {257----260},
title = {{Sample Size Required For Estimating The Standard Deviation as a Percent of Its True Value}},
url = {http://digitalcommons.unl.edu/usnavyresearch Greenwood,},
volume = {45},
year = {1950}
}
@article{Ritz2005,
abstract = {We describe an add-on package for the language and environment R which allows simultaneous fitting of several non-linear regression models. The focus is on analysis of dose response curves, but the functionality is applicable to arbitrary non-linear regression models. Features of the package is illustrated in examples.},
author = {Ritz, Christian and Streibig, Jens C},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Ritz\_2005.pdf:pdf},
journal = {Journal of Statistical Software},
keywords = {dose response data,multiple curves,non-linear regression},
number = {5},
title = {{Bioassay Analysis using R}},
volume = {12},
year = {2005}
}
@article{Hayashi2004a,
abstract = {This paper develops a mathematical model for describing the within-plate variation as the RSD (relative standard deviation) of absorbance measurements in a wide concentration range in competitive ELISA and proposes a method for determining the limit of detection (LOD) and range of quantitation (ROQ). The ELISA for 17 alpha-hydroxyprogesterone is taken as an example. The theoretical RSD description involves analyte concentration as an independent variable and error sources as parameters which concern the pipetting and absorbance measurement. Our model can dispense with repeated experiments of real samples, but the error parameters should be determined experimentally. The theory is in good agreement with the experiments. The most influential error sources at low and high sample concentrations are shown to be the pipetting of a viscous solution of antiserum and the absorbance inherent to the wells of a plate, respectively. The LOD and ROQ are defined as the concentration with 30\% RSD and the region with <10\% RSD, respectively, and are found in the theoretical plot of the RSD of concentration estimates vs concentration.},
author = {Hayashi, Yuzuru and Matsuda, Rieko and Maitani, Tamio and Imai, Kazuhiro and Nishimura, Waka and Ito, Katsutoshi and Maeda, Masako},
doi = {10.1021/ac0302859},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Hayashi et al. - 2004 - Precision, limit of detection and range of quantitation in competitive ELISA.pdf:pdf},
issn = {0003-2700},
journal = {Analytical chemistry},
keywords = {17-alpha-Hydroxyprogesterone,17-alpha-Hydroxyprogesterone: analysis,Calibration,Enzyme-Linked Immunosorbent Assay,Enzyme-Linked Immunosorbent Assay: methods,Enzyme-Linked Immunosorbent Assay: standards,Models,Research Design,Sensitivity and Specificity,Theoretical,Time Factors,calibration curve,elisa},
mendeley-tags = {calibration curve,elisa},
month = mar,
number = {5},
pages = {1295--301},
pmid = {14987084},
title = {{Precision, limit of detection and range of quantitation in competitive ELISA.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17297235},
volume = {76},
year = {2004}
}
@incollection{Lesaffre2012_chap1,
author = {Lesaffre, Emmanuel and Lawson, Andrew B},
booktitle = {Bayesian Biostatistics},
chapter = {1},
doi = {10.1007/BFb0083601},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lesaffre, Lawson - 2012 - Basic concepts in Bayesian Methods.pdf:pdf},
isbn = {978-3-540-51485-5},
pages = {3--19},
title = {{Basic concepts in Bayesian Methods}},
volume = {1387},
year = {2012}
}
@techreport{Hannon2004,
abstract = {NCCLS document I/LA23-A—Assessing the Quality of Immunoassay Systems: Radioimmunoassays and Enzyme, Fluorescence, and Luminescence Immunoassays; Approved Guideline addresses components for harmonizing and assessing the quality of immunoassay systems for several commonly used dose-response indicator categories, (e.g., radioisotopes, enzymes, fluorescence, luminescence, reagents, and experimental components criteria) essential to characterizing an immunoassay. The Area Committee on Immunology and Ligand Assays merged NCCLS documents LA1-A2—Assessing the Quality of Radioimmunoassay Systems; Approved Guideline—Second Edition and DI4-T—Enzyme and Fluorescence Immunoassays; Tentative Guideline into one document assimilating the residual segments of LA1-A2, and updating information in DI4-T into a more generic model, along with the addition of new information for each topic. I/LA23-A has broader utility and applicability while providing resource information previously available in the other two documents. This new guideline describes the iterations in the development, performance characterization, and certification from sample collection to method transferability. Specific nuances of each of the different dose-response systems for immunoassays are addressed while placing emphasis on mechanisms to assess the quality of the different immunoassay systems—factors that contribute to reliable and reproducible results. This guideline is particularly useful for specific details on optimization and harmonization of immunoassays, especially for those measurands (analytes) that are measured only by quantitation of antigen- antibody reactions (e.g., protein hormones, IgG, serum specific proteins). Suggested Citation (NCCLS. Assessing the Quality of Immunoassay Systems: Radioimmunoassays and Enzyme, Fluorescence, and Luminescence Immunoassays; Approved Guideline. NCCLS document I/LA23-A [ISBN 1-56238-533-X]. NCCLS, 940 West Valley Road, Suite 1400, Wayne, Pennsylvania 19087-1898 USA, 2004.)},
author = {Hannon, W Harry and Atkinson, Mark A and Ball, Dorothy J and Lorenz, Robin G and Matsson, Per N J and Moore, Deborah M and Whitley, Ronald J},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Hannon\_2004.pdf:pdf},
isbn = {156238533X},
keywords = {clsi},
mendeley-tags = {clsi},
number = {16},
series = {CLSI},
title = {{Assessing the Quality of Immunoassay Systems : Radioimmunoassays and Enzyme , Fluorescence , and Luminescence Immunoassays ; Approved Guideline I/LA23-A}},
volume = {24},
year = {2004}
}
@article{Christenson2012,
abstract = {Biomarkers typically evolve from a research setting to use in clinical care as evidence for their independent contribution to patient management accumulates. This evidence relies heavily on knowledge of the preanalytical, analytical, and postanalytical characteristics of the biomarker's measurement. For the preanalytical phase, considerations such specimen type, acceptable anticoagulants for blood samples, biologic variation and stability of the biomarker under various conditions are key. The analytical phase entails critical details for development and maintenance of assays having performance characteristics that are "fit for service" for the clinical application at hand. Often, these characteristics describe the ability to measure minute quantities in the biologic matrix used for measurement. Although techniques such as mass spectrometry are used effectively for biomarker discovery, routine quantification often relies on use of immunoassays; early in development, the most common immunoassay used is the enzyme-linked immunosorbent assay format. As biomarkers evolve successfully, they will be adapted to large main laboratory platforms or, depending on the need for speed, point-of-care devices. Users must pay particular attention to performance parameters of assays they are considering for clinical implementation. These parameters include the limit of blank, a term used to describe the limit of analytical noise for an assay; limit of detection, which describes the lowest concentration that can reliably be discriminated from analytical noise; and perhaps most importantly, the limit of quantitation, which is the lowest concentration at which a biomarker can be reliably measured within some predefined specifications for total analytical error that is based on clinical requirements of the test. The postanalytical phase involves reporting biomarker values, which includes reporting units, any normalization factors, and interpretation. Standardization, a process that involves metrological traceability to a primary reference material and definitive measurement method, is important to assure that all biomarker values are transferable in the literature and across institutions. In the absence of standardization, assays can be harmonized using secondary reference materials so that biomarker values can be combined for meta-analysis and interpreted clinically with common reference and decision limit values.},
author = {Christenson, Robert H and Duh, Show-Hong},
doi = {10.1016/j.pcad.2012.05.001},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Christenson, Duh - 2012 - Methodological and analytic considerations for blood biomarkers.pdf:pdf},
issn = {1873-1740},
journal = {Progress in cardiovascular diseases},
keywords = {Biological Assay,Biological Assay: standards,Biological Markers,Biological Markers: blood,Biomedical Research,Biomedical Research: standards,Guidelines as Topic,Humans,Limit of Detection,Predictive Value of Tests,Prognosis,Reproducibility of Results,Research Design,Research Design: standards,calibration curve},
mendeley-tags = {calibration curve},
number = {1},
pages = {25--33},
pmid = {22824107},
publisher = {Elsevier Inc.},
title = {{Methodological and analytic considerations for blood biomarkers.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22824107},
volume = {55},
year = {2012}
}
@article{Vickers2006,
abstract = {Background—Diagnostic and prognostic models are typically evaluated with measures of accuracy that do not address clinical consequences. Decision-analytic techniques allow assessment of clinical outcomes, but often require collection of additional information, and may be cumbersome to apply to models that yield a continuous result. We sought a method for evaluating and comparing prediction models that incorporates clinical consequences, requires only the dataset on which the models are tested, and can be applied to models that have either continuous or dichotomous results. Method—We describe decision curve analysis, a simple, novel method of evaluating predictive models. We start by assuming that the threshold probability of a disease or event at which a patient would opt for treatment is informative of how the patient weighs the relative harms of a false-positive and a false-negative prediction. This theoretical relationship is then used to derive the net benefit of the model across different threshold probabilities. Plotting net benefit against threshold probability yields the “decision curve”. We apply the method to models for the prediction of seminal vesicle invasion in prostate cancer patients. Decision curve analysis identified the range of threshold probabilities in which a model was of value, the magnitude of benefit, and which of several models was optimal. Conclusion—Decision curve analysis is a suitable method for evaluating alternative diagnostic and prognostic strategies that has advantages over other commonly used measures and techniques.},
author = {Vickers, Andrew J and Elkin, Elena B},
doi = {10.1177/0272989X06295361.Decision},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Vickers\_2006.pdf:pdf},
journal = {Medical Decision Making},
keywords = {decision analysis,multivariate analysis,prediction models},
number = {6},
pages = {565--574},
title = {{Decision curve analysis: a novel method for evaluating prediction models}},
volume = {26},
year = {2006}
}
@article{Gottschalk2005,
abstract = {Improvements in assay technology have reduced the amount of random variation in measured responses to the point where even slight asymmetry of the assay data can be more significant than random variation. Use of the five-parameter logistic (5PL) function to fit dose–response data easily accommodates such asymmetry. The 5PL can dramatically improve the accuracy of asymmetric assays over the use of symmetric models such as the four-parameter logistic (4PL) function. Until recently, however, the process of fitting the 5PL function has been difficult, with the result that the 4PL function has continued to be used even for highly asym- metric data. Various ad hoc modifications of the 4PL method have been developed in an attempt to address asymmetric data. How- ever, recent advances in numerical methods and assay analysis software have rendered easier the fitting of the 5PL routine. This paper demonstrates how use of the 5PL function can improve assay performance over the 4PL and its variants. Specifically, the improvement in the accuracy of concentration estimates that can be obtained using the 5PL over the 4PL as a function of the asym- metry present in the data is studied. The behavior of the 5PL curve and how it differs from the 4PL curve are discussed. Common experimental designs, which can lead to ill-conditioned regression problems, are also examined.},
author = {Gottschalk, Paul G and Dunn, John R},
doi = {10.1016/j.ab.2005.04.035},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Gottschalk\_2005.pdf:pdf},
journal = {Analytical Biochemistry},
keywords = {4pl,5pl,bioassay,calibration curve,curve model,data reduction,dose-response curve,statistical analysis},
mendeley-tags = {calibration curve},
pages = {54--65},
title = {{The five-parameter logistic: A characterization and comparison with the four-parameter logistic}},
volume = {343},
year = {2005}
}
@article{Herman2008,
abstract = {Appropriately modeled calibration curves are important for accurately estimating the concentrations of proteins in samples evaluated in sandwich-format enzyme-linked immunosorbent assay (ELISA). Calibration curves are commonly fit using polynomial or logistic models. We compared the fit of a quadratic, cubic and 4-parameter logistic model for highly-replicated calibration curves across seven assays used for quantifying transgenic proteins in commercial crops. Results indicate that it is typically undesirable to include zero-concentration data when modeling these curves over the quantitative range, and simple polynomial models are typically preferable to the commonly recommended 4-parameter logistic model. These results are applicable to assays where precision constraints preclude interpolating results from the flat portions of the calibration curve, and it is under these conditions that the moderate improvements in accuracy described here will have impact.},
author = {Herman, Rod a and Scherer, Peter N and Shan, Guomin},
doi = {10.1016/j.jim.2008.09.001},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Herman, Scherer, Shan - 2008 - Evaluation of logistic and polynomial models for fitting sandwich-ELISA calibration curves.pdf:pdf},
issn = {0022-1759},
journal = {Journal of immunological methods},
keywords = {Calibration,Calibration: standards,Enzyme-Linked Immunosorbent Assay,Enzyme-Linked Immunosorbent Assay: methods,Enzyme-Linked Immunosorbent Assay: standards,Models,Theoretical,calibration curve,elisa},
mendeley-tags = {calibration curve,elisa},
month = dec,
number = {2},
pages = {245----58},
pmid = {18822292},
publisher = {Elsevier B.V.},
title = {{Evaluation of logistic and polynomial models for fitting sandwich-ELISA calibration curves.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18822292},
volume = {339},
year = {2008}
}
@article{Feng2011,
abstract = {Immunoassays are primary diagnostic and research tools throughout the medical and life sciences. The common approach to the processing of immunoassay data involves estimation of the calibration curve followed by inversion of the calibration function to read off the concentration estimates. This approach, however, does not lend itself easily to acceptable estimation of confidence limits on the estimated concentrations. Such estimates must account for uncertainty in the calibration curve as well as uncertainty in the target measurement. Even point estimates can be problematic: because of the non-linearity of calibration curves and error heteroscedasticity, the neglect of components of measurement error can produce significant bias.},
annote = {
        From Duplicate 1 ( 
        
          A Bayesian approach for estimating calibration curves and unknown concentrations in immunoassays.
        
         - Feng, Feng; Sales, Ana Paula; Kepler, Thomas B )

        
        

        From Duplicate 2 ( 
        
        
          A Bayesian approach for estimating calibration curves and unknown concentrations in immunoassays.
        
        
         - Feng, Feng; Sales, Ana Paula; Kepler, Thomas B )

        
        

        

        

        

        

        From Duplicate 2 ( 
        
          A Bayesian approach for estimating calibration curves and unknown concentrations in immunoassays.
        
         - Feng, Feng; Sales, Ana Paula; Kepler, Thomas B )

        
        

        

        

      },
author = {Feng, Feng and Sales, Ana Paula and Kepler, Thomas B},
doi = {10.1093/bioinformatics/btq686},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Feng, Sales, Kepler - 2011 - A Bayesian approach for estimating calibration curves and unknown concentrations in immunoassays.pdf:pdf},
issn = {1367-4811},
journal = {Bioinformatics (Oxford, England)},
keywords = {Algorithms,Bayes Theorem,Calibration,Computational Biology,Computational Biology: methods,Computer Simulation,Evaluation Studies as Topic,Immunoassay,Immunoassay: methods,Markov Chains,Models,Monte Carlo Method,Software,Statistical,Uncertainty,bayesian,elisa},
mendeley-tags = {bayesian,elisa},
month = mar,
number = {5},
pages = {707--12},
pmid = {21149344},
title = {{A Bayesian approach for estimating calibration curves and unknown concentrations in immunoassays.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3465100\&tool=pmcentrez\&rendertype=abstract},
volume = {27},
year = {2011}
}
@article{Spiegelhalter1999,
author = {Spiegelhalter, David J and Myles, Jonathan P and Jones, David R and Abrams, Keith R},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Spiegelhalter\_1999.pdf:pdf},
journal = {British Medical Journal},
keywords = {bayesian},
mendeley-tags = {bayesian},
number = {August},
pages = {508--512},
title = {{An introduction to bayesian methods in health technology assessment}},
volume = {319},
year = {1999}
}
@incollection{Fox2002,
author = {Fox, John},
booktitle = {An R and S-PLUS Companion to Applied Regression},
chapter = {Appendix},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Fox2002\_appendix-nonlinear-regression.pdf:pdf},
number = {January},
pages = {1--5},
title = {{Nonlinear Regression and Nonlinear Least Squares}},
year = {2002}
}
@article{Vella2013,
abstract = {This paper describes a paper-based microfluidic device that measures two enzymatic markers of liver function (alkaline phosphatase ALP, and aspartate aminotransferase AST) and total serum protein. A device consists of four components: i) a top plastic sheet, ii) a filter membrane, iii) a patterned paper chip containing the reagents necessary for analysis, and iv) a bottom plastic sheet. The device performs both the sample preparation (separating blood plasma from erythrocytes) and the assays; it also enables both qualitative and quantitative analysis of data. The data obtained from the paper-microfluidic devices show standard deviations in calibration runs and “spiked” standards that are acceptable for routine clinical use. This device illustrates a type of test useable for a range of assays in resource-poor settings.},
author = {Vella, Sarah J and Beattie, Patrick D and Cademartiri, Rebecca and Laromaine, Anna and Martinez, Andres W and Phillips, Scott T and Mirica, Katherine A and Whitesides, George M},
doi = {10.1021/ac203434x.Measuring},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Vella\_2013.pdf:pdf},
journal = {Analytical chemistry},
keywords = {POC,elisa},
mendeley-tags = {POC,elisa},
number = {6},
pages = {2883--2891},
title = {{Paper Device Designed for Blood from a Fingerstick}},
volume = {84},
year = {2013}
}
@article{Deventer2011,
abstract = {BACKGROUND: Accurate measurement of free thyroxine (FT4) is important for diagnosing and managing thy- roid disorders. Most laboratories measure FT4 by di- rect analogue immunoassay methods. The validity of these methods have recently been questioned. The in- verse log-linear relationship between FT4 and thyroid- stimulating hormone (TSH) is well described and pro- vides a physiological rationale on which to base an evaluation of FT4 assays. METHODS: The study included 109 participants for whom FT4 measurement was requested by their clini- cian. Samples were selected for inclusion to reflect a wide spectrum of TSH and albumin results. FT4 and TSH were measured by use of the Siemens Immulite immunoassay (IA). FT4 was also measured by liquid chromatography–tandem mass spectrometry (LC-MS/ MS) (MS-FT4). RESULTS: The inverse log-linear correlation coefficient between TSH and FT4 was significantly better (P = 0.0001) for MS-FT4 (0.84, 95\% CI, 0.77–0.88) than for IA-FT4 (0.45, 95\% CI, 0.29–0.59). IA-FT4 showed a significant correlation with albumin (Spearman corre- lation coefficient 0.45, 95\% CI, 0.29–0.5, P?0.0001) and thyroxine-binding globulin (TBG) (Spearman correlation coefficient 0.23, 95\% CI, 0.05–0.41, P=0.02). In contrast,FT4 measurement byLC-MS/MSdid not show a significant correlation with albumin or TBG. CONCLUSIONS: The inverse log-linear relationship be- tween FT4 and TSH was significantly better for FT4 measured by LC-MS/MS than by IA. The MS-FT4 method therefore provides FT4 results that agree clinically with those obtained for TSH. Additionally, the significant correlation between IA-FT4 with albumin and TBG suggests that this FT4 method depends on binding protein concentrations and consequently does not accurately reflect FT4.},
author = {Deventer, Hendrick E Van and Mendu, Damodara R and Remaley, Alan T and Soldin, Steven J},
doi = {10.1373/clinchem.2010.154088},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Deventer et al. - 2011 - Inverse Log-Linear Relationship between Thyroid-Stimulating Hormone and Free Thyroxine Measured by Direct Analo.pdf:pdf},
journal = {Clininal Chemistry},
keywords = {calibration curve,elisa},
mendeley-tags = {calibration curve,elisa},
number = {1},
pages = {122----127},
title = {{Inverse Log-Linear Relationship between Thyroid-Stimulating Hormone and Free Thyroxine Measured by Direct Analog Immunoassay and Tandem Mass Spectrometry METHODS : RESULTS :}},
volume = {57},
year = {2011}
}
@article{Weir2005,
abstract = {Reliability, the consistency of a test or measurement, is frequently quantified in the movement sciences literature. A common metric is the intraclass correla- tion coefficient (ICC). In addition, the SEM, which can be cal- culated from the ICC, is also frequently reported in reliability studies. However, there are several versions of the ICC, and con- fusion exists in the movement sciences regarding which ICC to use. Further, the utility of the SEM is not fully appreciated. In this review, the basics of classic reliability theory are addressed in the context of choosing and interpreting an ICC. The primary distinction between ICC equations is argued to be one concern- ing the inclusion (equations 2,1 and 2,k) or exclusion (equations 3,1 and 3,k) of systematic error in the denominator of the ICC equation. Inferential tests of mean differences, which are per- formed in the process of deriving the necessary variance com- ponents for the calculation of ICC values, are useful to deter- mine if systematic error is present. If so, the measurement schedule should be modified (removing trials where learning and/or fatigue effects are present) to remove systematic error, and ICC equations that only consider random error may be safe- ly used. The use of ICC values is discussed in the context of estimating the effects of measurement error on sample size, sta- tistical power, and correlation attenuation. Finally, calculation and application of the SEM are discussed. It is shown how the SEM and its variants can be used to construct confidence inter- vals for individual scores and to determine the minimal differ- ence needed to be exhibited for one to be confident that a true change in performance of an individual has occurred.},
author = {Weir, Joseph P},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Weir\_2005.pdf:pdf},
journal = {Journal of Strength and Conditioning Research},
keywords = {SEM,consistency,error,intraclass correlation coefficien,precision,reproducibility},
number = {1},
pages = {231--240},
title = {{QUANTIFYING TEST-RETEST RELIABILITY USING THE INTRACLASS CORRELATION COEFFICIENT AND THE SEM}},
volume = {19},
year = {2005}
}
@techreport{Krouwer2006,
abstract = {Clinical and Laboratory Standards Institute document EP10-A3—Preliminary Evaluation of Quantitative Clinical Laboratory Measurement Procedures; Approved Guideline—Third Edition is intended to facilitate a limited, preliminary evaluation of the performance of a measurement procedure or device. Using the experimental design and data analysis procedure described, determination of whether a device has problems that require further evaluation or referral to the manufacturer can be done with a minimum expenditure of time and material. Included in Appendixes A and B are sample data sheets that should facilitate the analysis of the data. Appendix C contains a more sophisticated, powerful, statistical method for determining the possible causes of imprecision.},
author = {Krouwer, Jan S and Cembrowski, George S and Tholen, Daniel W},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/EP10A3\_sample.pdf:pdf},
number = {November},
title = {{Preliminary Evaluation of Quantitative Clinical Laboratory Measurement Procedures; Approved Guideline 3rd ed.}},
year = {2006}
}
@article{Kroll1999,
abstract = {Finding the linear reportable range is an important procedure for each method in clinical chemistry. One is often called upon to limit the reportable range in order to find the linear region. Limiting the reportable range by visual techniques is subjective, may introduce bias and is not programmable. Using Kroll and Emancipator's polynomial method for linearity, we compare the residuals of a test to determine whether eliminating a point from one end or the other of the data set worsens or improves the data sets' linearity. In an example of urinary cortisol, the root mean squares of the residuals improve by 2\% when the lowest point is removed, 39\% when the highest point is removed and 82\% when the two highest points are removed. The latter data set is the most linear.},
author = {Kroll, M H and Emancipator, K and Floering, D and Tholen, Daniel W},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Kroll\_1999.pdf:pdf},
issn = {0010-4825},
journal = {Computers in biology and medicine},
keywords = {Algorithms,Data Interpretation,Humans,Hydrocortisone,Hydrocortisone: urine,Linear Models,Mathematical Computing,Models,Nonlinear Dynamics,Reproducibility of Results,Software,Statistical,Theoretical},
month = sep,
number = {5},
pages = {289--301},
pmid = {10463796},
title = {{An algorithm for finding the linear region in a nonlinear data set}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/10463796},
volume = {29},
year = {1999}
}
@article{VandenBruel2007,
abstract = {OBJECTIVE: Before introducing a test in clinical practice, its characteristics and added value should be assessed. Diagnostic accuracy studies only are not sufficient; the test's impact on patient outcome ought to be assessed as well. To do this, we propose a stepwise evaluation of diagnostic tests. STUDY DESIGN AND SETTING: Theoretical-conceptual approach. RESULTS: First, the test's technical accuracy refers to the ability to produce usable information under standardized conditions. In a second step, the place of the new test in the clinical pathway is determined. Thirdly, the test's diagnostic accuracy is assessed, depending on its intended goal. The fourth step assesses the test's impact on the patient outcome. Depending on the place of the test in the clinical pathway, existing evidence can be used, or new evidence will be needed. At the final step, a cost-effectiveness analysis assesses the test's financial and societal consequences. CONCLUSION: Diagnostic tests evaluation should consider the technical accuracy, the test's place in the clinical pathway, its diagnostic accuracy, and its impact on patient outcome.},
author = {{Van den Bruel}, A and Cleemput, I and Aertgeerts, B and Ramaekers, D and Buntinx, F},
doi = {10.1016/j.jclinepi.2007.03.015},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/VandenBruel\_2007.pdf:pdf},
isbn = {3216337480},
issn = {0895-4356},
journal = {Journal of clinical epidemiology},
keywords = {Clinical Laboratory Techniques,Clinical Laboratory Techniques: economics,Clinical Laboratory Techniques: standards,Cost-Benefit Analysis,Cost-Benefit Analysis: economics,Diagnostic Errors,Diagnostic Errors: economics,Diagnostic Tests,Humans,Randomized Controlled Trials as Topic,Reproducibility of Results,Research Design,Routine,Routine: economics,Routine: standards,Sensitivity and Specificity,Treatment Outcome,agreement studies},
mendeley-tags = {agreement studies},
month = nov,
number = {11},
pages = {1116--22},
pmid = {17938052},
title = {{The evaluation of diagnostic tests: evidence on technical and diagnostic accuracy, impact on patient outcome and cost-effectiveness is needed.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17938052},
volume = {60},
year = {2007}
}
@book{Scott,
author = {Scott, Theresa A},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Scott.IntroToR.I.pdf:pdf},
institution = {Department of Biostatistics, Vanderbilt University},
title = {{An Introduction to the Fundamentals \& Functionality of the R Programming Language — Part I : An Overview}}
}
@incollection{Rattle2013a,
annote = {
        From Duplicate 1 ( 
        
          Lab-on-a-Chip, Micro- and Nanoscale Immunoassay Systems, and Microarrays
        
         - Rattle, Simon; Hofmann, Oliver; Price, Christopher P; Kricka, Larry J; Wild, David )

        
        

        

        

        From Duplicate 2 ( 
        
          Lab-on-a-Chip, Micro- and Nanoscale Immunoassay Systems, and Microarrays
        
         - Rattle, Simon; Hofmann, Oliver; Price, Christopher P; Kricka, Larry J; Wild, David )

        
        

        From Duplicate 2 ( 
        
        
          Lab-on-a-Chip, Micro- and Nanoscale Immunoassay Systems, and Microarrays
        
        
         - Rattle, Simon; Hofmann, Oliver; Price, Christopher P; Kricka, Larry J; Wild, David )

        
        

        From Duplicate 1 ( 
        
        
          Lab-on-a-Chip, Micro- and Nanoscale Immunoassay Systems, and Microarrays
        
        
         - Systems, Immunoassay; Rattle, Simon; Hofmann, Oliver; Price, Christopher P; Kricka, Larry J; Wild, David )

        
        
First reading: 18July2013
Context: background for work with Nandini

        
Read as general interest/background to understand implications of 'point-of-care (POC)', 'multiplex', 'lab-on-a-chip', etc.

        
Fabric chip is 'Lateral Flow Immunoassay'? I guess that fabric chips have different 'wetting' potential than 'injection molded plastics'?

        
How, if at all, do the usual error-prone steps differ in the 'microfluidic' environment?

        
The advantages of miniaturized analyzers (Table 1) do not seem to have implications for data analysis, but most of the disadvantages do. What additional information do we need? What is meant by 'Nonrepresentative sampling'?

        
What are the risks to sensitivity (due to tiny volumes) in the applications we're studying?

        

        From Duplicate 2 ( 
        
        
          CHAPTER 2.10 - Lab-on-a-Chip, Micro- and Nanoscale Immunoassay Systems, and Microarrays
        
        
         - Systems, Immunoassay; Rattle, Simon; Hofmann, Oliver; Price, Christopher P; Kricka, Larry J; Wild, David )

        
        

        

        

        
          
From Duplicate 3 ( 
        
        
          Lab-on-a-Chip, Micro- and Nanoscale Immunoassay Systems, and Microarrays
        
        
         - Systems, Immunoassay; Rattle, Simon; Hofmann, Oliver; Price, Christopher P; Kricka, Larry J; Wild, David )

        
        
First reading: 18July2013
Context: background for work with Nandini

        
Read as general interest/background to understand implications of 'point-of-care (POC)', 'multiplex', 'lab-on-a-chip', etc.

        
Fabric chip is 'Lateral Flow Immunoassay'?  I guess that fabric chips have different 'wetting' potential than 'injection molded plastics'?

        
How, if at all, do the usual error-prone steps differ in the 'microfluidic' environment?

        
The advantages of miniaturized analyzers (Table 1) do not seem to have implications for data analysis, but most of the disadvantages do.  What additional information do we need? What is meant by 'Nonrepresentative sampling'?

        
What are the risks to sensitivity (due to tiny volumes) in the applications we're studying?

        

        

        

        

        

      },
author = {Rattle, Simon and Hofmann, Oliver and Price, Christopher P and Kricka, Larry J and Wild, David},
booktitle = {The Immunoassay Handbook},
chapter = {2.10},
doi = {10.1016/B978-0-08-097037-0.00015-4},
edition = {Fourth Ed.},
editor = {Wild, David},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Rattle\_2013.pdf:pdf;:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Rattle et al. - 2013 - Lab-on-a-Chip, Micro- and Nanoscale Immunoassay Systems, and Microarrays.pdf:pdf},
isbn = {9780080970370},
pages = {175--202},
publisher = {Elsevier Ltd},
title = {{Lab-on-a-Chip, Micro- and Nanoscale Immunoassay Systems, and Microarrays}},
url = {http://dx.doi.org/10.1016/B978-0-08-097037-0.00015-4},
volume = {1},
year = {2013}
}
@article{Fu2010,
abstract = {Over the last decade, translational science has come into the focus of academic medicine, and significant intellectual and financial efforts have been made to initiate a multitude of bench-to-bedside projects. The quest for suitable biomarkers that will significantly change clinical practice has become one of the biggest challenges in translational medicine. Quantitative measurement of proteins is a critical step in biomarker discovery. Assessing a large number of potential protein biomarkers in a statistically significant number of samples and controls still constitutes a major technical hurdle. Multiplexed analysis offers significant advantages regarding time, reagent cost, sample requirements and the amount of data that can be generated. The two contemporary approaches in multiplexed and quantitative biomarker validation, antibody-based immunoassays and MS-based multiple (or selected) reaction monitoring, are based on different assay principles and instrument requirements. Both approaches have their own advantages and disadvantages and therefore have complementary roles in the multi-staged biomarker verification and validation process. In this review, we discuss quantitative immunoassay and multiple reaction monitoring/selected reaction monitoring assay principles and development. We also discuss choosing an appropriate platform, judging the performance of assays, obtaining reliable, quantitative results for translational research and clinical applications in the biomarker field.},
author = {Fu, Qin and Schoenhoff, Florian S and Savage, William J and Zhang, Pingbo and {Van Eyk}, Jennifer E},
doi = {10.1002/prca.200900217},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Fu\_2010.pdf:pdf},
issn = {1862-8354},
journal = {Proteomics. Clinical applications},
keywords = {Antibodies,Antibodies: immunology,Biological Markers,Biological Markers: analysis,Biological Markers: metabolism,Humans,Immunoassay,Immunoassay: methods,Immunoassay: standards,Mass Spectrometry,Reference Standards,Translational Medical Research,Translational Medical Research: methods,Translational Medical Research: trends},
month = mar,
number = {3},
pages = {271--84},
pmid = {21137048},
title = {{Multiplex assays for biomarker research and clinical application: translational science coming of age.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21137048},
volume = {4},
year = {2010}
}
@misc{Bernardo2001,
author = {Bernardo, Jos\'{e} M},
booktitle = {Encyclopedia of Life Support Systems},
editor = {UNESCO},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Bernardo\_2001.pdf:pdf},
keywords = {Amount of Information,Axiomatics,Bayes Estimator,Bayes Factor,Bayes Theorem,Credible Region,Decision Theory,Degree of Belief,Exchangeability,Foundations of Statistics,Hierarchical Models,Hypothesis Testing,Interval Estimation,Intrinsic Discrepancy,Likelihood Principle,Lindley’s Paradox,Logarithmic Divergence,Maximum Entropy,Model Choice,Model Criticism,Noninformative Priors,Nuisance Parameters,Objectivity,Point Estimation,Posterior Distribution,Predictive Distribution,Prior Choice,Prior Distribution,Probability Assessment,Probability Model,Probability Theory,Reference Distributions,Representation Theorem,Scientific Reporting,Sensitivity Analysis,Steins’s Paradox,Sufficiency,bayesian,education},
mendeley-tags = {bayesian,education},
publisher = {UNESCO},
title = {{Bayesian statistics}},
year = {2001}
}
@incollection{Linnet2012,
author = {Linnet, Kristian and Boyd, James C},
booktitle = {Tietz textbook of clinical chemistry and molecular diagnostics},
chapter = {2},
edition = {5th},
editor = {Burtis, Carl A and Ashwood, Edward R and Bruns, David E},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Linnet, Boyd - 2012 - Selection and Analytical Evaluation of Methods—With Statistical Techniques.pdf:pdf},
publisher = {Elsevier Saunders},
title = {{Selection and Analytical Evaluation of Methods—With Statistical Techniques}},
year = {2012}
}
@article{Pollock2012a,
abstract = {In developed nations,monitoring for drug-induced liver injury through serial measurements of serum transaminases [aspartate aminotransferase (AST) and alanine aminotransferase (ALT)] in at-risk individuals is the standard of care. Despite the need, monitoring for drug-related hepatotoxicity in resource-limited settings is often limited by expense and logistics, even for patients at highest risk. This article describes the development and clinical testing of a paper-based, multiplexed microfluidic assay designed for rapid, semiquantitative measurement of AST and ALT in a fingerstick specimen. Using 223 clinical specimens obtained by venipuncture and 10 fingerstick specimens from healthy volunteers, we have shown that our assay can, in 15 min, provide visual measurements of AST and ALT in whole blood or serum, which allow the user to place those values into one of three readout “bins” [<3× upper limit of normal (ULN), 3 to 5× ULN, and >5× ULN, corresponding to tuberculosis/HIV treatment guidelines] with >90\% accuracy. These data suggest that the ultimate point-of-care fingerstick device will have high impact on patient care in low-resource settings. INTRODUCTION},
author = {Pollock, Nira R and Rolland, Jason P and Kumar, Shailendra and Beattie, Patrick D and Jain, Sidhartha and Noubary, Farzad and Wong, Vicki L and Pohlmann, Rebecca A and Ryan, Una S and Whitesides, George M},
doi = {10.1126/scitranslmed.3003981},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Pollock\_2012.pdf:pdf;:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pollock et al. - 2012 - Supplementary Materials for A Paper-Based Multiplexed Transaminase Test for Low-Cost , Point-of-Care Liver Funct.pdf:pdf},
journal = {Science Translational Medicine},
keywords = {POC,calibration curve,elisa},
mendeley-tags = {POC,calibration curve,elisa},
title = {{A Paper-Based Multiplexed Transaminase Test for Low-Cost , Point-of-Care Liver Function Testing}},
volume = {129},
year = {2012}
}
@article{Waerner2012,
abstract = {In the last 10 years, the area of ELISA and protein-chip technology has developed and enthusiastically applied to an enormous variety of biological questions. However, the degree of stringency required in data analysis appears to have been underestimated. As a result, there are numerous published findings that are of questionable quality, requiring further confirmation and/or validation. In the course of feasibility and validation studies a number of key issues in research, development and clinical trial studies must be outlined, including those associated with laboratory design, analytical validation strategies, analytical completeness and data managements. The scope of the following book chapter should provide assistance for defining key parameters in assay evaluation, validation in research and clinical trial projects.},
address = {Dordrecht},
author = {Waerner, Thomas and Urthaler, Jochen and Krapfenbauer, Kurt},
doi = {10.1007/978-94-007-4602-2},
editor = {Costigliola, Vincenzo},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Waerner, Urthaler, Krapfenbauer - 2012 - Characterization and Validation of Biomarkers by Immunoassays Quality Requirements, Physical St.pdf:pdf},
isbn = {978-94-007-4601-5},
journal = {Advances in Predictive, Preventive and Personalised Medicine},
keywords = {ELISA,Experimental design,Minimal set of quality guidelines,Multiplex protein chip assay,Quality Requirements,Validation strategy to determine the predictive va},
pages = {377--396},
publisher = {Springer Netherlands},
series = {Advances in Predictive, Preventive and Personalised Medicine},
title = {{Characterization and Validation of Biomarkers by Immunoassays: Quality Requirements, Physical Standards and Data Management in Predictive Medicine}},
url = {http://link.springer.com/10.1007/978-94-007-4602-2},
volume = {1},
year = {2012}
}
@article{Boulanger2007,
author = {Boulanger, Bruno and Dew\'{e}, Walth\`{e}re and Gilbert, Aur\'{e}lie and Govaerts, Bernadette and Maumy-Bertrand, Myriam},
doi = {10.1016/j.chemolab.2006.06.008},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Boulanger et al. - 2007 - Risk management for analytical methods based on the total error concept Conciliating the objectives of the pre.pdf:pdf},
issn = {01697439},
journal = {Chemometrics and Intelligent Laboratory Systems},
month = apr,
number = {2},
pages = {198--207},
title = {{Risk management for analytical methods based on the total error concept: Conciliating the objectives of the pre-study and in-study validation phases}},
url = {http://linkinghub.elsevier.com/retrieve/pii/S0169743906001419},
volume = {86},
year = {2007}
}
@article{Belanger1996,
abstract = {Often with data from immunoassays, the concentration-response relationship is nonlinear and intra-assay response variance is heterogeneous. Estimation of the standard curve is usually based on a nonlinear heteroscedastic regression model for concentration-response, where variance is modeled as a function of mean response and additional variance parameters. This paper discusses calibration inference for immunoassay data which exhibit this nonlinear heteroscedastic mean-variance relationship. An assessment of the effect of variance function estimation in three types of approximate large-sample confidence intervals for unknown concentrations is given by theoretical and empirical investigation and application to two examples. A major finding is that the accuracy of such calibration intervals depends critically on the nature of response variance and the quality with which variance parameters are estimated.},
author = {Belanger, B a and Davidian, M and Giltinan, D M},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Belanger\_1996.pdf:pdf},
issn = {0006-341X},
journal = {Biometrics},
keywords = {Algorithms,Analysis of Variance,Animals,Biometry,Biometry: methods,Computer Simulation,Data Interpretation, Statistical,Enzyme-Linked Immunosorbent Assay,Enzyme-Linked Immunosorbent Assay: standards,Enzyme-Linked Immunosorbent Assay: statistics \& nu,Humans,Immunoassay,Immunoassay: standards,Immunoassay: statistics \& numerical data,Monte Carlo Method,Nonlinear Dynamics,Pharmaceutical Preparations,Pharmaceutical Preparations: analysis,Pharmaceutical Preparations: standards,Radioimmunoassay,Radioimmunoassay: standards,Radioimmunoassay: statistics \& numerical data,Recombinant Proteins,Recombinant Proteins: analysis,Reference Standards,Relaxin,Relaxin: analysis,Swine},
month = mar,
number = {1},
pages = {158--75},
pmid = {8934590},
title = {{The effect of variance function estimation on nonlinear calibration inference in immunoassay data.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/8934590},
volume = {52},
year = {1996}
}
@article{Wilkinson2008,
author = {Wilkinson, Leland},
doi = {10.1198/004017008000000460},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Wilkinson\_2008.pdf:pdf},
issn = {0040-1706},
journal = {Technometrics},
keywords = {07,1072954 registered office,14,30 april 2014,at,by,england and wales registered,francis,informa ltd registered in,mcgill university library,mortimer,number,on,publisher,s article was downloaded,taylor},
month = nov,
number = {4},
pages = {418--435},
title = {{The Future of Statistical Computing}},
url = {http://www.tandfonline.com/doi/abs/10.1198/004017008000000460},
volume = {50},
year = {2008}
}
@techreport{Marti2004,
abstract = {Quantitative fluorescence calibration (QFC) is an empiric system to calibrate fluorescence intensity in a way that preserves stoichiometry between the concentration of fluorochrome in solutions and the equivalent molar quantity of fluorochrome on stained measurands such as cells, gels, microspheres, and microdots. This guideline describes the basic principles, reference materials, and laboratory procedures upon which QFC is based. This guideline is intended for use with reference materials and procedures developed under the National Institute of Standards and Technology (NIST) Fluorescence Intensity Standards program. While the general principles of QFC apply to any fluorescence measurement, this guideline specifically addresses analysis of cells and microspheres by flow cytometry, including cellular immunophenotyping and suspension array technology. The current and emerging uses of these laboratory methods will have an increasing impact on public health and primary care, from large-scale screening of populations to the individual profiling of each patient’s disease.},
annote = {
        From Duplicate 1 ( 
        
        
          Fluorescence Calibration and Quantitative Measurement of Fluorescence Intensity ; Approved Guideline I/LA24-A
        
        
         - Marti, Gerald E; Vogt, Robert F Jr; Gaigalas, Adolfas K; Hixson, Craig S; Hoffman, Robert A; Lenkei, Rodica; Magruder, Louise E; Purvis, Norman B Jr; Schartz, Abe; Shapiro, Howard M; Waggoner, Alan )

        
        
These guidelines expand upon calibration and standardization of substances and instrumentation used in fluorescense ligand-binding assays. Some useful info, but I think not directly relevant to Achira fluorescence tests.

        
- Is 'quantitative fluorescence calibration' (QFC) relevant to our work with Achira?
- Are Achira hydrogels equivalent to microspheres?
- Do Achira hydrogels have 'predetermined binding capacities'?
- To which of the systems listed in section 6.2 is Achira's chip most closely related?

        

        From Duplicate 2 ( 
        
        
          Fluorescence Calibration and Quantitative Measurement of Fluorescence Intensity ; Approved Guideline I/LA24-A
        
        
         - Marti, Gerald E; Vogt, Robert F Jr; Gaigalas, Adolfas K; Hixson, Craig S; Hoffman, Robert A; Lenkei, Rodica; Magruder, Louise E; Purvis, Norman B Jr; Schartz, Abe; Shapiro, Howard M; Waggoner, Alan )

        
        

        From Duplicate 1 ( 
        
        
          Fluorescence Calibration and Quantitative Measurement of Fluorescence Intensity ; Approved Guideline I/LA24-A
        
        
         - Marti, Gerald E; Vogt, Robert F Jr; Gaigalas, Adolfas K; Hixson, Craig S; Hoffman, Robert A; Lenkei, Rodica; Magruder, Louise E; Purvis, Norman B Jr; Schartz, Abe; Shapiro, Howard M; Waggoner, Alan )

        
        
These guidelines expand upon calibration and standardization of substances and instrumentation used in fluorescense ligand-binding assays.  Some useful info, but I think not directly relevant to Achira fluorescence tests.

        
- Is 'quantitative fluorescence calibration' (QFC) relevant to our work with Achira?
- Are Achira hydrogels equivalent to microspheres?
- Do Achira hydrogels have 'predetermined binding capacities'?
- To which of the systems listed in section 6.2 is Achira's chip most closely related?

        

        From Duplicate 2 ( 
        
        
          Fluorescence Calibration and Quantitative Measurement of Fluorescence Intensity ; Approved Guideline I/LA24-A
        
        
         - Marti, Gerald E; Vogt, Robert F Jr; Gaigalas, Adolfas K; Hixson, Craig S; Hoffman, Robert A; Lenkei, Rodica; Magruder, Louise E; Purvis, Norman B Jr; Schartz, Abe; Shapiro, Howard M; Waggoner, Alan )

        
        

        From Duplicate 1 ( 
        
        
          Fluorescence Calibration and Quantitative Measurement of Fluorescence Intensity ; Approved Guideline I/LA24-A
        
        
         - Marti, Gerald E; Vogt, Robert F Jr; Gaigalas, Adolfas K; Hixson, Craig S; Hoffman, Robert A; Lenkei, Rodica; Magruder, Louise E; Purvis, Norman B Jr; Schartz, Abe; Shapiro, Howard M; Waggoner, Alan )

        
        
These guidelines expand upon calibration and standardization of substances and instrumentation used in fluorescense ligand-binding assays. Some useful info, but I think not directly relevant to Achira fluorescence tests.

        
- Is 'quantitative fluorescence calibration' (QFC) relevant to our work with Achira?
- Are Achira hydrogels equivalent to microspheres?
- Do Achira hydrogels have 'predetermined binding capacities'?
- To which of the systems listed in section 6.2 is Achira's chip most closely related?

        

        From Duplicate 2 ( 
        
        
          Fluorescence Calibration and Quantitative Measurement of Fluorescence Intensity ; Approved Guideline I/LA24-A
        
        
         - Marti, Gerald E; Vogt, Robert F Jr; Gaigalas, Adolfas K; Hixson, Craig S; Hoffman, Robert A; Lenkei, Rodica; Magruder, Louise E; Purvis, Norman B Jr; Schartz, Abe; Shapiro, Howard M; Waggoner, Alan )

        
        
These guidelines expand upon calibration and standardization of substances and instrumentation used in fluorescense ligand-binding assays.  Some useful info, but I think not directly relevant to Achira fluorescence tests.

        
- Is 'quantitative fluorescence calibration' (QFC) relevant to our work with Achira?
- Are Achira hydrogels equivalent to microspheres?
- Do Achira hydrogels have 'predetermined binding capacities'?
- To which of the systems listed in section 6.2 is Achira's chip most closely related?

        

        

        

        

        

      },
author = {Marti, Gerald E and Vogt, Robert F Jr and Gaigalas, Adolfas K and Hixson, Craig S and Hoffman, Robert A and Lenkei, Rodica and Magruder, Louise E and Purvis, Norman B Jr and Schartz, Abe and Shapiro, Howard M and Waggoner, Alan},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/ILA24-A.pdf:pdf},
institution = {Clinical and Laboratory Standards Institute},
isbn = {1562385437},
keywords = {clsi},
mendeley-tags = {clsi},
number = {26},
title = {{Fluorescence Calibration and Quantitative Measurement of Fluorescence Intensity ; Approved Guideline I/LA24-A}},
volume = {24},
year = {2004}
}
@article{Box1992,
abstract = {Genichi Taguchi has emphasized the use of designed experiments in several novel and important applications. In this paper we focus on the use of statistical experimental designs in designing products to be robust to environmental conditions. The engineering concept of robust product design is very important because it is frequently impossible or prohibitively expensive to control or eliminate variation resulting from environmental conditions. Robust product design enables the experimenter to discover how to modify the design of the product to minimize the eflect due to variation from environmental sources. In experiments of this kind, Taguchi's total experimental arrangement consists of a cross-product of two experimental designs: an inner array containing th8 design factors and an outer array containing the environmental factors. Except in situations where both these arrays are small, this arrangement may involve a prohibitively large amount of experimental work. One of the objectives of this paper is to show how this amount of work can be reduced. In this paper we investigate the applicability of split-plot designs for this particular experimental situation. Consider- ation of the efficiency of split-plot designs and an examination of several variants of split- plot designs indicates that experiments conducted in a split-plot mode can be of tremendous value in robust product design since they not only enable the contrasts of interest to be estimated efficiently but also the experiments can be considerably easier to conduct than the designs proposed by Taguchi.},
author = {Box, George and Jones, Stephen},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Box\_1992.pdf:pdf},
isbn = {0266476920000},
journal = {Journal of Applied Statistics},
number = {1},
pages = {3--26},
title = {{Split-plot designs for robust product experimentation}},
volume = {19},
year = {1992}
}
@book{Aragon2011,
author = {Aragon, Tomas J},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Aragon\_2011.pdf:pdf},
title = {{Applied Epidemiology Using R}},
year = {2011}
}
@article{Kass2012,
abstract = {Statistics has moved beyond the frequentist-Bayesian controversies of the past. Where does this leave our ability to interpret results? I suggest that a philosophy compatible with statistical practice, labelled here statistical pragmatism, serves as a foundation for inference. Statistical pragmatism is inclusive and emphasizes the assumptions that connect statistical models with observed data. I argue that introductory courses often mis-characterize the process of statistical inference and I propose an alternative “big picture” depiction.},
author = {Kass, Robert E},
doi = {10.1214/10-STS337.Statistical},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Kass - 2012 - Statistical Inference the Big Picture.pdf:pdf},
journal = {Statistical Science},
keywords = {bayesian,confidence,frequentist,philosophy of science,statistical,statistical education,statistical pragmatism},
mendeley-tags = {philosophy of science},
number = {February},
pages = {1--9},
title = {{Statistical Inference: the Big Picture}},
volume = {26},
year = {2012}
}
@article{Biesheuvel2006,
abstract = {The ultimate goal of medical care, including diagnostic testing, is to improve patient outcome. Accordingly, it has been advocated widely that when establishing a test's diagnostic accuracy, the impact of the test on patient outcome subsequently must be quantified. When studying patient outcome in medical research, the use of randomized comparisons comes into perspective. In our view, randomized studies often are not necessary to validly estimate the effect of the diagnostic test on patient outcome. Results of cross-sectional diagnostic studies, combined with results from therapeutic studies, often will suffice.},
author = {Biesheuvel, Cornelis J and Grobbee, Diederick E and Moons, Karel G M},
doi = {10.1016/j.annepidem.2005.10.004},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Moons\_2006.pdf:pdf},
issn = {1047-2797},
journal = {Annals of epidemiology},
keywords = {Cross-Sectional Studies,Diagnosis,Humans,Random Allocation,Research Design},
month = jul,
number = {7},
pages = {540--4},
pmid = {16386925},
title = {{Distraction from randomization in diagnostic research.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16386925},
volume = {16},
year = {2006}
}
@article{Gottschalk2005b,
abstract = {A method is described here that uses a modified Monte-Carlo method to provide an improved estimate of the confidence bounds of concentration estimates. This method accommodates even strongly nonlinear curve models, such as the five parameter logistic model, in contrast to the common but often poor approach of linearizing the regression problem and using linear theory to obtain the confidence bounds. The method uses an interpolation technique to reduce artifacts in the precision profile due to small simulation sample sizes and proximity to horizontal asymptotes in the curve model. The paper also describes how to define and calculate the minimum and maximum acceptable concentrations of dose-response curves by locating the concentrations where the size of the error, defined in terms of the size of the concentration confidence interval, exceeds the threshold of acceptability determined for the application.},
annote = {
        From Duplicate 1 ( 
        
          Determining the error of dose estimates and minimum and maximum acceptable concentrations from assays with nonlinear dose-response curves
        
         - Gottschalk, Paul G; Dunn, John R )

        
        

        

        

      },
author = {Gottschalk, Paul G and Dunn, John R},
doi = {10.1016/j.cmpb.2005.08.003},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Gottschalk, Dunn - 2005 - Determining the error of dose estimates and minimum and maximum acceptable concentrations from assays with non.pdf:pdf},
issn = {0169-2607},
journal = {Computer methods and programs in biomedicine},
keywords = {Algorithms,Biological,Computer Simulation,Confidence Intervals,Data Interpretation,Dose-Response Relationship,Drug,Immunoassay,Immunoassay: methods,Maximum Allowable Concentration,Models,Monte Carlo Method,Nonlinear Dynamics,Pharmaceutical Preparations,Reproducibility of Results,Sample Size,Sensitivity and Specificity,Statistical,calibration curve},
mendeley-tags = {calibration curve},
month = dec,
number = {3},
pages = {204--15},
pmid = {16256244},
title = {{Determining the error of dose estimates and minimum and maximum acceptable concentrations from assays with nonlinear dose-response curves}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16256244 http://www.sciencedirect.com/science/article/pii/S0169260705001689},
volume = {80},
year = {2005}
}
@article{Lunn2009,
abstract = {BUGS is a software package for Bayesian inference using Gibbs sampling. The software has been instru- mental in raising awareness of Bayesian modelling among both academic and commercial communities internationally, and has enjoyed considerable success over its 20-year life span. Despite this, the software has a number of shortcomings and a principal aim of this paper is to provide a balanced critical appraisal, in particular highlighting how various ideas have led to unprecedented flexibility while at the same time producing negative side effects. We also present a historical overview of the BUGS project and some future perspectives. Copyright},
author = {Lunn, David and Spiegelhalter, David J and Thomas, Andrew and Best, Nicky},
doi = {10.1002/sim},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lunn et al. - 2009 - The BUGS project Evolution, critique and future directions.pdf:pdf},
journal = {Statistics in Medicine},
keywords = {bayesian,bayesian modelling,bugs,computing,graphical models,openbugs,winbugs},
mendeley-tags = {bayesian,computing},
number = {June},
pages = {3049--3067},
title = {{The BUGS project : Evolution, critique and future directions}},
volume = {28},
year = {2009}
}
@article{Shoukri2004,
abstract = {The reliability of continuous or binary outcome measures is usually assessed by estimation of the intraclass correlation coefficient (ICC). A crucial step for this purpose is the determination of the required sample size. In this review, we discuss the contributions made in this regard and derive the optimal allocation for the number of subjects k and the number of repeated measurements n that minimize the variance of the estimated ICC. Cost constraints are discussed for both normally and non-normally distributed responses, with emphasis on the case of dichotomous assessments. Tables showing optimal choices of k and n are given along with the guidelines for the efficient design of reliability studies.},
annote = {
        From Duplicate 2 ( 
        
          Sample size requirements for the design of reliability study: review and new results
        
         - Shoukri, Mohamed M; Asyali, M H; Donner, Allan )

        
        

        

        

      },
author = {Shoukri, Mohamed M and Asyali, M H and Donner, Allan},
doi = {10.1191/0962280204sm365ra},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Shoukri\_2004.pdf:pdf},
journal = {Statistical Methods in Medical Research},
pages = {251----271},
title = {{Sample size requirements for the design of reliability study: review and new results}},
volume = {13},
year = {2004}
}
@article{Bastarache2011,
abstract = {Background: Multiplex immunoassays offer many advantages over singleplex assays for the analysis of multiple analytes in a single sample. We sought to validate a specific multiplex cytokine immunoassay (Human 9-plex cytokine array on the Searchlight® platform by Thermoscientific) prior to use in a large clinical study. Methods: We compared spike and recovery of recombinant proteins on the Searchlight® platform to singleplex immunoassays purchased from R\&D Systems, measured identical patient samples on the two different platforms, and measured identical patient samples on different days to measure intra- and inter-assay variability. Results: Assays using the Searchlight® platform had inefficient recovery of spiked recombinant proteins compared to R\&D Systems singleplex assays. Assaying identical patients samples on different days on the Searchlight platform had acceptable intra-assay variability (intra-assay coefficient of variation (CV\%) range for all analytes of 9.1–13.7) but unacceptably high inter-assay variability (CV\% range for all analytes 16.7–119.3) suggesting plate-to plate variability. Similar assays for individual cytokinesontheR\&Dplatformhadanintra-assayCV\%range of 1.6–6.4andan inter-assayCV\%rangeof3.8–7.1.Somedeficiencies in Searchlight®assayperformancemaybedue to irregularities in spotting of capture antibodies duringmanufacturing. Conclusions:Weconclude that the Searchlight®multiplex immunoassay platformwould require extensive additional assay optimization prior to widespread clinical research use.},
author = {Bastarache, Julie A and Koyama, Tatsuki and Wickersham, Nancy E and Mitchell, Daphne B and Mernaugh, Ray L and Ware, Lorraine B},
doi = {10.1016/j.jim.2011.01.005},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bastarache et al. - 2011 - Accuracy and reproducibility of a multiplex immunoassay platform A validation study.pdf:pdf},
issn = {0022-1759},
journal = {Journal of Immunological Methods},
keywords = {Acute lung injury,Acute respiratory distress syndrome,Biomarkers,Enzyme linked immunosorbent assay,elisa},
mendeley-tags = {elisa},
number = {1-2},
pages = {33--39},
publisher = {Elsevier B.V.},
title = {{Accuracy and reproducibility of a multiplex immunoassay platform: A validation study}},
url = {http://dx.doi.org/10.1016/j.jim.2011.01.005},
volume = {367},
year = {2011}
}
@article{Pollock2012,
author = {Pollock, Nira R and Rolland, Jason P and Kumar, Shailendra and Beattie, Patrick D and Jain, Sidhartha and Noubary, Farzad and Wong, Vicki L and Pohlmann, Rebecca A and Ryan, Una S and Whitesides, George M},
doi = {10.1126/scitranslmed.3003981},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pollock et al. - 2012 - Supplementary Materials for A Paper-Based Multiplexed Transaminase Test for Low-Cost , Point-of-Care Liver Funct.pdf:pdf},
journal = {Science Translational Medicine},
title = {{Supplementary Materials for A Paper-Based Multiplexed Transaminase Test for Low-Cost , Point-of-Care Liver Function Testing}},
volume = {4},
year = {2012}
}
@incollection{He2013,
author = {He, Jianwen},
booktitle = {The Immunoassay Handbook},
chapter = {5.1},
doi = {10.1016/B978-0-08-097037-0.00071-3},
edition = {Fourth Ed},
editor = {Wild, David},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/He\_2013.pdf:pdf},
isbn = {9780080970370},
pages = {381--393},
publisher = {Elsevier Ltd},
title = {{Practical Guide to ELISA Development}},
url = {http://dx.doi.org/10.1016/B978-0-08-097037-0.00071-3},
year = {2013}
}
@article{Das2011,
abstract = {Method validation is pursued as the first step in establishing Lean-Total Quality Management in a new clinical laboratory, in order to eliminate error in test results. Validation of all the new tests were done (with particular reference to alkaline phosphatase) by verifying reference intervals, analytical accuracy and precision, inter-assay and intra-assay variations, analytical sensitivity, limit of detection, linearity and reportable range, i.e. (i) Analytical measurement range (AMR) and (ii) Clinically reportable range (CRR). Our obtained reference range was within that of the manufacturer's and showed high degree of analytical accuracy between two laboratories (r(2) = 0.99). Precision was comparable with the manufacturer's claim with inter-assay variation CV 1.04\% and intra-assay variation CV 1.54\%. Lowest limit of detection was 1.0324 ± 0.007 with CV 0.34\%. AMR was also verified with CV 1.26 and 0.69\%, for level 1 and level 2 control sera, respectively. The assay was linear with different dilutions. Lean concept was also verified with high recovery percentage. Validation ensures that accurate and precise results are reported in a clinically relevant turn around time.},
author = {Das, Barnali},
doi = {10.1007/s12291-011-0110-x},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Das - 2011 - Validation Protocol First Step of a Lean-Total Quality Management Principle in a New Laboratory Set-up in a Tertiary Care H.pdf:pdf},
isbn = {1229101101},
issn = {0974-0422},
journal = {Indian journal of clinical biochemistry : IJCB},
keywords = {alkaline phosphatase \'{a} lean,concept,management \'{a},validation \'{a} total quality},
month = jul,
number = {3},
pages = {235--243},
pmid = {22754186},
title = {{Validation Protocol: First Step of a Lean-Total Quality Management Principle in a New Laboratory Set-up in a Tertiary Care Hospital in India.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3162948\&tool=pmcentrez\&rendertype=abstract},
volume = {26},
year = {2011}
}
@book{Gelman2007,
address = {New York, NY},
annote = {Have Kindle edition},
author = {Gelman, Andrew and Hill, Jennifer},
publisher = {Cambridge University Press},
title = {{Data Analysis Using Regression and Multilevel/Hierarchical Models}},
year = {2007}
}
@article{Desilva2003,
abstract = {Purpose. With this publication a subcommittee of the AAPS Ligand Binding Assay Bioanalytical Focus Group (LBABFG) makes recom- mendations for the development, validation, and implementation of ligand binding assays (LBAs) that are intended to support pharma- cokinetic and toxicokinetic assessments of macromolecules. Methods. This subcommittee was comprised of 10 members repre- senting Pharmaceutical, Biotechnology, and the contract research or- ganization industries from the United States, Canada, and Europe. Each section of this consensus document addresses a specific analyti- cal performance characteristic or aspect for validation of a LBA. Within each section the topics are organized by an assay’s life cycle, the development phase, pre-study validation, and in-study validation. Because unique issues often accompany bioanalytical assays for mac- romolecules, this document should be viewed as a guide for designing and conducting the validation of ligand binding assays. Results. Values of ±20\% (25\% at the lower limit of quantification [LLOQ]) are recommended as default acceptance criteria for accu- racy (\% relative error [RE], mean bias) and interbatch precision (\%coefficient of variation [CV]). In addition, we propose as second- ary criteria for method acceptance that the sum of the interbatch precision (\%CV) and the absolute value of the mean bias (\%RE) be less than or equal to 30\%. This added criterion is recommended to help ensure that in-study runs of test samples will meet the proposed run acceptance criteria of 4-6-30. Exceptions to the proposed process and acceptance criteria are appropriate when accompanied by a sound scientific rationale. Conclusions. In this consensus document, we attempt to make rec- ommendations that are based on bioanalytical best practices and sta- tistical thinking for development and validation of LBAs.},
author = {Desilva, Binodh and Smith, Wendell and Weiner, Russell and Kelley, Marian and Smolec, Jomarie and Lee, Ben and Khan, Masood and Tacey, Richard and Hill, Howard and Celniker, Abbie},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Desilva\_2003.pdf:pdf},
journal = {Pharmaceutical Research},
keywords = {best practices,bioanalytical assay,biological matrices,consensus document,elisa,immunoassay},
mendeley-tags = {elisa},
number = {11},
pages = {1885----1900},
title = {{Recommendations for the Bioanalytical Method Validation of Ligand-binding Assays to Support Pharmacokinetic Assessments of Macromolecules}},
volume = {20},
year = {2003}
}
@article{Ritz2010,
abstract = {This study reviews dose-response models that are used in ecotoxicology. The focus lies on clarification of differences and similarities between models, and as a side effect, their different guises in ecotoxicology are unravelled. A look at frequently used dose-response models reveals major discrepancies, among other things in naming conventions. Therefore, there is a need for a unified view on dose-response modeling in order to improve the understanding of it and to facilitate communication and comparison of findings across studies, thus realizing its full potential. This study attempts to establish a general framework that encompasses most dose-response models that are of interest to ecotoxicologists in practice. The framework includes commonly used models such as the log-logistic and Weibull models, but also features entire suites of models as found in various guidance documents. An outline on how the proposed framework can be implemented in statistical software systems is also provided.},
author = {Ritz, Christian},
doi = {10.1002/etc.7},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Ritz\_2010.pdf:pdf},
issn = {0730-7268},
journal = {Environmental toxicology and chemistry / SETAC},
keywords = {Dose-Response Relationship, Drug,Ecotoxicology,Logistic Models,Models, Statistical,Software},
month = jan,
number = {1},
pages = {220--9},
pmid = {20821438},
title = {{Toward a unified approach to dose-response modeling in ecotoxicology.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/20821438},
volume = {29},
year = {2010}
}
@inproceedings{Allinson2009,
address = {London},
author = {Allinson, John L},
booktitle = {Cancer Reserach UK},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/JohnLAllinsonPresentation.pdf:pdf},
title = {{Fit-for-Purpose Biomarker Assay Development and Method Validation}},
year = {2009}
}
@article{Noble2008,
abstract = {BACKGROUND: Immunoassays allow the specific detection and quantitation of biological molecules in complex samples at physiologically relevant concentrations. However, there are concerns over the comparability of such techniques when the same assay is performed by different operators or laboratories. An international intercomparison study was performed to assess the uncertainty involved in the estimation of a protein cytokine concentration using a fluorescent ELISA. METHODS: The intercomparison study method was based on a non-competitive sandwich immunoassay with an enhancement step to generate a fluorescent readout. The intercomparison was performed in two phases, with the uncertainty of the instrument determined separately from that of the assay. The 11 laboratories participating in the study represented national metrology institutes or nominated expert laboratories. RESULTS: Participants were asked to determine an undisclosed concentration of interferon using a supplied standard. The mean participant estimate and experimental standard deviation of the mean was 3.54+/-0.22 mg/L, with the spread of data ranging around +/-35\% of the mean. The quantitation range of the ELISA and of participants' instruments displayed large variation that contributed to the overall uncertainty. CONCLUSIONS: Identified sources of uncertainty within the ELISA methodology included pipetting, data fitting, model selection and instrument/plate variation.},
author = {Noble, James E and Wang, Lili and Cerasoli, Eleonora and Knight, Alex E and Porter, Robert and Gray, Elaine and Howe, Chris and Hannes, Elisabeth and Corbisier, Philippe and Wang, Jing and Wu, Liqing and Altieri, Ilaria and Patriarca, Marina and Hoffman, Angelika and Resch-Genger, Ute and Ebert, Bernd and Voigt, Jan and Shigeri, Yasushi and Vonsky, Maxim S and Konopelko, Leonid and Gaigalas, Adolfas K and Bailey, Marc J},
doi = {10.1515/CCLM.2008.182},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Noble et al. - 2008 - An international comparability study to determine the sources of uncertainty associated with a non-competitive san.pdf:pdf},
isbn = {4420894369},
issn = {1434-6621},
journal = {Clinical chemistry and laboratory medicine},
keywords = {Enzyme-Linked Immunosorbent Assay,Enzyme-Linked Immunosorbent Assay: methods,Humans,Interferon-alpha,Interferon-alpha: analysis,Recombinant Proteins,Recombinant Proteins: analysis,Sensitivity and Specificity,Uncertainty,elisa},
mendeley-tags = {elisa},
month = jan,
number = {7},
pages = {1033--45},
pmid = {18605964},
title = {{An international comparability study to determine the sources of uncertainty associated with a non-competitive sandwich fluorescent ELISA.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18605964},
volume = {46},
year = {2008}
}
@manual{R2012,
address = {Vienna, Austria},
annote = {\{ISBN\} 3-900051-07-0},
author = {{R Core Team}},
organization = {R Foundation for Statistical Computing},
title = {{R: A Language and Environment for Statistical Computing}},
url = {http://www.r-project.org/},
year = {2012}
}
@article{Cembal2004,
abstract = {Uncertainty is a parameter associated with the result of a measurement; this parameter characterizes the dispersion of the values that could reasonably be attributed to the sample. Data processing methods do not take into account the influence of the imprecision and deviation of the experimental points of the calibration system and their impact on the final result of a sample analysis. The aim of this work is: (a) to propose, for each run, a simple method to calculate the uncertainty due to the calibration system (Uc); and (b) to present a method to determine the “intra-assay total uncertainty” (Ut) and evaluate its impact on the final result for an analyte. Ten replicates of standards, controls, and two serum- male and female samples were measured in the same run with a manual kit for determination of testosterone. To calculate Ut, random duplicate responses were selected. For controls and samples, Ut was affected by Uc (2.91\% to 6.59\%) and by the uncertainty of the measurement of the sample (Us) (1.01 to 8.73\%); this allowed us to determine that Ut had values from 3.73\% to 9.87\%. While Us affects the result of a given sample, Uc affects the result of all the samples with a similar response (cpm). In the method proposed, Ut involves Us and Uc, both factors that introduce variations into the result of a sample by random causes. Intra- assay total uncertainty includes the most probable result for the analytical methodology selected. Key},
author = {Cembal, Samy and Ambrosio, Jorge and Aranda, Claudio and Colombani, Miriam and Fenili, Cecilia and Fradinger, Erich and Klecha, Alicia and Sragowicz, Deborah and Zylbersztein, Cecilia},
doi = {10.1081/IAS-120027222},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cembal et al. - 2004 - Intra-assay Total Uncertainty of Results in Immunoassay Techniques.pdf:pdf},
journal = {Journal of Immunoassay and Immunochemistry},
keywords = {Calibration,Curve fitting method,Immunoassay,Radioimmunoassay,Uncertainty,elisa},
mendeley-tags = {elisa},
number = {August 2013},
pages = {1----15},
title = {{Intra-assay Total Uncertainty of Results in Immunoassay Techniques}},
volume = {25},
year = {2004}
}
@misc{GraphPad,
title = {{GraphPad Prism Help}},
url = {http://www.graphpad.com/guides/prism/6/curve-fitting/index.htm}
}
@article{Cheng2012,
abstract = {Methods for sample size calculations in ROC studies often assume independent normal distributions for test scores among the diseased and nondiseased populations. We consider sample size requirements under the default two-group normal model when the data distribution for the diseased population is either skewed or multimodal. For these two common scenarios we investigate the potential for robustness of calculated sample sizes under the mis-specified normal model and we compare to sample sizes calculated under a more flexible nonparametric Dirichlet process mixture model. We also highlight the utility of flexible models for ROC data analysis and their importance to study design. When nonstandard distributional shapes are anticipated, our Bayesian nonparametric approach allows investigators to determine a sample size based on the use of more appropriate distributional assumptions than are generally applied. The method also provides researchers a tool to conduct a sensitivity analysis to sample size calculations that are based on a two-group normal model. We extend the proposed approach to comparative studies involving two continuous tests. Our simulation-based procedure is implemented using the WinBUGS and R software packages and example code is made available.},
author = {Cheng, Dunlei and Branscum, Adam J and Johnson, Wesley O},
doi = {10.1002/sim.4396},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Cheng, Branscum, Johnson - 2012 - Sample size calculations for ROC studies parametric robustness and Bayesian nonparametrics.pdf:pdf},
issn = {1097-0258},
journal = {Statistics in medicine},
keywords = {Bayes Theorem,Humans,Nonparametric,ROC Curve,Sample Size,Statistics},
month = jan,
number = {2},
pages = {131--42},
pmid = {22139729},
title = {{Sample size calculations for ROC studies: parametric robustness and Bayesian nonparametrics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/22139729},
volume = {31},
year = {2012}
}
@article{Wilson2014,
abstract = {Scientists spend an increasing amount of time building and using software. However, most scientists are never taught how to do this efficiently. As a result, many are unaware of tools and practices that would allow them to write more reliable and maintainable code with less effort. We describe a set of best practices for scientific software development that have solid foundations in research and experience, and that improve scientists’ productivity and the reliability of their software.},
annote = {
        From Duplicate 1 ( 
        
          Best Practices for Scientific Computing
        
         - Wilson, Greg; Aruliah, D A; Brown, C Titus; Hong, Neil P Chue; Davis, Matt; Guy, Richard T; Haddock, Steven H D; Huff, Katy; Mitchell, Ian M; Plumbley, Mark D; Waugh, Ben; White, Ethan P; Wilson, Paul )

        
        

        

        

      },
archivePrefix = {arXiv},
arxivId = {arXiv:1210.0530v3},
author = {Wilson, Greg and Aruliah, D a and Brown, C Titus and {Chue Hong}, Neil P and Davis, Matt and Guy, Richard T and Haddock, Steven H D and Huff, Katy Kathryn D and Mitchell, Ian M and Plumbley, Mark D and Waugh, Ben and White, Ethan P and Wilson, Paul and Hong, Neil P Chue},
doi = {10.1371/journal.pbio.1001745},
eprint = {arXiv:1210.0530v3},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wilson et al. - Unknown - Best Practices for Scientific Computing.pdf:pdf;:D$\backslash$:/Home/SkyDrive/Documents/Literature/Wilson\_2014.pdf:pdf},
issn = {1545-7885},
journal = {PLoS biology},
month = jan,
number = {1},
pages = {1--6},
pmid = {24415924},
title = {{Best Practices for Scientific Computing}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3886731\&tool=pmcentrez\&rendertype=abstract},
volume = {12},
year = {2014}
}
@article{Bhandari2011,
abstract = {Low cost and scalable manufacture of lab-on-chip devices for applications such as point-of-care testing is an urgent need. Weaving is presented as a unified, scalable and low-cost platform for the manufacture of fabric chips that can be used to perform such testing. Silk yarns with different properties are first selected, treated with the appropriate reagent solutions, dried and handloom-woven in one step into an integrated fabric chip. This platform has the unique advantage of scaling up production using existing and low cost physical infrastructure. We have demonstrated the ability to create pre-defined flow paths in fabric by using wetting and non-wetting silk yarns and a Jacquard attachment in the loom. Further, we show that yarn parameters such as the yarn twist frequency and weaving coverage area may be conveniently used to tune both the wicking rate and the absorptive capacity of the fabric. Yarns optimized for their final function were used to create an integrated fabric chip containing reagent-coated yarns. Strips of this fabric were then used to perform a proof-of-concept immunoassay with sample flow taking place by capillary action and detection being performed by a visual readout.},
annote = {
        From Duplicate 1 ( 
        
        
          'Fab-chips': a versatile, fabric-based platform for low-cost, rapid and multiplexed diagnostics.
        
        
         - Bhandari, Paridhi; Narahari, Tanya; Dendukuri, Dhananjaya )

        
        

        

        

        From Duplicate 2 ( 
        
        
          'Fab-chips': a versatile, fabric-based platform for low-cost, rapid and multiplexed diagnostics.
        
        
         - Bhandari, Paridhi; Narahari, Tanya; Dendukuri, Dhananjaya )

        
        

        From Duplicate 1 ( 
        
        
          'Fab-chips': a versatile, fabric-based platform for low-cost, rapid and multiplexed diagnostics.
        
        
         - Bhandari, Paridhi; Narahari, Tanya; Dendukuri, Dhananjaya )

        
        

        From Duplicate 2 ( 
        
        
          'Fab-chips': a versatile, fabric-based platform for low-cost, rapid and multiplexed diagnostics.
        
        
         - Bhandari, Paridhi; Narahari, Tanya; Dendukuri, Dhananjaya )

        
        

        

        

        

        

        

        

      },
author = {Bhandari, Paridhi and Narahari, Tanya and Dendukuri, Dhananjaya},
doi = {10.1039/c1lc20373h},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Bhandari, Narahari, Dendukuri - 2011 - 'Fab-chips' a versatile, fabric-based platform for low-cost, rapid and multiplexed diagnostics(2).pdf:pdf},
issn = {1473-0189},
journal = {Lab on a chip},
keywords = {Immunoassay,Immunoassay: instrumentation,Immunoassay: methods,Microarray Analysis,Microarray Analysis: instrumentation,Microarray Analysis: methods,Silk},
month = aug,
number = {15},
pages = {2493--9},
pmid = {21735030},
title = {{'Fab-chips': a versatile, fabric-based platform for low-cost, rapid and multiplexed diagnostics.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/21735030},
volume = {11},
year = {2011}
}
@book{Motulsky2003,
abstract = {A practical guide to curve fitting. GraphPad Prism 4.0},
author = {Motulsky, Harvey and Christopoulos, Arthur},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Motulsky\_2003.pdf:pdf},
publisher = {GraphPad Software Inc., San Diego CA},
title = {{Fitting Models to Biological Data using Linear and Nonlinear Regression}},
url = {www.graphpad.com},
year = {2003}
}
@article{Obuchowski2006,
abstract = {ROC curves and summary measures of accuracy derived from them, such as the area under the ROC curve, have become the standard for describing and comparing the accuracy of diagnostic tests. Methods for estimating ROC curves rely on the existence of a gold standard which dichotomizes patients into disease present or absent. There are, however, many examples of diagnostic tests whose gold standards are not binary-scale, but rather continuous-scale. Unnatural dichotomization of these gold standards leads to bias and inconsistency in estimates of diagnostic accuracy. In this paper, we propose a non-parametric estimator of diagnostic test accuracy which does not require dichotomization of the gold standard. This estimator has an interpretation analogous to the area under the ROC curve. We propose a confidence interval for test accuracy and a statistical test for comparing accuracies of tests from paired designs. We compare the performance (i.e. CI coverage, type I error rate, power) of the proposed methods with several alternatives. An example is presented where the accuracies of two quick blood tests for measuring serum iron concentrations are estimated and compared.},
author = {Obuchowski, Nancy A},
doi = {10.1002/sim.2228},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Obuchowski\_2006.pdf:pdf},
issn = {0277-6715},
journal = {Statistics in medicine},
keywords = {80 and over,Adolescent,Adult,Aged,Computer Simulation,Confidence Intervals,Data Interpretation,Diagnostic Tests,Female,Ferritins,Ferritins: blood,Humans,Iron,Iron: blood,Middle Aged,Monte Carlo Method,ROC Curve,Routine,Routine: standards,Statistical,Transferrin,Transferrin: analysis,agreement studies},
mendeley-tags = {agreement studies},
month = feb,
number = {3},
pages = {481--93},
pmid = {16287217},
title = {{An ROC-type measure of diagnostic accuracy when the gold standard is continuous-scale.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/16287217},
volume = {25},
year = {2006}
}
@inproceedings{Blackwood,
abstract = {Slides.},
author = {Blackwood, James and Koch, David D},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Blackwood, Koch - Unknown - USING CLSI GUIDELINES TO PERFORM METHOD EVALUATION(2).pdf:pdf},
title = {{USING CLSI GUIDELINES TO PERFORM METHOD EVALUATION}}
}
@article{Nobile2000,
abstract = {A Bayesian analysis for factorial experiments is presented, using finite mixture distri- butions to model the main effects and interactions. This allows both estimation and an analogue of hypothesis testing in a posterior analysis using a single prior specification. A detailed formulation based on this approach is provided for the case of the two-way model with replication, allowing interactions. Issues in formulating a suitable prior are discussed in detail, and, in the context of two illustrative applications, we discuss implementation, presentation of posterior distributions, sensitivity and performance of the Markov chain Monte Carlo methods that are used.},
author = {Nobile, A and Grenn, P J},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Nobile, Grenn - 2000 - Bayesian analysis of factorial experiments by mixture modelling.pdf:pdf},
journal = {Biometrika},
keywords = {Analysis of variance,Bayes linear model,Finite mixture distribution,Identifiability,Markov chain Monte Carlo,Multiple comparisons,Partial exchangeability,Random partition,Reversible jump,Sensitivity analysis},
number = {1},
pages = {15--35},
title = {{Bayesian analysis of factorial experiments by mixture modelling}},
volume = {87},
year = {2000}
}
@article{Stang2013,
abstract = {Since its introduction, null hypothesis significance testing (NHST) has caused much debate. Many publications on common misunderstandings have appeared. Despite the many cautions, NHST remains one of the most prevalent, misused and abused statistical procedures in the biomedical literature. This article is directed at practicing researchers with limited statistical background who are driven by subject matter questions and have empirical data to be analyzed. We use a dialogue as in ancient Greek literature for didactic purposes. We illustrate several, though only a few, irritations that can come up when a researcher with minimal statistical background but a good sense of what she wants her study to do, and of what she wants to do with her study, asks for consultation by a statistician. We provide insights into the meaning of several concepts including null and alternative hypothesis, one- and two-sided null hypotheses, statistical models, test statistic, rejection and acceptance regions, type I and II error, p value, and the frequentist' concept of endless study repetitions.},
author = {Stang, Andreas and Poole, Charles},
doi = {10.1007/s10654-013-9861-4},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Stang\_2013.pdf:pdf},
issn = {1573-7284},
journal = {European journal of epidemiology},
keywords = {Data Interpretation, Statistical,Humans,Models, Statistical,Physicians,Probability,Research Personnel,Statistical Distributions},
month = dec,
number = {12},
pages = {939--44},
pmid = {24233192},
title = {{The researcher and the consultant: a dialogue on null hypothesis significance testing.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/24233192},
volume = {28},
year = {2013}
}
@incollection{Dunn2013,
author = {Dunn, John R and Wild, David},
booktitle = {The Immunoassay Handbook},
chapter = {3.6},
doi = {10.1016/B978-0-08-097037-0.00013-0},
edition = {4},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Dunn\_2013.pdf:pdf;:D$\backslash$:/Home/SkyDrive/Documents/Literature/Wild2013\_Chap3\_6-annotated.pdf:pdf},
isbn = {9780080970370},
keywords = {elisa},
mendeley-tags = {elisa},
pages = {323--337},
publisher = {Elsevier},
title = {{Calibration Curve Fitting}},
year = {2013}
}
@inproceedings{Yagmur2013,
author = {Yagmur, Eray and Haumann, Michaela and Koch, Alexander and Lutz, Holger and Trautwein, Christian and Tacke, Frank},
booktitle = {Clinic \& Lab Dialogue},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Yagmur et al.\_2013.pdf:pdf},
pages = {2013},
title = {{Evaluation of an automated hyaluronan latex agglutination assay enables the implementation of routine laboratory HEPASCORE calculation}},
year = {2013}
}
@article{Xie2013,
author = {Xie, Yihui},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Xie\_2013.pdf:pdf},
number = {1},
pages = {1--11},
title = {{knitr : A General-Purpose Tool for Dynamic Report Generation in R}},
volume = {8},
year = {2013}
}
@article{Peeling2006,
abstract = {Accurate diagnostic tests have a key role in patient management and the control of most infectious diseases. Unfortunately, in many developing countries, clinical care is often critically compromised by the lack of regulatory controls on the quality of these tests. The information available on the performance of a diagnostic test can be biased or flawed because of failings in the design of the studies which assessed the performance characteristics of the test. As a result, diagnostic tests are sold and used in much of the developing world without evidence of effectiveness. Misdiagnosis leading to failure to treat a serious infection or wasting expensive treatment on people who are not infected remains a serious obstacle to health.},
author = {Peeling, Rosanna W and Smith, Peter G and Bossuyt, Patrick M M},
doi = {10.1038/nrmicro1522},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Bossuyt\_2010.pdf:pdf},
issn = {1740-1534},
journal = {Nature reviews. Microbiology},
keywords = {Communicable Disease Control,Communicable Disease Control: methods,Communicable Diseases,Communicable Diseases: diagnosis,Developing Countries,Developing Countries: statistics \& numerical data,Diagnostic Services,Diagnostic Tests, Routine,Diagnostic Tests, Routine: standards,Public Health,Public Health: methods,World Health},
month = sep,
number = {9 Suppl},
pages = {S2--6},
pmid = {17003769},
publisher = {Nature Publishing Group},
title = {{A guide for diagnostic evaluations.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/17003769},
volume = {4},
year = {2006}
}
@article{Kang2004,
abstract = {We propose a simple method to compute sample size for an arbitrary test hypothesis in population pharmacokinetics (PK) studies analysed with non-linear mixed effects models. Sample size procedures exist for linear mixed effects model, and have been recently extended by Rochon using the generalized estimating equation of Liang and Zeger. Thus, full model based inference in sample size computation has been possible. The method we propose extends the approach using a first-order linearization of the non-linear mixed effects model and use of the Wald chi(2) test statistic. The proposed method is general. It allows an arbitrary non-linear model as well as arbitrary distribution of random effects characterizing both inter- and intra-individual variability of the mixed effects model. To illustrate possible uses of the method we present tables of minimum sample sizes, in particular, with an illustration of the effect of sampling design on sample size. We demonstrate how (D-)optimal or frequent sampling requires fewer subjects in comparison to a sparse sampling design. We also present results from Monte Carlo simulations showing that the computed sample size can produce the desired power. The proposed method greatly reduces computing times compared with simulation-based methods of estimating sample sizes for population PK studies.},
author = {Kang, Dongwoo and Schwartz, Janice B and Verotta, Davide},
doi = {10.1002/sim.1695},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Kang\_2004.pdf:pdf},
issn = {0277-6715},
journal = {Statistics in medicine},
keywords = {African Continental Ancestry Group,Calcium Channel Blockers,Calcium Channel Blockers: pharmacokinetics,Computer Simulation,European Continental Ancestry Group,Humans,Models, Biological,Models, Statistical,Monte Carlo Method,Nifedipine,Nifedipine: pharmacokinetics,Pharmacokinetics,Sample Size},
month = aug,
number = {16},
pages = {2551--66},
pmid = {15287084},
title = {{A sample size computation method for non-linear mixed effects models with applications to pharmacokinetics models.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15287084},
volume = {23},
year = {2004}
}
@incollection{Wheeler2013,
annote = {
        From Duplicate 1 ( 
        
          Infertility
        
         - Wheeler, Michael J )
And  Duplicate 2 ( 
        
          Infertility
        
         - Wheeler, Michael J )

        
        

        

        

        From Duplicate 3 ( 
        
          Infertility
        
         - Wheeler, Michael J )

        
        

        From Duplicate 1 ( 
        
        
          Infertility
        
        
         - Wheeler, Michael J )
And  Duplicate 2 ( 
        
        
          Infertility
        
        
         - Wheeler, Michael J )
And  Duplicate 4 ( 
        
        
          Infertility
        
        
         - Wheeler, Michael J )
And  Duplicate 5 ( 
        
        
          Infertility
        
        
         - Wheeler, Michael J )
And  Duplicate 6 ( 
        
        
          Infertility
        
        
         - Wheeler, Michael J )
And  Duplicate 7 ( 
        
        
          Infertility
        
        
         - Wheeler, Michael J )
And  Duplicate 8 ( 
        
        
          Infertility
        
        
         - Wheeler, Michael J )

        
        

        

        

        

        

      },
author = {Wheeler, Michael J},
booktitle = {The Immunoassay Handbook},
chapter = {9.5},
doi = {10.1016/B978-0-08-097037-0.00048-8},
edition = {Fourth Ed},
editor = {Wild, David},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Wheeler - 2013 - Infertility.pdf:pdf},
isbn = {9780080970370},
pages = {721--734},
publisher = {Elsevier Ltd},
title = {{Infertility}},
year = {2013}
}
@techreport{Wakabayashi2010,
author = {Wakabayashi, Katsumi},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/ELISA-AtoZ E.pdf:pdf},
title = {{ELISA -A to Z ..... from introduction to practice}},
year = {2010}
}
@misc{Clyde2001,
abstract = {This entry provides an overview of experimental design using a Bayesian decision-theoretic framework. Scientific experimentation requires decisions about how an experiment will be conducted and analyzed. Such decisions depend on the goals and purpose of the experiment, but certain choices may be re- stricted by available resources and ethical considerations. Prior information may be available from earlier experiments or from conjectures which moti- vate the investigation. The Bayesian approach provides a coherent framework where prior information and uncertainties regarding unknown quantities can be combined to find an experimental design that optimizes the goals of the experiment.},
author = {Clyde, Merlise A},
booktitle = {International Encyclopedia of Social and Behavioural Sciences},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Clyde\_2001.pdf:pdf},
number = {April},
pages = {1--22},
title = {{Experimental Design : A Bayesian Perspective}},
year = {2001}
}
@article{Currie1995,
abstract = {This IUPAC nomenclature document has been prepared to help establish a uniform and meaningful approach to terminology, notation, and formulation for performance characteristics of the Chemical Measurement Process (CMP). Following definition of the CMP and its Performance Characteristics, the document addresses fundamental quantities related to the observed response and calibration, and the complement to the calibration function: the evaluation function. Performance characteristics related to precision and accuracy comprise the heart of the document. These include measures for the means or "expected values" of the relevant chemical quantities, as well as dispersion measures such as variance and standard error. Attention is given also to important issues involving: assumptions, internal and external quality control, estimation, error propagation and uncertainty components, and bounds for systematic error. Special treatment is given to terminology and concepts underlying detection and quantification capabilities in chemical metrology, and the significance of the blank. The document concludes with a note on the important distinction between the Sampled Population and the Target Population, especially in relation to the interlaboratory environment.},
author = {Currie, Lloyd A},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Currie\_1995.pdf:pdf},
journal = {Pure \& Applied Chemistry},
number = {10},
pages = {1699--1723},
title = {{Nomenclature in evaluation of analytical methods including detection and quantification capabiIities (IUPAC Recommendations 1995)}},
volume = {67},
year = {1995}
}
@article{Sadray2003,
abstract = {Generalized least squares regression with variance function estimation was used to derive the calibration function for measurement of methotrexate plasma concentration and its results were compared with weighted least squares regression by usual weight factors and also with that of ordinary least squares method. In the calibration curve range of 0.05 to 100 mM, both heteroscedasticity and non-linearity were present therefore ordinary least squares linear regression methods could result in large errors in the calculation of methotrexate concentration. Generalized least squares regression with variance function estimation worked better than both the weighted regression with the usual weight factors and ordinary least squares regression and gave better estimates for methotrexate concentration.},
author = {Sadray, Sima and Rezaee, Saeed and Rezakhah, Saeid},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sadray, Rezaee, Rezakhah - 2003 - Non-linear heteroscedastic regression model for determination of methotrexate in human plasma by high-.pdf:pdf},
journal = {Journal of Chromatography},
keywords = {methotrexate,variance function estimation,weighted least squares regression},
pages = {293--302},
title = {{Non-linear heteroscedastic regression model for determination of methotrexate in human plasma by high-performance liquid chromatography}},
volume = {787},
year = {2003}
}
@article{Rabenau2007,
abstract = {This review summarizes major issues of verification and validation procedures and describes minimum requirements for verification and validation of diagnostic assays in clinical virology including instructions for CE/IVD-labeled as well as for self-developed (“home-brewed”) tests or test systems. It covers techniques useful for detection of virus specific antibodies, for detection of viral antigens, for detection of viral nucleic acids, and for isolation of viruses on cell cultures in the routine virology laboratory.},
author = {Rabenau, Holger F and Kessler, Harald H and Kortenbusch, Marhild and Steinhorst, Andreas and Raggam, Reinhard B and Berger, Annemarie},
doi = {10.1016/j.jcv.2007.07.009},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Rabenau\_2007.pdf:pdf},
journal = {Journal of Clinical Virology},
keywords = {ce,diagnostics,ivd,validation,verification,virology},
pages = {93--98},
title = {{Verification and validation of diagnostic laboratory tests in clinical virology}},
volume = {40},
year = {2007}
}
@article{Bonate1993,
abstract = {The goal of calibration is to estimate a sample's concentration and the error associated with that estimate when only its signal response is known. Simulations were run to test the accuracy and precision of various parametric confidence intervaals generated by the bias-corected, nonparametric bootstrap approach. None of the methods studied reached their asympototic coverage probability, although the exact parametric confidence interval came the closest. The ranges of exact parametric confidence intervals were significantly larger than the other methods. Approximate parametric confidence intervals and bootstrap confidence intervals were both dependent on the number of replicates analyzed and on the coefficient of variation of the assay. When only a single replicate was available for analysis, the bootstrap method was dismal in containing the true sample concentration. As the number of replicates increases, the width of the bootstrap confidence interval converged to the exact parametric confidence interval. Bootstrap confidence intervals produce maximal coverage with minimal range when 2--4 replicate samples are available for analysis.},
author = {Bonate, Peter L},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Bonate Approximate CIs in calibration using the bootstrap Anal Chem 65$\backslash$; 1367-1372$\backslash$; 1993.pdf:pdf},
journal = {Analytical Chemistry},
number = {10},
pages = {1367----1372},
title = {{Approximate CIs in calibration using the bootstrap}},
volume = {65},
year = {1993}
}
@unpublished{Geweke2011,
abstract = {This paper develops a multi-way analysis of variance for non-Gaussian multivariate distributions and provides a practical simulation algorithm to estimate the corresponding components of variance. It specifically addresses variance in Bayesian predictive distributions, showing that it may be decomposed into the sum of extrinsic variance, arising from posterior uncertainty about parameters, and intrinsic variance, which would exist even if parameters were known. Depending on the application at hand, further decomposition of extrinsic or intrinsic variance (or both) may be useful. The paper shows how to produce simulation-consistent estimates of all of these components, and the method demands little additional effort or computing time beyond that already invested in the posterior simulator. It illustrates the methods using a dynamic stochastic general equilibrium model of the US economy, both before and during the global financial crisis.},
author = {Geweke, John and Amisano, Gianni},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Geweke, Amisano - 2011 - ANALYSIS OF VARIANCE FOR BAYESIAN INFERENCE.pdf:pdf},
institution = {European Central Bank},
keywords = {Bayesian inference,analysis of variance,posterior simulation,predictive distributions},
series = {Working Paper Series},
title = {{ANALYSIS OF VARIANCE FOR BAYESIAN INFERENCE}},
url = {http://ssrn.com/abstract\_id=1969437},
year = {2011}
}
@article{Chesher2008a,
abstract = {* When evaluating the precision of a method it is necessary to assess the repeatability (within-run) and the total or within-laboratory precision. * It is insufficient to assess repeatability in a single run. * Clinical and Laboratory Standards Institute (CLSI) document EP05-A2 describes the protocols for determining the precision of a method. The precision of a method should be tested at at-least two levels; each run in duplicate, with two runs per day over 20 days. CLSI document EP15-A2 describes the protocols that should be undertaken by the user to verify precision claims by a manufacturer. Precision claims by a manufacturer should be tested at at-least two levels, by running three replicates over five days. * A spreadsheet for assisting with the calculations described in this article is available from the AACB web-site.},
author = {Chesher, Douglas},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Chesher\_2008.pdf:pdf},
issn = {0159-8090},
journal = {The Clinical biochemist. Reviews / Australian Association of Clinical Biochemists},
month = aug,
number = {August},
pages = {S23--6},
pmid = {18852851},
title = {{Evaluating assay precision.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=2556577\&tool=pmcentrez\&rendertype=abstract},
volume = {29 Suppl 1},
year = {2008}
}
@article{Daniels1994,
abstract = {Methodologies for establishing standard curves on automated immunoassay systems are dependent on the computational capability of the instrument, its throughput, and, perhaps, the technical experience of the user. Here, factors constraining this approach are discussed in general and the procedures used for the Serono SR1 analyzer are considered in detail. The SR1 is a moderate-throughput, fully automated immunoassay system capable of performing tests for (currently) 18 analytes. The associated user- generated standard curves are utilized not only for interpolation of results but also for evaluation of system performance. The choices of curve-fitting method, weighting of data, acceptance criteria, and outlier rejection are taken out of the hands of the user and are made a vital and integral part of the system. I justify this approach and demonstrate how a sensible mathematical approach must be tailored to the configuration of the instrument.},
author = {Daniels, Phelim B},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Daniels\_1994.pdf:pdf},
journal = {Clinical chemistry},
keywords = {calibration,data handling,standardization},
number = {4},
pages = {513--517},
title = {{The Fitting, Acceptance, and Processing of Standard Curve Data in Automated Immunoassay Systems, as Exemplified by the Serono SR1 Analyzer}},
volume = {40},
year = {1994}
}
@article{O'Malley2008,
abstract = {A precision profile, the relationship between the concentration of a substance and its measured precision, is a convenient way of conveying the ability of an immunoassay to accurately measure the concentration of a substance in blood serum. A precision profile is characterized by the definition of precision. Historically, precision has been evaluated as the standard error of an estimator of the concentration in a sample conditional on the true concentration. In this paper, Bayesian predictive inference is used to develop a new measure of precision based on the accuracy with which an assay could infer the concentration in a hypothetical new sample. This leads to a natural procedure for evaluating a precision profile that avoids using approximations such as those inherent in traditional methods. Substances: Thyroxine},
annote = {
        From Duplicate 1 ( 
        
        
          A Bayesian precision profile for measuring the quality of immunoassay experiments
        
        
         - O'Malley, A James )

        
        

        From Duplicate 2 ( 
        
        
          A Bayesian precision profile for measuring the quality of immunoassay experiments
        
        
         - O'Malley, A James )

        
        

        

        

        

        

      },
author = {O'Malley, A James},
doi = {10.1098/rsta.2008.0034},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/OMalley\_2008.pdf:pdf},
journal = {Philosophical Transactions of the Royal Society A},
keywords = {Bayes Theorem,Bayesian analysis,Biometry,Blood Chemical Analysis/standards,Blood Chemical Analysis/statistics \& numerical dat,Humans,Immunoassay/standards*,Immunoassay/statistics \& numerical data*,Nonlinear Dynamics,Predictive Value of Tests,Quality Control,Radioimmunoassay/standards,Radioimmunoassay/statistics \& numerical data,Thyroxine/blood,calibration,calibration curve,elisa,immunoassay,nonlinear regression,precision profile,predictive inference},
mendeley-tags = {Bayes Theorem,Biometry,Blood Chemical Analysis/standards,Blood Chemical Analysis/statistics \& numerical dat,Humans,Immunoassay/standards*,Immunoassay/statistics \& numerical data*,Nonlinear Dynamics,Predictive Value of Tests,Quality Control,Radioimmunoassay/standards,Radioimmunoassay/statistics \& numerical data,Thyroxine/blood},
pages = {2301----2312},
title = {{A Bayesian precision profile for measuring the quality of immunoassay experiments}},
url = {http://rsta.royalsocietypublishing.org/content/366/1874/2301.full},
volume = {366},
year = {2008}
}
@article{Shrout1979,
abstract = {Reliability coefficients often take the form of intraclass correlation coefficients. In this article, guidelines are given for choosing among six different forms of the intraclass correlation for reliability studies in which n target are rated by k judges. Relevant to the choice of the coefficient are the appropriate statistical model for the reliability and the application to be made of the reliability results. Confidence intervals for each of the forms are reviewed.},
author = {Shrout, P E and Fleiss, J L},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Shrout\_1979.pdf:pdf},
issn = {0033-2909},
journal = {Psychological bulletin},
month = mar,
number = {2},
pages = {420--8},
pmid = {18839484},
title = {{Intraclass correlations: uses in assessing rater reliability.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/18839484},
volume = {86},
year = {1979}
}
@techreport{Davis2003,
abstract = {Bio-Plex ™ suspension array system},
author = {Davis, Diana and Zhang, Aiguo and Etienne, Chloe and Huang, Ivan},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Davis\_2003.pdf:pdf},
institution = {Bio-Rad},
keywords = {elisa},
mendeley-tags = {elisa},
pages = {1--4},
title = {{Principles of Curve Fitting for Multiplex Sandwich Immunoassays}},
url = {http://www.bio-rad.com/LifeScience/pdf/Bulletin\_2861.pdf},
year = {2003}
}
@article{Published2010,
author = {McDaniel, Glen},
doi = {10.1309/LM22RU1RCUXZPRXN},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/McDaniel\_2010.pdf:pdf},
issn = {0007-5027},
journal = {Laboratory Medicine},
month = jul,
number = {8},
pages = {499--500},
title = {{Point-of-Care Testing Guideline Published by CLSI}},
url = {http://labmed.ascpjournals.org/cgi/doi/10.1309/LM22RU1RCUXZPRXN},
volume = {41},
year = {2010}
}
@article{Klauenberg2011,
author = {Klauenberg, Katy and Ebert, Bernd and Voigt, Jan and Walzel, Monika and Noble, James E and Knight, Alex E and Elster, Clemens},
doi = {10.1515/CCLM.2011.648Ad},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Klauenberg et al. - 2011 - Supplemental Data (Klauenberg).pdf:pdf},
journal = {Clinical chemistry and laboratory medicine},
keywords = {bayesian,elisa},
mendeley-tags = {bayesian,elisa},
number = {9},
pages = {Ad1--Ad5},
title = {{Supplemental Data (Klauenberg)}},
volume = {49},
year = {2011}
}
@techreport{Pierson2009,
abstract = {Clinical and Laboratory Standards Institute document EP25-A—Evaluation of Stability of In Vitro Diagnostic Reagents; Approved Guideline provides guidance and regression-based procedures for establishing stability-related claims of in vitro diagnostic (IVD) reagents such as reagent kits, calibrators, control products, and sample diluents. This guideline was written primarily for manufacturers and regulatory agencies, but will also be of interest to clinical laboratories. It provides information on the design, implementation, data analysis, and documentation needs for studies to establish and verify shelf life and in-use life of IVD reagents. Additional topics address assessment of product transport conditions on stability and accelerated stability testing. Clinical},
annote = {
        From Duplicate 2 ( 
        
          Evaluation of Stability of In Vitro Diagnostic Reagents ; Approved Guideline EP25-A
        
         - Pierson-Perry, James F; Altaie, Sousan S; Danielson, Susan J; Jorgenson, Birgitte Lund; Poetsch, Bettina; Savol, Rosanne M; Vaks, Jeffrey E; Budd, Jeffrey; De Vore, Karl; Magari, Robert )

        
        

        

        

      },
author = {Pierson-Perry, James F and Altaie, Sousan S and Danielson, Susan J and Jorgenson, Birgitte Lund and Poetsch, Bettina and Savol, Rosanne M and Vaks, Jeffrey E and Budd, Jeffrey and {De Vore}, Karl and Magari, Robert},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Pierson-Perry et al. - 2009 - Evaluation of Stability of In Vitro Diagnostic Reagents Approved Guideline EP25-A.pdf:pdf},
isbn = {1562387065},
keywords = {clsi},
mendeley-tags = {clsi},
number = {20},
title = {{Evaluation of Stability of In Vitro Diagnostic Reagents ; Approved Guideline EP25-A}},
volume = {29},
year = {2009}
}
@book{Kruschke2011,
author = {Kruschke, John K},
isbn = {978-0-12-381485-2},
publisher = {Academic Press, Elsevier, USA},
title = {{Doing Bayesian Data Analysis: A Tutorial with R and BUGS}},
url = {http://www.indiana.edu/~kruschke/DoingBayesianDataAnalysis/},
year = {2011}
}
@article{Lee2006,
author = {Lee, Jean W and Devanarayan, Viswanath and Barrett, Yu Chen and Weiner, Russell and Allinson, John and Fountain, Scott and Keller, Stephen and Weinryb, Ira and Green, Marie and Duan, Larry and Rogers, James A and Millham, Robert and Brien, Peter J O and Sailstad, Jeff and Khan, Masood and Ray, Chad and Wagner, John A},
doi = {10.1007/s11095-005-9045-3},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Lee\_2005.pdf:pdf},
journal = {Pharmaceutical Research},
number = {2 (February)},
pages = {312--328},
title = {{Fit-for-Purpose Method Development and Validation for Successful Biomarker Measurement}},
volume = {23},
year = {2006}
}
@article{Thanassi2012,
abstract = {Objective. To find a statistically significant separation point for the QuantiFERON Gold In-Tube (QFT) interferon gamma release assay that could define an optimal "retesting zone" for use in serially tested low-risk populations who have test "reversions" from initially positive to subsequently negative results. Method. Using receiver operating characteristic analysis (ROC) to analyze retrospective data collected from 3 major hospitals, we searched for predictors of reversion until statistically significant separation points were revealed. A confirmatory regression analysis was performed on an additional sample. Results. In 575 initially positive US healthcare workers (HCWs), 300 (52.2\%) had reversions, while 275 (47.8\%) had two sequential positive tests. The most statistically significant (Kappa = 0.48, chi-square = 131.0, P < 0.001) separation point identified by the ROC for predicting reversion was the tuberculosis antigen minus-nil (TBag-nil) value at 1.11 International Units per milliliter (IU/mL). The second separation point was found at TBag-nil at 0.72 IU/mL (Kappa = 0.16, chi-square = 8.2, P < 0.01). The model was validated by the regression analysis of 287 HCWs. Conclusion. Reversion likelihood increases as the TBag-nil approaches the manufacturer's cut-point of 0.35 IU/mL. The most statistically significant separation point between those who test repeatedly positive and those who revert is 1.11 IU/mL. Clinicians should retest low-risk individuals with initial QFT results < 1.11 IU/mL.},
author = {Thanassi, Wendy and Noda, Art and Hernandez, Beatriz and Newell, Jeffery and Terpeluk, Paul and Marder, David and Yesavage, Jerome a},
doi = {10.1155/2012/291294},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Thannasi\_2012.pdf:pdf},
issn = {2090-1844},
journal = {Pulmonary medicine},
keywords = {dawi publishing corporation},
month = jan,
pages = {291294},
pmid = {23326660},
title = {{Delineating a Retesting Zone Using Receiver Operating Characteristic Analysis on Serial QuantiFERON Tuberculosis Test Results in US Healthcare Workers.}},
url = {http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=3544373\&tool=pmcentrez\&rendertype=abstract},
volume = {2012},
year = {2012}
}
@article{Jones1995,
annote = {
        From Duplicate 1 ( 
        
        
          Sources of experimental variation in calibration curves for enzyme-linked immunosorbent assay
        
        
         - Jones, Geoffrey; Wortberg, Monika; Kreissig, Sabine B; Hammock, Bruce D; Rocke, David M )

        
        
Jones et al (1995) experiments with conditions suspected to affect calibration curve parameters (4PL) in this nice, clear article.  They do not, however, address weighting/precision profile.

        
Many of their observations are on plates.  I think an analogous study on Achira's gels would be interesting.  Does Achira's cartridge/chip system eliminate some of the problems of within-plate variability?

        
"...if the variation between curves of different plates is no greater than [within-plate variation], there is nothing to be lost by borrowing the curve, or some of its parameters..."
What is the analogous comparisons in Achira's system?  Had they envisioned creating their standard curves by running many chips?  Or when many standard sample gels are run, is there a different platform?

        
Does Achira have a max. (observed or theoretical) for bound antibody on gels?  What are 'blanks'?  It would be really nice to have the OD readings for those upper limits.

        
What would be the Bayesian version of this type of analysis?

        
Introduction alludes to evolution of curve functions.  Would like to review this.  See their refs 9--13. 

        

        From Duplicate 2 ( 
        
        
          Sources of experimental variation in calibration curves for enzyme-linked immunosorbent assay
        
        
         - Jones, Geoffrey; Wortberg, Monika; Kreissig, Sabine B; Hammock, Bruce D; Rocke, David M )

        
        
Jones et al (1995) experiments with conditions suspected to affect calibration curve parameters (4PL) in this nice, clear article. They do not, however, address weighting/precision profile.

        
Many of their observations are on plates. I think an analogous study on Achira's gels would be interesting. Does Achira's cartridge/chip system eliminate some of the problems of within-plate variability?

        
"...if the variation between curves of different plates is no greater than [within-plate variation], there is nothing to be lost by borrowing the curve, or some of its parameters..."
What is the analogous comparisons in Achira's system? Had they envisioned creating their standard curves by running many chips? Or when many standard sample gels are run, is there a different platform?

        
Does Achira have a max. (observed or theoretical) for bound antibody on gels? What are 'blanks'? It would be really nice to have the OD readings for those upper limits.

        
What would be the Bayesian version of this type of analysis?

        
Introduction alludes to evolution of curve functions. Would like to review this. See their refs 9--13. 

        

      },
author = {Jones, Geoffrey and Wortberg, Monika and Kreissig, Sabine B and Hammock, Bruce D and Rocke, David M},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Jones\_1995.pdf:pdf},
journal = {Analytica chimica \ldots},
keywords = {elisa},
mendeley-tags = {elisa},
pages = {197--207},
title = {{Sources of experimental variation in calibration curves for enzyme-linked immunosorbent assay}},
url = {http://www.sciencedirect.com/science/article/pii/000326709500249Y},
volume = {313},
year = {1995}
}
@article{Shao1993,
author = {Shao, Jun},
doi = {10.2307/2290328},
file = {:D$\backslash$:/Home/SkyDrive/Documents/Literature/Shao\_1993.pdf:pdf},
issn = {01621459},
journal = {Journal of the American Statistical Association},
keywords = {cross-validation,ear model selection by},
month = jun,
number = {422},
pages = {486},
title = {{Linear Model Selection by Cross-Validation}},
url = {http://www.jstor.org/stable/2290328?origin=crossref},
volume = {88},
year = {1993}
}
@incollection{Wild2013a,
booktitle = {The Immunoassay Handbook},
chapter = {Front matt},
doi = {10.1016/B978-0-08-097037-0.01001-0},
edition = {Fourth Ed},
editor = {Wild, David},
file = {:C$\backslash$:/Users/Tanya/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Unknown - 2013 - The Immunoassay Handbook Theory and applications of ligand binding.pdf:pdf},
publisher = {Elsevier Ltd},
title = {{The Immunoassay Handbook Theory and applications of ligand binding}},
year = {2013}
}
